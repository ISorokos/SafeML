{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python notebook for case-study run on the CICIDS security dataset, by Will Bridges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code set-up: Imports, Packages, Environment variables, and Methods\n",
    "\n",
    "The software versions used are:\n",
    "- The Python3 version used for this work is: Python 3.8.x\n",
    "- The scikit-learn version used is: scikit-learn 0.24.0\n",
    "- The seaborn version used is: 0.11.1\n",
    "- The Pandas version used is: 1.1.5 (although 1.2.0 was released recently, this should also work)\n",
    "\n",
    "Before running, please run these commands via pip, in the terminal:\n",
    "- pip install pandas\n",
    "- pip install scikit-learn\n",
    "- pip install scikit-plot\n",
    "- pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (CVM_Distance.py, line 31)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3418\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5cfb6f5d7d74>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from CVM_Distance import CVM_Dist\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/williambridges/Documents/SafeML/Implementation_in_Python/CVM_Distance.py\"\u001b[0;36m, line \u001b[0;32m31\u001b[0m\n\u001b[0;31m    return CVM_Dist\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os, sys # For accessing Python Modules in the System Path (for accessing the Statistical Measures modules)\n",
    "# See: https://stackoverflow.com/a/39311677\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "# Importing local modules (statistical distance measures)\n",
    "from CVM_Distance import CVM_Dist\n",
    "\n",
    "import pandas as pd # For DataFrames, Series, and reading csv data in.\n",
    "import seaborn as sns # Graphing, built ontop of MatPlot for ease-of-use and nicer diagrams.\n",
    "import matplotlib.pyplot as plt # MatPlotLib for graphing data visually. Seaborn more likely to be used.\n",
    "import numpy as np # For manipulating arrays and changing data into correct formats for certain libraries\n",
    "import sklearn # For Machine Learning algorithms\n",
    "import scikitplot # Confusion matrix plotting\n",
    "from sklearn.decomposition import PCA # For PCA dimensionality reduction technique\n",
    "from sklearn.preprocessing import StandardScaler # For scaling to unit scale, before PCA application\n",
    "from sklearn.preprocessing import LabelBinarizer # For converting categorical data into numeric, for modeling stage\n",
    "from sklearn.model_selection import StratifiedKFold # For optimal train_test splitting, for model input data\n",
    "from sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbors ML classifier (default n. of neighbors = 5)\n",
    "from scikitplot.metrics import plot_confusion_matrix # For plotting confusion matrices\n",
    "from sklearn.metrics import accuracy_score # For getting the accuracy of a model's predictions\n",
    "from sklearn.metrics import classification_report # Various metrics for model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean_dataset() method is used to remove infinite and Nan value errors (in the original dataset), which was causing errors in the PCA transform step.\n",
    "\n",
    "Code reference: https://stackoverflow.com/a/46581125 (with a minor change = removed the conversion to float64 type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_PCA_feature_names() method is used to generate feature names for the number of PCA components passed in as a param. Returns a list of feature names for principal component column headings, in a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCA_feature_names(num_of_pca_components):\n",
    "    feature_names = []\n",
    "    for i in range(num_of_pca_components):    \n",
    "        feature_names.append(f\"Principal component {i+1}\")\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_model_predict() method is used to train an input model, using StratifiedFKold for train_test splitting, and uses the trained model to predict the test data. It outputs a classification report which has various useful prediction metrics displayed. It also outputs a confusion matrix for the model's predictions. Finally, it returns the accuracy of the model's predictions.\n",
    "\n",
    "- 1) The for loop ('for train_index, test_index in skf.split(X, y):') is required as it uses the indexes that the  StratifiedKFold model (**skf**) produces to select the appropriate data rows/ points required for each data split.\n",
    "\n",
    "- 2) The 'X_train, X_test = X.iloc[train_index], X.iloc[test_index]' uses the skf indexes to find the index location (iloc) of each index, so it can extract the correct rows for the train_test split.\n",
    "\n",
    "- 3) The 'reshaped_y_train = np.asarray(y_train).reshape(-1, 1)' is required to reshape the label (y_train and y_test) to a 1D array, rather than a 2D array that is output by the train_test split.\n",
    "\n",
    "- 4) The 'model.fit(X_train, reshaped_y_train.ravel())' uses the input model and fits it (trains the model) on the training data. The '.ravel()' method just reshapes the label array again (flattens it) to match the input structure required by the sklearn method.\n",
    "\n",
    "- 5) The 'pred_y = model.predict(X_test)' uses the, now trained, model to attempt to predict the test data (X_test is passed in, and it predicts the label, pred_y).\n",
    "\n",
    "- 6) The 'score = classification_report(reshaped_y_test, pred_y)' calculates prediction metrics based upon the model's predictions. More info in the docs: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "The rest is self-explanatory. A confusion matrix is plotted and output after the method runs. The accuracy of the model is returned back to the caller, as well as other data required for the statistical distance measure methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See documentation above to understand what each step does, and why.\n",
    "def train_model_predict(model, model_name, X, y, skf):\n",
    "    for train_index, test_index in skf.split(X, y): # 1)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index] # 2)\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        reshaped_y_train = np.asarray(y_train).reshape(-1, 1) # 3)\n",
    "        reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "        \n",
    "    model.fit(X_train, reshaped_y_train.ravel()) # 4)\n",
    "    pred_y = model.predict(X_test) # 5)\n",
    "    score = classification_report(reshaped_y_test, pred_y) # 6)\n",
    "    print('Classification report: \\n', score, '\\n')\n",
    "    plot_confusion_matrix(reshaped_y_test, pred_y, title='Confusion Matrix for {}'.format(model_name))\n",
    "        \n",
    "    return accuracy_score(reshaped_y_test, pred_y), X_train, X_test, y_train, pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Reduced dimensions' variable for altering the number of PCA principal components. Can be altered for needs.\n",
    "dimensions_num_for_PCA = 30\n",
    "\n",
    "# Max number of permutations to run. Can be altered for needs.\n",
    "permutation_num = 10\n",
    "\n",
    "# 10 folds is usually the heuristic to follow for larger datasets of around this size.\n",
    "num_of_splits_for_skf = 10\n",
    "\n",
    "# Seed value to pass into models so that repeated runs result in the same output\n",
    "seed_val = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset into Pandas.DataFrame and showing the top 5 entries via 'df.head()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Friday_Morning_Data = pd.read_csv('Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "df = Friday_Morning_Data.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing column name issues\n",
    "\n",
    "Because of Excel being used to create the csv, the column headings/ names contain whitespace padding, incorrect capitalisation, etc... which makes it difficult to correctly select by column names. This piece of code below just removes these issues. \n",
    "\n",
    "Code Reference: https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the original data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing issues with ScikitLearn's PCA transform on this dataset\n",
    "\n",
    "Without cleaning the dataset, the PCA transform was throwing this error: \n",
    "- \"sklearn error ValueError: Input contains NaN, infinity or a value too large for dtype('float64')\". \n",
    "\n",
    "It isn't obvious which attribute and/ or data point is causing this as the input dataset is supposed to be fully clean with no Nan or erroneous values. Also, there are too many attributes to manually search through to check this too. Thus, a quick solution via stackoverflow was found to work (see the 'clean_dataset(df)' method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows have been removed by the cleaning, indicating that some rows did have issues/ errors within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()\n",
    "df_cleaned = clean_dataset(df_cleaned) # see methods at top of notebook\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting indexes since rows have been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.reset_index()\n",
    "# Removing un-needed index column added by reset_index method\n",
    "df_cleaned.drop('index', axis=1, inplace=True)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations before PCA can be used correctly (before Data Preparation feature selection via PCA)\n",
    "Looking at this resource and many others (https://towardsdatascience.com/pca-is-not-feature-selection-3344fb764ae6), it can be seen that PCA can, quite easily, be used incorrectly without proper consideration and/ or understanding.\n",
    "\n",
    "From the resource:\n",
    "- \"A common mistake new data scientists make is to apply PCA to non-continuous variables. While it is technically possible to use PCA on discrete variables, or categorical variables that have been one hot encoded variables, you should not. Simply put, if your variables don’t belong on a coordinate plane, then do not apply PCA to them\"\n",
    "\n",
    "Thus, PCA should **only** be applied to the numeric features- which **must** be scaled down to unit scale.\n",
    "\n",
    "### What features should be included from PCA, and why?\n",
    "\n",
    "Looking at the list of feature names in the dataset (shown below), one can see that all other features should be of numeric type (with domain knowledge). They're all currently numeric type (either float or int). Consequently, PCA **can be** fully applied after scaling them all to unit scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation: PCA Dimension reduction and scaling (Hughes' Phenomenon)\n",
    "\n",
    "PCA acts to reduce the dimensions/ search space of the dataset as much as possible, while trying to maintain the most information possible e.g. It can easily reduce the dimensionality by more than half, while still maintaining 99% of the original data's information- it does this by extracting out the most important information/ trends/ spread (variance) of each dimension/ attribute- into n 'principal components'.\n",
    "\n",
    "More formally: PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance.\n",
    "\n",
    "##### *Key note:* \n",
    "\"**PCA centers but does not scale the input data** for each feature before applying the SVD. The optional parameter whiten=True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for **Support Vector Machines with the RBF kernel** and the **K-Means clustering algorithm**.\" (https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "\n",
    "PCA still works without standardizing the features to unit scale **but tranforming to unit scale should still be done** to prevent large variance features from having an over-bearing affect on other lower variance features (via something like StandardScaler here https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). \n",
    "\n",
    "This is **particularly important with this dataset**, as some features have massively wide variances and others do not (e.g. the 'idle_std' values can range from e+06, all the way to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the label attribute before dropping it.\n",
    "df_labels = df_cleaned['label']\n",
    "# Shows all the possible labels/ classes a model can predict.\n",
    "# Need to alter these to numeric 0, 1, etc... for model comprehension (e.g. pd.get_dummies()).\n",
    "df_labels.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label column has to be removed as you wouldn't want this involved in the PCA process. It can be concatted back with the PCA tranformed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Axis=1 means columns. Axis=0 means rows. inplace=False means that the original 'df' isn't altered.\n",
    "df_no_labels = df_cleaned.drop('label', axis=1, inplace=False)\n",
    "# Getting feature names for the StandardScaler process\n",
    "df_features = df_no_labels.columns.tolist()\n",
    "# Printing out Dataframe with no label column, to show successful dropping\n",
    "df_no_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using StandardScaler to transform features into unit scale, ready for PCA\n",
    "\n",
    "Code references: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60 & https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = df_no_labels.loc[:, df_features].values\n",
    "df_scaled = StandardScaler().fit_transform(df_no_labels)\n",
    "# Converting back to dataframe\n",
    "df_scaled = pd.DataFrame(data = df_scaled, columns = df_features)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting principle component variance\n",
    "\n",
    "A scree plot displays the variance explained by each principal component within the analysis.\n",
    "\n",
    "**The plot below shows that using the first 30 PCA components actually describes most/ all of the variation (information) within the original data. This is a huge dimension reduction from the initial 78 features, down to just 30.**\n",
    "\n",
    "Thus, looking at the Environment Variables (at the top of the notebook), the 'dimensions_num_for_PCA' variable will be set to **30** based upon this evidence.\n",
    "\n",
    "(Code reference: https://medium.com/district-data-labs/principal-component-analysis-with-python-4962cd026465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test = PCA().fit(df_scaled)\n",
    "plt.plot(np.cumsum(pca_test.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now fitting and transforming the data with PCA\n",
    "\n",
    "Thus, the optimal number of principle components is set to the environment variable and this is now used to produce the appropriate multi-dimensional principle component array. This will be formatted back to a Pandas dataframe afterwards.\n",
    "\n",
    "References: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html and https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=dimensions_num_for_PCA)\n",
    "principal_components = pca.fit(df_scaled).transform(df_scaled)\n",
    "principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Principal Component feature names, dynamically, for the optimal number of components (passed in as a param)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Methods at the top of the notebook\n",
    "principal_component_headings = get_PCA_feature_names(dimensions_num_for_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the Principal Components back into a Pandas Dataframe, ready for concatting back with the **label** feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc = pd.DataFrame(data = principal_components, columns = principal_component_headings)\n",
    "df_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining/ concatinating the label feature back onto the pca transformed dataset. Label still needs to be transformed into binary data (for model comprehension/ understanding i.e. the model doesn't understand string data but string data can be transformed into numeric data, which is model can understand and use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_pc, df_labels], axis = 1)\n",
    "# Scroll to the RHS end of dataframe to see attached label feature\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the label feature's categorical data into numeric data (via LabelBinarizer)\n",
    "\n",
    "Again, a model can't understand e.g. 'yes' and 'no' strings but, these can be mapped to a 1 for yes and a 0 for no.\n",
    "\n",
    "The **sklearn.preprocessing.LabelBinarizer** can be used to convert the column data into binary numbers, which will then be correctly interpreted.\n",
    "\n",
    "1. Fit the List- this tells the LabelBinarizer what values exist, and how to map them. \n",
    "\n",
    "2. Call transform, passing a List, and this will return the encoded List.\n",
    "\n",
    "**(Note: if label column has more than 2 unique labels, pandas.get_dummies is required instead)**\n",
    "\n",
    "(Code reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "df_final['label'] = lb.fit_transform(df_final['label'])\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the transformation. **Again, to note, if label isn't binary then pd.get_dummies is required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before LabelBinarizer: \", df_labels.unique())\n",
    "print(\"After LabelBinarizer: \", df_final['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data is now fully cleaned and transformed, ready for pre-modeling test_train data splitting\n",
    "\n",
    "## K-Fold Cross Validation and Stratified splitting\n",
    "K-Fold is a technique which splits data into K folds (splits). Train of a model K times, and for each training iteration, K-Fold selects a different fold to use for testing; the remaining K - 1 folds become the training data. Typically, the optimal K value can be derived using the size of your dataset (num of rows). Ideally, each fold should be statistically representative of the population. Too small and it won't be useful. Too large, and you lose the positives from doing K-Fold.\n",
    "\n",
    "You can use Stratified splitting with K-Fold, which ensures balance between some criteria (balances out the classes) e.g. equal portion of label classes in each fold.\n",
    "\n",
    "Class Imbalance is a significant issue in the ML/ Data Mining domain. It leads to incorrect results e.g. if one fold had all of 1 label (accidentally), then it would produce terrible predictive results as it wouldn't know what the other label class data point would look like. You can only work with the data you have, so this has to be dealt with.\n",
    "\n",
    "Benefits of K-Fold:\n",
    "- Use more of the data towards making a succesful model.\n",
    "- Obtain K models to evaluate, can improve the confidence that you have selected an appropriate model algorithm and cleaned/ prepared the data correctly, e.g. normal split with 1 model, one doesn't know if it's good or not- it could be heavily biased. Multiple models ensures less bias and increased variance.\n",
    "- Looking at the accuracy results from each of the k-Folds, you can identify data issues e.g. a certain fold performs really badly. Could this suggest that more cleaning is required? Maybe the data preparation was performed incorrectly?\n",
    "- If all folds return similar accuracies, one can be more confident that a deployed model will perform similarly to how one expects.\n",
    "\n",
    "Issues with K-Fold:\n",
    "- Creating K separate models requires more computation.\n",
    "- If you haven't got much data, you might not get many folds. Less folds means K-Fold loses its benefits.\n",
    "- If K is very large, each fold is small, and harder to ensure statistical distribution of.\n",
    "- Choosing the best of K models introduces bias. Real world data could perform better under a more general, lower performing model.\n",
    "\n",
    "Code reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the label so that the answers aren't provided to the model, in training.\n",
    "X = df_final.drop(['label'], axis = 1)\n",
    "y = df_final['label']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the StratifiedKFold model (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=num_of_splits_for_skf, shuffle=False)\n",
    "skf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, splitting the data into train and test data, using the optimal splitting techniques of K-Fold and Stratified Splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    reshaped_y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "    reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "    \n",
    "print( 'X_train length: ', len(X_train) ) # To check if splits worked\n",
    "print( 'y_train length: ', len(y_train) )\n",
    "print( 'X_test length: ', len(X_test) )\n",
    "print( 'y_test length: ', len(y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling stage\n",
    "Data is now fully transformed and ready for ML model training and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest neighbor ML classifier\n",
    "The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are assumed to be near to each other.\n",
    "\n",
    "The most important factor in training a KNN model is the **number of neighbors hyperparameter**. You want to choose the K  value that reduces the number of errors, while maintaining the algorithm’s ability to accurately make predictions when it’s given data it hasn’t seen before. Here are some important considerations:\n",
    "\n",
    "- As you decrease the value of K to **1**, predictions become less stable e.g. imagine K=1 and you have a query point surrounded by several red 'dots' and one green 'dot', but the green dot is the single nearest neighbor. Reasonably, you would think the query point is most likely red, but because K=1, KNN incorrectly predicts that the query point is green.\n",
    "\n",
    "- Inversely, as you increase the value of K, predictions become more stable due to majority voting/ averaging, and thus, more likely to make more accurate predictions (up to a certain critical point- an **'overfitting' threshold**). Eventually, you would begin to witness an increasing number of errors. It is at this point you'd know that you have pushed the value of K too far.\n",
    "\n",
    "- In cases where you are taking a majority vote (e.g. picking the mode in a classification problem) amongst labels/ classes, you usually make K an odd number to have a tiebreaker.\n",
    "\n",
    "The sklearn's default k value is 5 (also true for MATLAB's implementation).\n",
    "\n",
    "References: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html & https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model and predicting test data results (confusion matrix)\n",
    "The selected Machine Learning classifier/ model/ models can now be trained on training data (from the StratifiedKFold splitting). Once the model is trained, it can be used to predict the test data's labels (based upon what it has seen before).\n",
    "\n",
    "The performance of the model can be seen below in the Classification Report, Confusion Matrix, and the model's predicitive accuracy result.\n",
    "\n",
    "(see **methods** section, at the top of the notebook, for the train_model_predict() method code. Note that it can take a few minutes to run due to the vast amount of data used, and the training time required for the model to learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpacking the method return values. Last 4 are needed for statistical distance measure methods.\n",
    "accuracy, X_train, X_test, y_train, pred_y = train_model_predict(knn_model, \"K-Nearest Neighbor\", X, y, skf)\n",
    "print(\"Model accuracy= \", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SafeML statistical distance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the number of classes and labels from the label feature\n",
    "class_num = len(df_final['label'].unique())\n",
    "labels = df_final['label'].unique()\n",
    "print(\"Number of classes: \", class_num)\n",
    "print(\"Labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test using only the first label (i.e. label[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 = X_test[np.where(np.asarray(y_train).reshape(-1, 1).ravel() == labels[0])]\n",
    "# X_train_L = X_train.iloc[np.where(y_train[:,1] == 1)]\n",
    "X_train_L = X_train.loc[y_train == labels[0]]\n",
    "X_train_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_L = X_test.loc[pred_y == labels[0]]\n",
    "X_test_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results(jj, 2, kk) = Cramer_Von_Mises(XTrain_L,XTest_L);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvm_distance = Cramer_Von_Mises"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
