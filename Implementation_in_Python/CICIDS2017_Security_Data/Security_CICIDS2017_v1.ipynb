{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python notebook for case-study run on the CICIDS security dataset, by William Bridges (part of BSc dissertation work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code set-up: Imports, Packages, Environment variables, and Methods\n",
    "\n",
    "The software versions used are:\n",
    "- The Python3 version used for this work is: Python 3.8.x\n",
    "- The scikit-learn version used is: scikit-learn 0.24.0\n",
    "- The seaborn version used is: 0.11.1\n",
    "- The Pandas version used is: 1.1.5 (although 1.2.0 was released recently, this should also work)\n",
    "\n",
    "Before running, please run these commands via pip, in the terminal:\n",
    "- pip install pandas\n",
    "- pip install scikit-learn\n",
    "- pip install scikit-plot\n",
    "- pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys # For accessing Python Modules in the System Path (for accessing the Statistical Measures modules)\n",
    "# See: https://stackoverflow.com/a/39311677\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "# Importing local modules (statistical distance measures)\n",
    "from CVM_Distance import CVM_Dist as Cramer_Von_Mises_Dist\n",
    "from Anderson_Darling_Distance import Anderson_Darling_Dist\n",
    "from Kolmogorov_Smirnov_Distance import Kolmogorov_Smirnov_Dist\n",
    "from KuiperDistance import Kuiper_Dist\n",
    "from WassersteinDistance import Wasserstein_Dist\n",
    "from DTS_Distance import DTS_Dist # Combo of Anderson_Darling and CVM distance.\n",
    "\n",
    "import pandas as pd # For DataFrames, Series, and reading csv data in.\n",
    "import seaborn as sns # Graphing, built ontop of MatPlot for ease-of-use and nicer diagrams.\n",
    "import matplotlib.pyplot as plt # MatPlotLib for graphing data visually. Seaborn more likely to be used.\n",
    "import numpy as np # For manipulating arrays and changing data into correct formats for certain libraries\n",
    "import sklearn # For Machine Learning algorithms\n",
    "import scikitplot # Confusion matrix plotting\n",
    "from sklearn.decomposition import PCA # For PCA dimensionality reduction technique\n",
    "from sklearn.preprocessing import StandardScaler # For scaling to unit scale, before PCA application\n",
    "from sklearn.preprocessing import LabelBinarizer # For converting categorical data into numeric, for modeling stage\n",
    "from sklearn.model_selection import StratifiedKFold # For optimal train_test splitting, for model input data\n",
    "from sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbors ML classifier (default n. of neighbors = 5)\n",
    "from scikitplot.metrics import plot_confusion_matrix # For plotting confusion matrices\n",
    "from sklearn.metrics import accuracy_score # For getting the accuracy of a model's predictions\n",
    "from sklearn.metrics import classification_report # Various metrics for model performance\n",
    "from sklearn.neural_network import MLPClassifier # For Neural Network classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean_dataset()** method is used to remove infinite and Nan value errors (in the original dataset), which was causing errors in the PCA transform step.\n",
    "\n",
    "- **assert** keyword can be used for testing purposes. If the input param is not of type Pandas Dataframe, then the error message will be shown. Thus, you 'assert' that a pd.Dataframe is input.\n",
    "\n",
    "\n",
    "- **.dropna(inplace=True)** drops any rows which contain NaN/ Null values. NaN values would cause issues later on, with model training and other functions. A lot of functions require NaN values to be removed to work (see docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html).\n",
    "\n",
    "Code reference: https://stackoverflow.com/a/46581125 (with a minor change = removed the conversion to float64 type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_PCA_feature_names()** method is used to generate feature names for the number of PCA components passed in as a param. Returns a list of feature names for principal component column headings, in a Pandas Dataframe.\n",
    "\n",
    "- No special code here, just pure Python code. Instantiating a list, and populating it with strings for the PCA header names. Then, appending each to the list and returning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCA_feature_names(num_of_pca_components):\n",
    "    feature_names = []\n",
    "    for i in range(num_of_pca_components):    \n",
    "        feature_names.append(f\"Principal component {i+1}\")\n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_model_predict()** method is used to train an input model, using StratifiedFKold for train_test splitting, and uses the trained model to predict the test data. It outputs a classification report which has various useful prediction metrics displayed. It also outputs a confusion matrix for the model's predictions. Finally, it returns the accuracy of the model's predictions.\n",
    "\n",
    "- 1) The for loop ('for train_index, test_index in skf.split(X, y):') is required as it uses the indexes that the  StratifiedKFold model (**skf**) produces to select the appropriate data rows/ points required for each data split.\n",
    "\n",
    "\n",
    "- 2) The 'X_train, X_test = X.iloc[train_index], X.iloc[test_index]' uses the skf indexes to find the index location (iloc) of each index, so it can extract the correct rows for the train_test split.\n",
    "\n",
    "\n",
    "- 3) The 'reshaped_y_train = np.asarray(y_train).reshape(-1, 1)' is required to reshape the label (y_train and y_test) to a 1D array, rather than a 2D array that is output by the train_test split.\n",
    "\n",
    "\n",
    "- 4) The 'model.fit(X_train, reshaped_y_train.ravel())' uses the input model and fits it (trains the model) on the training data. The '.ravel()' method just reshapes the label array again (flattens it) to match the input structure required by the sklearn method.\n",
    "\n",
    "\n",
    "- 5) The 'pred_y = model.predict(X_test)' uses the, now trained, model to attempt to predict the test data (X_test is passed in, and it predicts the label, pred_y).\n",
    "\n",
    "\n",
    "- 6) The 'score = classification_report(reshaped_y_test, pred_y)' calculates prediction metrics based upon the model's predictions. More info in the docs: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "The rest is self-explanatory. A confusion matrix is plotted and output after the method runs. The accuracy of the model is returned back to the caller, as well as other data required for the statistical distance measure methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See documentation above to understand what each step does, and why.\n",
    "def train_model_predict(model, model_name, X, y, skf):\n",
    "    for train_index, test_index in skf.split(X, y): # 1)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index] # 2)\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        reshaped_y_train = np.asarray(y_train).reshape(-1, 1) # 3)\n",
    "        reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "        \n",
    "    model.fit(X_train, reshaped_y_train.ravel()) # 4)\n",
    "    pred_y = model.predict(X_test) # 5)\n",
    "    score = classification_report(reshaped_y_test, pred_y) # 6)\n",
    "    print('Classification report: \\n', score, '\\n')\n",
    "    plot_confusion_matrix(reshaped_y_test, pred_y, title='Confusion Matrix for {}'.format(model_name))\n",
    "        \n",
    "    return accuracy_score(reshaped_y_test, pred_y), X_train, X_test, y_train, pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_shuffled_stratifiedKFold_train_test_split()** method is for retrieving shuffled, StratifiedKFold train test split data. It's similar to the 'train_model_predict()' method above- see this for details behind each step/ line of code. This is industry-standard for dealing with datasets which have a large class imbalance, such as this one.\n",
    "\n",
    "The method takes in X (dataset without the labels/ classes) and y (the label/ class feature). It instantiates a new StratifiedKFold 'model' which comes from the 'sklearn.model_selection' library. The **n_splits** parameter denotes how many folds (K) are to be run. The **Shuffle** parameter denotes that each split will be shuffled and therefore randomized to get varying shuffles between runs. This is ideal, as each permutation to be run should be as varied as possible to give good variance in plotting- so the SafeML idea can be showcased & plotted optimally.\n",
    "\n",
    "The for loop uses the StratifiedKFold 'model' to extract indices that have been found by the skf model. The label/ class feature needs to be reshaped in SkLearn, as un-shaped label data causes exceptions/ errors when attempting to train, predict, or use any SkLearn ML classifiers/ models.\n",
    "\n",
    "Finally, all the extracted data is returned.\n",
    "\n",
    "See docs here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shuffled_stratifiedKFold_train_test_split(X, y):\n",
    "    # Shuffle to True, to get different shuffles each time. Permutations being varied is the goal here.\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "    # For loop to get index for training and test data, using StratifiedKFold (3 splits)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        # Reshape needed for label, so that the model can use it (required in SciKit Learn)\n",
    "        reshaped_y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "        reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "        \n",
    "    # After for loop ends, all training and test data has been retrieved thus can return it.\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks (MLP), quick explanation/ clarification\n",
    "Neural Networks, and specifically the sklearn Multi-Layer Perceptron (MLPClassifier), aims to model the brain's neurons and its inner workings. \n",
    "\n",
    "A set of input values and weights are evaluated, and passed into the first layer of the neural network. The neural network can have multiple layers, with a varied amount of nodes (via hyperparameters) in each layer. \n",
    "\n",
    "Hyperparameters are the parameters of the model that- if changed- affect the learning rate and prediction accuracy.\n",
    "\n",
    "If an input weight, X, along with a learning rate, Y, evaluates to a value above a given **threshold** (like a neuron's Potassium & Sodium based Action Potential threshold), then just like a real neuron, the neuron will fire and send information to the next node in the layer, or possibly the output node. \n",
    "\n",
    "The output node will take all of the processed information, from all the nodes in each interconnected (fully-connected MLP) layer, and a classification result will be given.\n",
    "\n",
    "The inner workings of large Neural Nets are still un-explainable (as of March 2021), which is why they're referred to as a **Black-box classifier/ model**.\n",
    "\n",
    "This is how a Neural Network learns and predicts a label/ class based on input data. The methods by which learning occurs (Backpropagation, Momentum, Learning rate input, Weight adjustments, etc...) is beyond the scope of this solution.\n",
    "\n",
    "From docs: \"Multi-layer Perceptron classifier. This model optimizes the log-loss function using LBFGS or stochastic gradient descent\".\n",
    "\n",
    "Important note from the default MLP params, from the docs: \n",
    "- \"Note: The default solver ‘adam’ works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, ‘lbfgs’ can converge faster and perform better\". \n",
    "\n",
    "###### So the 'lbfgs solver' hyperparameter may not be ideal for this larger dataset. Thus, the 'Adam solver' will be used. \n",
    "\n",
    "(Code references: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal Neural Network hyperparameters is hard\n",
    "\"So what about size of the hidden layer(s)--how many neurons? There are some empirically-derived rules-of-thumb, of these, the most commonly relied on is 'the optimal size of the hidden layer is usually between the size of the input and size of the output layers\" by Jeff Heaton (https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw). As per the reference, a low amount of nodes in the hidden layer can be ideal for a particular dataset. \n",
    "\n",
    "Input size is around 30 PCA component features, with 1 output node (for the label). Therefore between 10 and 20 nodes, with 1 hidden layer, would be following these heuristics. It can be incredibly hard to train and larger Neural Network correctly, although it has been proved- theoretically- that a Neural Network can solve any task, as long as it's large enough and has the correct hyperparameter tuning. This is an NP-hard problems though.\n",
    "\n",
    "See docs: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_and_predict_Neural_Network_MLP_model()** method trains a Neural Network (MLP) classifier and uses this to predict the labels of the X_test data. See more in-depth information about Neural Networks above^^^. For in-depth info on the hyperparameter tuning in SciKit (will be similar to TensorFlow & Keras, PyTorch, FastAI, and any other ML library), see below...\n",
    "\n",
    "**Hyperparameter explanations:**\n",
    "- **activiation=** ReLu is an industry-standard activation function \"rectified linear unit function, f(x) = max(0, x)\" (SciKit docs)\n",
    "- **hidden_layer_sizes=** (15,) means 1 hidden layer with 15 nodes in\n",
    "- **solver=** Adam solver works efficiently on large datasets, and is the \"stochastic gradient-based optimizer\" (SciKit docs)\n",
    "- **alpha=** is a penalty (regularization term) parameter\n",
    "- **batch_size=** refers to the size of mini-batches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch. When set to “auto”, batch_size=min(200, n_samples)\n",
    "- **learning_rate=** is the \"learning rate schedule for weight updates\". Setting as 'constant' would keep the learning rate the same as the learning_rate_init value, even if two epochs were to not decrease by at least 'tol', which is not ideal. Thus, an 'adaptive' learning rate is used and this \"keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if ‘early_stopping’ is on, the current learning rate is divided by 5\" (SciKit docs)\n",
    "- **learning_rate_init=** is just the initial learning rate value. It \"controls the step-size in updating the weights\".\n",
    "- **max_iter=** refers to the maximum number of iterations the MLP classifier will run through (each iteration/ epoch updates the node weights in accordance to other hyperparam info e.g. activation function, alpha, learning rate, etc... This is to try and increase the predictive capability of the MLP model). SciKit docs state \"For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps\". Although, the more iterations, the longer the training takes. After a certain threshold, more iterations will not improve the predictive capability of the MLP model.\n",
    "\n",
    "Note: Could add if statement to print out confusion matrix x amount of times, during this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_Neural_Network_MLP_model(X_train, X_test, y_train, y_test):\n",
    "    # Instantiating new Neural Network classifier and setting Hyperparameters\n",
    "    Neural_Net_model = MLPClassifier(hidden_layer_sizes=(15,), activation='relu',\n",
    "                                    solver='adam', alpha=0.0001, batch_size='auto',\n",
    "                                    learning_rate='adaptive', learning_rate_init=0.005,\n",
    "                                    max_iter=400)\n",
    "    \n",
    "    # Fitting the model is synonymous to training the model. Need to call .ravel() to get array in correct format.\n",
    "    Neural_Net_model.fit(X_train, y_train.ravel())\n",
    "    \n",
    "    # Using the model to predict the label/ classes, based upon X_test data only. This is the model's answers.\n",
    "    pred_y = Neural_Net_model.predict(X_test) \n",
    "    \n",
    "    # Returning model answers and the accuracy of the model i.e. how well it predicts the answers.\n",
    "    return pred_y, accuracy_score(y_test, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Reduced dimensions' variable for altering the number of PCA principal components. Can be altered for needs.\n",
    "dimensions_num_for_PCA = 30\n",
    "\n",
    "# Max number of permutations to run. Can be altered for needs.\n",
    "number_of_permutations = 10\n",
    "\n",
    "# 10 folds is usually the heuristic to follow for larger datasets of around this size.\n",
    "num_of_splits_for_skf = 10\n",
    "\n",
    "# Seed value to pass into models so that repeated runs result in the same output\n",
    "seed_val = 1\n",
    "\n",
    "# Number of statistical distance measures to run (for the results, columns section)\n",
    "num_of_statistical_dist_measures = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset into Pandas.DataFrame and showing the top 5 entries via 'df.head()'\n",
    "\n",
    "- **pd.read_csv()** reads in the csv dataset, which is in the **.gitignore file** as this shouldn't be included in the repo. Thus, to run, please put the same dataset into the same folder as this jupyter notebook file (see docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n",
    "\n",
    "\n",
    "- The csv data is coverted into a **Pandas Dataframe** object (via the pd.read_csv method). Pandas is really useful for data manipulation, and acts as a standardised way to work with data- allowing easier use of in-built functions without worrying about datatype coversion and compatability (see docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "\n",
    "- The Pandas Dataframe consists of multiple **Pandas Series** objects, which are \"One-dimensional ndarray with axis labels (including time series)\". Thus, each Pandas Dataframe column/ feature/ attribute can be pulled out, and would then be a Pandas Series object (see docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)\n",
    "\n",
    "\n",
    "- Finally, to note, **numpy** can also be used to manipulate the data where needed, if the functionality you're looking for is not in the Pandas library (e.g. specialised array-based manip functions). Numpy works nicely with Pandas (see docs: https://numpy.org/)\n",
    "\n",
    "\n",
    "- **DataFrame.copy()** does a **deep copy** of the Dataframe, whereas **df_copy = df** would only do a **shallow copy**.\n",
    "\n",
    "\n",
    "- **df.head()** just shows the top (5 by default) entries in the imported dataset, which is now of type Pandas.Dataframe (see docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of Dataframe i.e. Pandas Dataframe:  <class 'pandas.core.frame.DataFrame'>\n",
      "Datatype of Column i.e. Pandas Series:  <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination Port</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Total Fwd Packets</th>\n",
       "      <th>Total Backward Packets</th>\n",
       "      <th>Total Length of Fwd Packets</th>\n",
       "      <th>Total Length of Bwd Packets</th>\n",
       "      <th>Fwd Packet Length Max</th>\n",
       "      <th>Fwd Packet Length Min</th>\n",
       "      <th>Fwd Packet Length Mean</th>\n",
       "      <th>Fwd Packet Length Std</th>\n",
       "      <th>...</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3268</td>\n",
       "      <td>112740690</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>1152</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>201.5</td>\n",
       "      <td>204.724205</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>3.594286e+02</td>\n",
       "      <td>1.199802e+01</td>\n",
       "      <td>380</td>\n",
       "      <td>343</td>\n",
       "      <td>16100000.0</td>\n",
       "      <td>4.988048e+05</td>\n",
       "      <td>16400000</td>\n",
       "      <td>15400000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389</td>\n",
       "      <td>112740560</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>5056</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>201.5</td>\n",
       "      <td>204.724205</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>3.202857e+02</td>\n",
       "      <td>1.574499e+01</td>\n",
       "      <td>330</td>\n",
       "      <td>285</td>\n",
       "      <td>16100000.0</td>\n",
       "      <td>4.987937e+05</td>\n",
       "      <td>16400000</td>\n",
       "      <td>15400000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>113757377</td>\n",
       "      <td>545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.361829e+06</td>\n",
       "      <td>7.324646e+06</td>\n",
       "      <td>18900000</td>\n",
       "      <td>19</td>\n",
       "      <td>12200000.0</td>\n",
       "      <td>6.935824e+06</td>\n",
       "      <td>20800000</td>\n",
       "      <td>5504997</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5355</td>\n",
       "      <td>100126</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>54760</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
       "0               3268       112740690                  32   \n",
       "1                389       112740560                  32   \n",
       "2                  0       113757377                 545   \n",
       "3               5355          100126                  22   \n",
       "4                  0           54760                   4   \n",
       "\n",
       "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
       "0                       16                         6448   \n",
       "1                       16                         6448   \n",
       "2                        0                            0   \n",
       "3                        0                          616   \n",
       "4                        0                            0   \n",
       "\n",
       "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
       "0                          1152                     403   \n",
       "1                          5056                     403   \n",
       "2                             0                       0   \n",
       "3                             0                      28   \n",
       "4                             0                       0   \n",
       "\n",
       "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
       "0                       0                    201.5              204.724205   \n",
       "1                       0                    201.5              204.724205   \n",
       "2                       0                      0.0                0.000000   \n",
       "3                      28                     28.0                0.000000   \n",
       "4                       0                      0.0                0.000000   \n",
       "\n",
       "   ...   min_seg_size_forward   Active Mean    Active Std   Active Max  \\\n",
       "0  ...                     32  3.594286e+02  1.199802e+01          380   \n",
       "1  ...                     32  3.202857e+02  1.574499e+01          330   \n",
       "2  ...                      0  9.361829e+06  7.324646e+06     18900000   \n",
       "3  ...                     32  0.000000e+00  0.000000e+00            0   \n",
       "4  ...                      0  0.000000e+00  0.000000e+00            0   \n",
       "\n",
       "    Active Min   Idle Mean      Idle Std   Idle Max   Idle Min   Label  \n",
       "0          343  16100000.0  4.988048e+05   16400000   15400000  BENIGN  \n",
       "1          285  16100000.0  4.987937e+05   16400000   15400000  BENIGN  \n",
       "2           19  12200000.0  6.935824e+06   20800000    5504997  BENIGN  \n",
       "3            0         0.0  0.000000e+00          0          0  BENIGN  \n",
       "4            0         0.0  0.000000e+00          0          0  BENIGN  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Friday_Morning_Data = pd.read_csv('Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "df = Friday_Morning_Data.copy()\n",
    "print(\"Datatype of Dataframe i.e. Pandas Dataframe: \", type(df))\n",
    "print(\"Datatype of Column i.e. Pandas Series: \", type(df.iloc[:, 1]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing column name issues\n",
    "\n",
    "Because of Excel being used to create the csv, the column headings/ names contain whitespace padding, incorrect capitalisation, etc... which makes it difficult to correctly select by column names. This piece of code below just removes these issues.\n",
    "\n",
    "- **df_columns** can be used to access all the column heading names (see docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html)\n",
    "\n",
    "\n",
    "- The code below acts on each of the header names (for ease of use): \n",
    "    - **str.strip()** just strips/ removes any leading and trailing spaces\n",
    "    - **str.lower()** just converts all characters to lower case\n",
    "    - **str.replace('x', 'y')** just replaces all instances of x with y, i.e. changing spaces to '_'\n",
    "    - **str.replace('x', '')** can be used to remove the specified x characters (replacing with nothing/ empty char)\n",
    "\n",
    "Code Reference: https://medium.com/@chaimgluck1/working-with-pandas-fixing-messy-column-names-42a54a6659cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-6efba12d8d07>:1: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination_port</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>fwd_packet_length_max</th>\n",
       "      <th>fwd_packet_length_min</th>\n",
       "      <th>fwd_packet_length_mean</th>\n",
       "      <th>fwd_packet_length_std</th>\n",
       "      <th>...</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3268</td>\n",
       "      <td>112740690</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>1152</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>201.5</td>\n",
       "      <td>204.724205</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>3.594286e+02</td>\n",
       "      <td>1.199802e+01</td>\n",
       "      <td>380</td>\n",
       "      <td>343</td>\n",
       "      <td>16100000.0</td>\n",
       "      <td>4.988048e+05</td>\n",
       "      <td>16400000</td>\n",
       "      <td>15400000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389</td>\n",
       "      <td>112740560</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>5056</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>201.5</td>\n",
       "      <td>204.724205</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>3.202857e+02</td>\n",
       "      <td>1.574499e+01</td>\n",
       "      <td>330</td>\n",
       "      <td>285</td>\n",
       "      <td>16100000.0</td>\n",
       "      <td>4.987937e+05</td>\n",
       "      <td>16400000</td>\n",
       "      <td>15400000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>113757377</td>\n",
       "      <td>545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.361829e+06</td>\n",
       "      <td>7.324646e+06</td>\n",
       "      <td>18900000</td>\n",
       "      <td>19</td>\n",
       "      <td>12200000.0</td>\n",
       "      <td>6.935824e+06</td>\n",
       "      <td>20800000</td>\n",
       "      <td>5504997</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5355</td>\n",
       "      <td>100126</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>54760</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   destination_port  flow_duration  total_fwd_packets  total_backward_packets  \\\n",
       "0              3268      112740690                 32                      16   \n",
       "1               389      112740560                 32                      16   \n",
       "2                 0      113757377                545                       0   \n",
       "3              5355         100126                 22                       0   \n",
       "4                 0          54760                  4                       0   \n",
       "\n",
       "   total_length_of_fwd_packets  total_length_of_bwd_packets  \\\n",
       "0                         6448                         1152   \n",
       "1                         6448                         5056   \n",
       "2                            0                            0   \n",
       "3                          616                            0   \n",
       "4                            0                            0   \n",
       "\n",
       "   fwd_packet_length_max  fwd_packet_length_min  fwd_packet_length_mean  \\\n",
       "0                    403                      0                   201.5   \n",
       "1                    403                      0                   201.5   \n",
       "2                      0                      0                     0.0   \n",
       "3                     28                     28                    28.0   \n",
       "4                      0                      0                     0.0   \n",
       "\n",
       "   fwd_packet_length_std  ...  min_seg_size_forward   active_mean  \\\n",
       "0             204.724205  ...                    32  3.594286e+02   \n",
       "1             204.724205  ...                    32  3.202857e+02   \n",
       "2               0.000000  ...                     0  9.361829e+06   \n",
       "3               0.000000  ...                    32  0.000000e+00   \n",
       "4               0.000000  ...                     0  0.000000e+00   \n",
       "\n",
       "     active_std  active_max  active_min   idle_mean      idle_std  idle_max  \\\n",
       "0  1.199802e+01         380         343  16100000.0  4.988048e+05  16400000   \n",
       "1  1.574499e+01         330         285  16100000.0  4.987937e+05  16400000   \n",
       "2  7.324646e+06    18900000          19  12200000.0  6.935824e+06  20800000   \n",
       "3  0.000000e+00           0           0         0.0  0.000000e+00         0   \n",
       "4  0.000000e+00           0           0         0.0  0.000000e+00         0   \n",
       "\n",
       "   idle_min   label  \n",
       "0  15400000  BENIGN  \n",
       "1  15400000  BENIGN  \n",
       "2   5504997  BENIGN  \n",
       "3         0  BENIGN  \n",
       "4         0  BENIGN  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the original data types\n",
    "Self-explanatory. Just shows the data types of each feature/ column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "destination_port                 int64\n",
       "flow_duration                    int64\n",
       "total_fwd_packets                int64\n",
       "total_backward_packets           int64\n",
       "total_length_of_fwd_packets      int64\n",
       "                                ...   \n",
       "idle_mean                      float64\n",
       "idle_std                       float64\n",
       "idle_max                         int64\n",
       "idle_min                         int64\n",
       "label                           object\n",
       "Length: 79, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing issues with ScikitLearn's PCA transform on this dataset\n",
    "\n",
    "Without cleaning the dataset, the PCA transform will throw this error: \n",
    "- \"sklearn error ValueError: Input contains NaN, infinity or a value too large for dtype('float64')\". \n",
    "\n",
    "It isn't obvious which attribute and/ or data point are causing this, as the input dataset is supposed to be fully clean with no Nan or erroneous values. Also, there are too many attributes to manually search through to check this too. Thus, a quick solution via stackoverflow was found to work (see the 'clean_dataset(df)' method at the top of the notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows have been removed by the cleaning, indicating that some rows did have issues/ errors within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination_port</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>fwd_packet_length_max</th>\n",
       "      <th>fwd_packet_length_min</th>\n",
       "      <th>fwd_packet_length_mean</th>\n",
       "      <th>fwd_packet_length_std</th>\n",
       "      <th>...</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3268</td>\n",
       "      <td>112740690</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>1152</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>201.5</td>\n",
       "      <td>204.724205</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>3.594286e+02</td>\n",
       "      <td>1.199802e+01</td>\n",
       "      <td>380</td>\n",
       "      <td>343</td>\n",
       "      <td>16100000.0</td>\n",
       "      <td>4.988048e+05</td>\n",
       "      <td>16400000</td>\n",
       "      <td>15400000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389</td>\n",
       "      <td>112740560</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>5056</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>201.5</td>\n",
       "      <td>204.724205</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>3.202857e+02</td>\n",
       "      <td>1.574499e+01</td>\n",
       "      <td>330</td>\n",
       "      <td>285</td>\n",
       "      <td>16100000.0</td>\n",
       "      <td>4.987937e+05</td>\n",
       "      <td>16400000</td>\n",
       "      <td>15400000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>113757377</td>\n",
       "      <td>545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.361829e+06</td>\n",
       "      <td>7.324646e+06</td>\n",
       "      <td>18900000</td>\n",
       "      <td>19</td>\n",
       "      <td>12200000.0</td>\n",
       "      <td>6.935824e+06</td>\n",
       "      <td>20800000</td>\n",
       "      <td>5504997</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5355</td>\n",
       "      <td>100126</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>54760</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191028</th>\n",
       "      <td>53</td>\n",
       "      <td>61452</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>354</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191029</th>\n",
       "      <td>53</td>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>272</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191030</th>\n",
       "      <td>53</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>354</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191031</th>\n",
       "      <td>123</td>\n",
       "      <td>16842</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191032</th>\n",
       "      <td>53</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>100</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190911 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        destination_port  flow_duration  total_fwd_packets  \\\n",
       "0                   3268      112740690                 32   \n",
       "1                    389      112740560                 32   \n",
       "2                      0      113757377                545   \n",
       "3                   5355         100126                 22   \n",
       "4                      0          54760                  4   \n",
       "...                  ...            ...                ...   \n",
       "191028                53          61452                  4   \n",
       "191029                53            171                  2   \n",
       "191030                53            222                  2   \n",
       "191031               123          16842                  1   \n",
       "191032                53            153                  2   \n",
       "\n",
       "        total_backward_packets  total_length_of_fwd_packets  \\\n",
       "0                           16                         6448   \n",
       "1                           16                         6448   \n",
       "2                            0                            0   \n",
       "3                            0                          616   \n",
       "4                            0                            0   \n",
       "...                        ...                          ...   \n",
       "191028                       2                          180   \n",
       "191029                       2                           80   \n",
       "191030                       2                           90   \n",
       "191031                       1                           48   \n",
       "191032                       2                           68   \n",
       "\n",
       "        total_length_of_bwd_packets  fwd_packet_length_max  \\\n",
       "0                              1152                    403   \n",
       "1                              5056                    403   \n",
       "2                                 0                      0   \n",
       "3                                 0                     28   \n",
       "4                                 0                      0   \n",
       "...                             ...                    ...   \n",
       "191028                          354                     45   \n",
       "191029                          272                     40   \n",
       "191030                          354                     45   \n",
       "191031                           48                     48   \n",
       "191032                          100                     34   \n",
       "\n",
       "        fwd_packet_length_min  fwd_packet_length_mean  fwd_packet_length_std  \\\n",
       "0                           0                   201.5             204.724205   \n",
       "1                           0                   201.5             204.724205   \n",
       "2                           0                     0.0               0.000000   \n",
       "3                          28                    28.0               0.000000   \n",
       "4                           0                     0.0               0.000000   \n",
       "...                       ...                     ...                    ...   \n",
       "191028                     45                    45.0               0.000000   \n",
       "191029                     40                    40.0               0.000000   \n",
       "191030                     45                    45.0               0.000000   \n",
       "191031                     48                    48.0               0.000000   \n",
       "191032                     34                    34.0               0.000000   \n",
       "\n",
       "        ...  min_seg_size_forward   active_mean    active_std  active_max  \\\n",
       "0       ...                    32  3.594286e+02  1.199802e+01         380   \n",
       "1       ...                    32  3.202857e+02  1.574499e+01         330   \n",
       "2       ...                     0  9.361829e+06  7.324646e+06    18900000   \n",
       "3       ...                    32  0.000000e+00  0.000000e+00           0   \n",
       "4       ...                     0  0.000000e+00  0.000000e+00           0   \n",
       "...     ...                   ...           ...           ...         ...   \n",
       "191028  ...                    20  0.000000e+00  0.000000e+00           0   \n",
       "191029  ...                    32  0.000000e+00  0.000000e+00           0   \n",
       "191030  ...                    32  0.000000e+00  0.000000e+00           0   \n",
       "191031  ...                    20  0.000000e+00  0.000000e+00           0   \n",
       "191032  ...                    20  0.000000e+00  0.000000e+00           0   \n",
       "\n",
       "        active_min   idle_mean      idle_std  idle_max  idle_min   label  \n",
       "0              343  16100000.0  4.988048e+05  16400000  15400000  BENIGN  \n",
       "1              285  16100000.0  4.987937e+05  16400000  15400000  BENIGN  \n",
       "2               19  12200000.0  6.935824e+06  20800000   5504997  BENIGN  \n",
       "3                0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "4                0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "...            ...         ...           ...       ...       ...     ...  \n",
       "191028           0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "191029           0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "191030           0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "191031           0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "191032           0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "\n",
       "[190911 rows x 79 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = df.copy()\n",
    "df_cleaned = clean_dataset(df_cleaned) # see methods at top of notebook\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resetting indexes since rows have been dropped. See the difference between above dataframe indexes and below.\n",
    "\n",
    "- **.reset_index()** method resets the Pandas Dataframe indexes, for the rows. Useful to do after removing rows, as this messes up the indexes. It creates a new 'index' column that needs to be dropped, as it's useless (see docs: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.reset_index.html)\n",
    "\n",
    "\n",
    "- **.drop([column_name], axis=x, inplace=y)** drops a specified column from the dataframe and returns a new copy. When **inplace=True**, it transforms the Dataframe itself (instead of needing to copy). The **axis** specifies the dimension of what to drop i.e. if **axis=0, it drops a row**. If **axis=1, it drops a column**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination_port</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>fwd_packet_length_max</th>\n",
       "      <th>fwd_packet_length_min</th>\n",
       "      <th>fwd_packet_length_mean</th>\n",
       "      <th>fwd_packet_length_std</th>\n",
       "      <th>...</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3268</td>\n",
       "      <td>112740690</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>1152</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>201.5</td>\n",
       "      <td>204.724205</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>3.594286e+02</td>\n",
       "      <td>1.199802e+01</td>\n",
       "      <td>380</td>\n",
       "      <td>343</td>\n",
       "      <td>16100000.0</td>\n",
       "      <td>4.988048e+05</td>\n",
       "      <td>16400000</td>\n",
       "      <td>15400000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389</td>\n",
       "      <td>112740560</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>5056</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>201.5</td>\n",
       "      <td>204.724205</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>3.202857e+02</td>\n",
       "      <td>1.574499e+01</td>\n",
       "      <td>330</td>\n",
       "      <td>285</td>\n",
       "      <td>16100000.0</td>\n",
       "      <td>4.987937e+05</td>\n",
       "      <td>16400000</td>\n",
       "      <td>15400000</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>113757377</td>\n",
       "      <td>545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9.361829e+06</td>\n",
       "      <td>7.324646e+06</td>\n",
       "      <td>18900000</td>\n",
       "      <td>19</td>\n",
       "      <td>12200000.0</td>\n",
       "      <td>6.935824e+06</td>\n",
       "      <td>20800000</td>\n",
       "      <td>5504997</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5355</td>\n",
       "      <td>100126</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>54760</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190906</th>\n",
       "      <td>53</td>\n",
       "      <td>61452</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>354</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190907</th>\n",
       "      <td>53</td>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>272</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190908</th>\n",
       "      <td>53</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>354</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190909</th>\n",
       "      <td>123</td>\n",
       "      <td>16842</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190910</th>\n",
       "      <td>53</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>100</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190911 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        destination_port  flow_duration  total_fwd_packets  \\\n",
       "0                   3268      112740690                 32   \n",
       "1                    389      112740560                 32   \n",
       "2                      0      113757377                545   \n",
       "3                   5355         100126                 22   \n",
       "4                      0          54760                  4   \n",
       "...                  ...            ...                ...   \n",
       "190906                53          61452                  4   \n",
       "190907                53            171                  2   \n",
       "190908                53            222                  2   \n",
       "190909               123          16842                  1   \n",
       "190910                53            153                  2   \n",
       "\n",
       "        total_backward_packets  total_length_of_fwd_packets  \\\n",
       "0                           16                         6448   \n",
       "1                           16                         6448   \n",
       "2                            0                            0   \n",
       "3                            0                          616   \n",
       "4                            0                            0   \n",
       "...                        ...                          ...   \n",
       "190906                       2                          180   \n",
       "190907                       2                           80   \n",
       "190908                       2                           90   \n",
       "190909                       1                           48   \n",
       "190910                       2                           68   \n",
       "\n",
       "        total_length_of_bwd_packets  fwd_packet_length_max  \\\n",
       "0                              1152                    403   \n",
       "1                              5056                    403   \n",
       "2                                 0                      0   \n",
       "3                                 0                     28   \n",
       "4                                 0                      0   \n",
       "...                             ...                    ...   \n",
       "190906                          354                     45   \n",
       "190907                          272                     40   \n",
       "190908                          354                     45   \n",
       "190909                           48                     48   \n",
       "190910                          100                     34   \n",
       "\n",
       "        fwd_packet_length_min  fwd_packet_length_mean  fwd_packet_length_std  \\\n",
       "0                           0                   201.5             204.724205   \n",
       "1                           0                   201.5             204.724205   \n",
       "2                           0                     0.0               0.000000   \n",
       "3                          28                    28.0               0.000000   \n",
       "4                           0                     0.0               0.000000   \n",
       "...                       ...                     ...                    ...   \n",
       "190906                     45                    45.0               0.000000   \n",
       "190907                     40                    40.0               0.000000   \n",
       "190908                     45                    45.0               0.000000   \n",
       "190909                     48                    48.0               0.000000   \n",
       "190910                     34                    34.0               0.000000   \n",
       "\n",
       "        ...  min_seg_size_forward   active_mean    active_std  active_max  \\\n",
       "0       ...                    32  3.594286e+02  1.199802e+01         380   \n",
       "1       ...                    32  3.202857e+02  1.574499e+01         330   \n",
       "2       ...                     0  9.361829e+06  7.324646e+06    18900000   \n",
       "3       ...                    32  0.000000e+00  0.000000e+00           0   \n",
       "4       ...                     0  0.000000e+00  0.000000e+00           0   \n",
       "...     ...                   ...           ...           ...         ...   \n",
       "190906  ...                    20  0.000000e+00  0.000000e+00           0   \n",
       "190907  ...                    32  0.000000e+00  0.000000e+00           0   \n",
       "190908  ...                    32  0.000000e+00  0.000000e+00           0   \n",
       "190909  ...                    20  0.000000e+00  0.000000e+00           0   \n",
       "190910  ...                    20  0.000000e+00  0.000000e+00           0   \n",
       "\n",
       "        active_min   idle_mean      idle_std  idle_max  idle_min   label  \n",
       "0              343  16100000.0  4.988048e+05  16400000  15400000  BENIGN  \n",
       "1              285  16100000.0  4.987937e+05  16400000  15400000  BENIGN  \n",
       "2               19  12200000.0  6.935824e+06  20800000   5504997  BENIGN  \n",
       "3                0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "4                0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "...            ...         ...           ...       ...       ...     ...  \n",
       "190906           0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "190907           0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "190908           0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "190909           0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "190910           0         0.0  0.000000e+00         0         0  BENIGN  \n",
       "\n",
       "[190911 rows x 79 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = df_cleaned.reset_index()\n",
    "# Removing un-needed index column added by reset_index method\n",
    "df_cleaned.drop('index', axis=1, inplace=True)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations before PCA can be used correctly (before Data Preparation feature selection via PCA)\n",
    "Looking at this resource and many others (https://towardsdatascience.com/pca-is-not-feature-selection-3344fb764ae6), it can be seen that PCA can, quite easily, be used incorrectly without proper consideration and/ or understanding.\n",
    "\n",
    "From the resource:\n",
    "- \"A common mistake new data scientists make is to apply PCA to non-continuous variables. While it is technically possible to use PCA on discrete variables, or categorical variables that have been one hot encoded variables, you should not. Simply put, if your variables don’t belong on a coordinate plane, then do not apply PCA to them\"\n",
    "\n",
    "Thus, PCA should **only** be applied to the numeric features- which **must** be scaled down to unit scale.\n",
    "\n",
    "### What features should be included from PCA, and why?\n",
    "\n",
    "Looking at the list of feature names in the dataset (shown below), one can see that all other features should be of numeric type (with domain knowledge). They're all currently numeric type (either float or int). Consequently, PCA **can be** fully applied after scaling them all to unit scale.\n",
    "\n",
    "- **df.columns.tolist()** converts the Dataframe column names into a Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['destination_port',\n",
       " 'flow_duration',\n",
       " 'total_fwd_packets',\n",
       " 'total_backward_packets',\n",
       " 'total_length_of_fwd_packets',\n",
       " 'total_length_of_bwd_packets',\n",
       " 'fwd_packet_length_max',\n",
       " 'fwd_packet_length_min',\n",
       " 'fwd_packet_length_mean',\n",
       " 'fwd_packet_length_std',\n",
       " 'bwd_packet_length_max',\n",
       " 'bwd_packet_length_min',\n",
       " 'bwd_packet_length_mean',\n",
       " 'bwd_packet_length_std',\n",
       " 'flow_bytes/s',\n",
       " 'flow_packets/s',\n",
       " 'flow_iat_mean',\n",
       " 'flow_iat_std',\n",
       " 'flow_iat_max',\n",
       " 'flow_iat_min',\n",
       " 'fwd_iat_total',\n",
       " 'fwd_iat_mean',\n",
       " 'fwd_iat_std',\n",
       " 'fwd_iat_max',\n",
       " 'fwd_iat_min',\n",
       " 'bwd_iat_total',\n",
       " 'bwd_iat_mean',\n",
       " 'bwd_iat_std',\n",
       " 'bwd_iat_max',\n",
       " 'bwd_iat_min',\n",
       " 'fwd_psh_flags',\n",
       " 'bwd_psh_flags',\n",
       " 'fwd_urg_flags',\n",
       " 'bwd_urg_flags',\n",
       " 'fwd_header_length',\n",
       " 'bwd_header_length',\n",
       " 'fwd_packets/s',\n",
       " 'bwd_packets/s',\n",
       " 'min_packet_length',\n",
       " 'max_packet_length',\n",
       " 'packet_length_mean',\n",
       " 'packet_length_std',\n",
       " 'packet_length_variance',\n",
       " 'fin_flag_count',\n",
       " 'syn_flag_count',\n",
       " 'rst_flag_count',\n",
       " 'psh_flag_count',\n",
       " 'ack_flag_count',\n",
       " 'urg_flag_count',\n",
       " 'cwe_flag_count',\n",
       " 'ece_flag_count',\n",
       " 'down/up_ratio',\n",
       " 'average_packet_size',\n",
       " 'avg_fwd_segment_size',\n",
       " 'avg_bwd_segment_size',\n",
       " 'fwd_header_length.1',\n",
       " 'fwd_avg_bytes/bulk',\n",
       " 'fwd_avg_packets/bulk',\n",
       " 'fwd_avg_bulk_rate',\n",
       " 'bwd_avg_bytes/bulk',\n",
       " 'bwd_avg_packets/bulk',\n",
       " 'bwd_avg_bulk_rate',\n",
       " 'subflow_fwd_packets',\n",
       " 'subflow_fwd_bytes',\n",
       " 'subflow_bwd_packets',\n",
       " 'subflow_bwd_bytes',\n",
       " 'init_win_bytes_forward',\n",
       " 'init_win_bytes_backward',\n",
       " 'act_data_pkt_fwd',\n",
       " 'min_seg_size_forward',\n",
       " 'active_mean',\n",
       " 'active_std',\n",
       " 'active_max',\n",
       " 'active_min',\n",
       " 'idle_mean',\n",
       " 'idle_std',\n",
       " 'idle_max',\n",
       " 'idle_min',\n",
       " 'label']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation: PCA Dimension reduction and scaling (Hughes' Phenomenon)\n",
    "\n",
    "PCA acts to reduce the dimensions/ search space of the dataset as much as possible, while trying to maintain the most information possible e.g. It can easily reduce the dimensionality by more than half, while still maintaining 99% of the original data's information- it does this by extracting out the most important information/ trends/ spread (variance) of each dimension/ attribute- into n 'principal components'.\n",
    "\n",
    "More formally: PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance.\n",
    "\n",
    "##### *Key note:* \n",
    "\"**PCA centers but does not scale the input data** for each feature before applying the SVD. The optional parameter whiten=True makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for **Support Vector Machines with the RBF kernel** and the **K-Means clustering algorithm**.\" (https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "\n",
    "PCA still works without standardizing the features to unit scale **but tranforming to unit scale should still be done** to prevent large variance features from having an over-bearing affect on other lower variance features (via something like StandardScaler here https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). \n",
    "\n",
    "This is **particularly important with this dataset**, as some features have massively wide variances and others do not (e.g. the 'idle_std' values can range from e+06, all the way to zero).\n",
    "\n",
    "- **Dataframe['x']** syntax allows the selection of one of the Dataframe columns (where 'x' is the column heading/ name).\n",
    "\n",
    "\n",
    "- **df_Series.unique()** returns an array, showing all the unique values inside the Pandas Series object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BENIGN', 'Bot'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the label attribute before dropping it.\n",
    "df_labels = df_cleaned['label']\n",
    "# Shows all the possible labels/ classes a model can predict.\n",
    "# Need to alter these to numeric 0, 1, etc... for model comprehension (e.g. pd.get_dummies()).\n",
    "df_labels.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label column has to be removed as you wouldn't want this involved in the PCA process. It can be concatted back with the PCA tranformed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination_port</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>fwd_packet_length_max</th>\n",
       "      <th>fwd_packet_length_min</th>\n",
       "      <th>fwd_packet_length_mean</th>\n",
       "      <th>fwd_packet_length_std</th>\n",
       "      <th>...</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3268</td>\n",
       "      <td>112740690</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>1152</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>201.5</td>\n",
       "      <td>204.724205</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>3.594286e+02</td>\n",
       "      <td>1.199802e+01</td>\n",
       "      <td>380</td>\n",
       "      <td>343</td>\n",
       "      <td>16100000.0</td>\n",
       "      <td>4.988048e+05</td>\n",
       "      <td>16400000</td>\n",
       "      <td>15400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>389</td>\n",
       "      <td>112740560</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>6448</td>\n",
       "      <td>5056</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "      <td>201.5</td>\n",
       "      <td>204.724205</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>3.202857e+02</td>\n",
       "      <td>1.574499e+01</td>\n",
       "      <td>330</td>\n",
       "      <td>285</td>\n",
       "      <td>16100000.0</td>\n",
       "      <td>4.987937e+05</td>\n",
       "      <td>16400000</td>\n",
       "      <td>15400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>113757377</td>\n",
       "      <td>545</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.361829e+06</td>\n",
       "      <td>7.324646e+06</td>\n",
       "      <td>18900000</td>\n",
       "      <td>19</td>\n",
       "      <td>12200000.0</td>\n",
       "      <td>6.935824e+06</td>\n",
       "      <td>20800000</td>\n",
       "      <td>5504997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5355</td>\n",
       "      <td>100126</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>616</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>54760</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190906</th>\n",
       "      <td>53</td>\n",
       "      <td>61452</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "      <td>354</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190907</th>\n",
       "      <td>53</td>\n",
       "      <td>171</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>272</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190908</th>\n",
       "      <td>53</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>354</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190909</th>\n",
       "      <td>123</td>\n",
       "      <td>16842</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190910</th>\n",
       "      <td>53</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>100</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190911 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        destination_port  flow_duration  total_fwd_packets  \\\n",
       "0                   3268      112740690                 32   \n",
       "1                    389      112740560                 32   \n",
       "2                      0      113757377                545   \n",
       "3                   5355         100126                 22   \n",
       "4                      0          54760                  4   \n",
       "...                  ...            ...                ...   \n",
       "190906                53          61452                  4   \n",
       "190907                53            171                  2   \n",
       "190908                53            222                  2   \n",
       "190909               123          16842                  1   \n",
       "190910                53            153                  2   \n",
       "\n",
       "        total_backward_packets  total_length_of_fwd_packets  \\\n",
       "0                           16                         6448   \n",
       "1                           16                         6448   \n",
       "2                            0                            0   \n",
       "3                            0                          616   \n",
       "4                            0                            0   \n",
       "...                        ...                          ...   \n",
       "190906                       2                          180   \n",
       "190907                       2                           80   \n",
       "190908                       2                           90   \n",
       "190909                       1                           48   \n",
       "190910                       2                           68   \n",
       "\n",
       "        total_length_of_bwd_packets  fwd_packet_length_max  \\\n",
       "0                              1152                    403   \n",
       "1                              5056                    403   \n",
       "2                                 0                      0   \n",
       "3                                 0                     28   \n",
       "4                                 0                      0   \n",
       "...                             ...                    ...   \n",
       "190906                          354                     45   \n",
       "190907                          272                     40   \n",
       "190908                          354                     45   \n",
       "190909                           48                     48   \n",
       "190910                          100                     34   \n",
       "\n",
       "        fwd_packet_length_min  fwd_packet_length_mean  fwd_packet_length_std  \\\n",
       "0                           0                   201.5             204.724205   \n",
       "1                           0                   201.5             204.724205   \n",
       "2                           0                     0.0               0.000000   \n",
       "3                          28                    28.0               0.000000   \n",
       "4                           0                     0.0               0.000000   \n",
       "...                       ...                     ...                    ...   \n",
       "190906                     45                    45.0               0.000000   \n",
       "190907                     40                    40.0               0.000000   \n",
       "190908                     45                    45.0               0.000000   \n",
       "190909                     48                    48.0               0.000000   \n",
       "190910                     34                    34.0               0.000000   \n",
       "\n",
       "        ...  act_data_pkt_fwd  min_seg_size_forward   active_mean  \\\n",
       "0       ...                15                    32  3.594286e+02   \n",
       "1       ...                15                    32  3.202857e+02   \n",
       "2       ...                 0                     0  9.361829e+06   \n",
       "3       ...                21                    32  0.000000e+00   \n",
       "4       ...                 0                     0  0.000000e+00   \n",
       "...     ...               ...                   ...           ...   \n",
       "190906  ...                 3                    20  0.000000e+00   \n",
       "190907  ...                 1                    32  0.000000e+00   \n",
       "190908  ...                 1                    32  0.000000e+00   \n",
       "190909  ...                 0                    20  0.000000e+00   \n",
       "190910  ...                 1                    20  0.000000e+00   \n",
       "\n",
       "          active_std  active_max  active_min   idle_mean      idle_std  \\\n",
       "0       1.199802e+01         380         343  16100000.0  4.988048e+05   \n",
       "1       1.574499e+01         330         285  16100000.0  4.987937e+05   \n",
       "2       7.324646e+06    18900000          19  12200000.0  6.935824e+06   \n",
       "3       0.000000e+00           0           0         0.0  0.000000e+00   \n",
       "4       0.000000e+00           0           0         0.0  0.000000e+00   \n",
       "...              ...         ...         ...         ...           ...   \n",
       "190906  0.000000e+00           0           0         0.0  0.000000e+00   \n",
       "190907  0.000000e+00           0           0         0.0  0.000000e+00   \n",
       "190908  0.000000e+00           0           0         0.0  0.000000e+00   \n",
       "190909  0.000000e+00           0           0         0.0  0.000000e+00   \n",
       "190910  0.000000e+00           0           0         0.0  0.000000e+00   \n",
       "\n",
       "        idle_max  idle_min  \n",
       "0       16400000  15400000  \n",
       "1       16400000  15400000  \n",
       "2       20800000   5504997  \n",
       "3              0         0  \n",
       "4              0         0  \n",
       "...          ...       ...  \n",
       "190906         0         0  \n",
       "190907         0         0  \n",
       "190908         0         0  \n",
       "190909         0         0  \n",
       "190910         0         0  \n",
       "\n",
       "[190911 rows x 78 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Axis=1 means columns. Axis=0 means rows. inplace=False means that the original 'df' isn't altered.\n",
    "df_no_labels = df_cleaned.drop('label', axis=1, inplace=False)\n",
    "# Getting feature names for the StandardScaler process\n",
    "df_features = df_no_labels.columns.tolist()\n",
    "# Printing out Dataframe with no label column, to show successful dropping\n",
    "df_no_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using StandardScaler to transform features into unit scale, ready for PCA\n",
    "- **StandardScaler()** is an imported model from the sklearn.preprocessing library. It scales the specified Pandas Dataframe or Series object values to unit scale/ variance. This is usually required for certain functions to perform correctly, e.g. the PCA transform later (see docs: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "\n",
    "- StandardScaler().**fit_transform(df)** fits the StandardScaler model to the data, and transforms it into unit scale.\n",
    "\n",
    "\n",
    "- **pd.Dataframe(data=x, columns=y)** can convert the data 'x' into a Pandas Dataframe object, using the respective columns 'y'\n",
    "\n",
    "Code references: https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>destination_port</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>total_fwd_packets</th>\n",
       "      <th>total_backward_packets</th>\n",
       "      <th>total_length_of_fwd_packets</th>\n",
       "      <th>total_length_of_bwd_packets</th>\n",
       "      <th>fwd_packet_length_max</th>\n",
       "      <th>fwd_packet_length_min</th>\n",
       "      <th>fwd_packet_length_mean</th>\n",
       "      <th>fwd_packet_length_std</th>\n",
       "      <th>...</th>\n",
       "      <th>act_data_pkt_fwd</th>\n",
       "      <th>min_seg_size_forward</th>\n",
       "      <th>active_mean</th>\n",
       "      <th>active_std</th>\n",
       "      <th>active_max</th>\n",
       "      <th>active_min</th>\n",
       "      <th>idle_mean</th>\n",
       "      <th>idle_std</th>\n",
       "      <th>idle_max</th>\n",
       "      <th>idle_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.208443</td>\n",
       "      <td>3.291795</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>0.737720</td>\n",
       "      <td>-0.008219</td>\n",
       "      <td>0.411408</td>\n",
       "      <td>-0.570375</td>\n",
       "      <td>1.277916</td>\n",
       "      <td>0.962640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004463</td>\n",
       "      <td>1.005074</td>\n",
       "      <td>-0.098639</td>\n",
       "      <td>-0.109209</td>\n",
       "      <td>-0.137345</td>\n",
       "      <td>-0.072847</td>\n",
       "      <td>0.877130</td>\n",
       "      <td>0.149265</td>\n",
       "      <td>0.866895</td>\n",
       "      <td>0.849955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.380963</td>\n",
       "      <td>3.291791</td>\n",
       "      <td>0.016543</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>0.737720</td>\n",
       "      <td>-0.007042</td>\n",
       "      <td>0.411408</td>\n",
       "      <td>-0.570375</td>\n",
       "      <td>1.277916</td>\n",
       "      <td>0.962640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004463</td>\n",
       "      <td>1.005074</td>\n",
       "      <td>-0.098684</td>\n",
       "      <td>-0.109202</td>\n",
       "      <td>-0.137382</td>\n",
       "      <td>-0.072923</td>\n",
       "      <td>0.877130</td>\n",
       "      <td>0.149260</td>\n",
       "      <td>0.866895</td>\n",
       "      <td>0.849955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.404273</td>\n",
       "      <td>3.324902</td>\n",
       "      <td>0.483712</td>\n",
       "      <td>-0.011095</td>\n",
       "      <td>-0.075734</td>\n",
       "      <td>-0.008567</td>\n",
       "      <td>-0.315197</td>\n",
       "      <td>-0.570375</td>\n",
       "      <td>-0.443399</td>\n",
       "      <td>-0.312913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009872</td>\n",
       "      <td>-4.009835</td>\n",
       "      <td>10.681442</td>\n",
       "      <td>14.744955</td>\n",
       "      <td>14.028865</td>\n",
       "      <td>-0.073272</td>\n",
       "      <td>0.596286</td>\n",
       "      <td>3.118198</td>\n",
       "      <td>1.175767</td>\n",
       "      <td>0.128155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.083383</td>\n",
       "      <td>-0.376184</td>\n",
       "      <td>0.007436</td>\n",
       "      <td>-0.011095</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>-0.008567</td>\n",
       "      <td>-0.264713</td>\n",
       "      <td>0.097560</td>\n",
       "      <td>-0.204209</td>\n",
       "      <td>-0.312913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010197</td>\n",
       "      <td>1.005074</td>\n",
       "      <td>-0.099053</td>\n",
       "      <td>-0.109234</td>\n",
       "      <td>-0.137630</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-0.282251</td>\n",
       "      <td>-0.080798</td>\n",
       "      <td>-0.284355</td>\n",
       "      <td>-0.273412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.404273</td>\n",
       "      <td>-0.377661</td>\n",
       "      <td>-0.008956</td>\n",
       "      <td>-0.011095</td>\n",
       "      <td>-0.075734</td>\n",
       "      <td>-0.008567</td>\n",
       "      <td>-0.315197</td>\n",
       "      <td>-0.570375</td>\n",
       "      <td>-0.443399</td>\n",
       "      <td>-0.312913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009872</td>\n",
       "      <td>-4.009835</td>\n",
       "      <td>-0.099053</td>\n",
       "      <td>-0.109234</td>\n",
       "      <td>-0.137630</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-0.282251</td>\n",
       "      <td>-0.080798</td>\n",
       "      <td>-0.284355</td>\n",
       "      <td>-0.273412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190906</th>\n",
       "      <td>-0.401097</td>\n",
       "      <td>-0.377444</td>\n",
       "      <td>-0.008956</td>\n",
       "      <td>-0.009744</td>\n",
       "      <td>-0.053026</td>\n",
       "      <td>-0.008460</td>\n",
       "      <td>-0.234062</td>\n",
       "      <td>0.503093</td>\n",
       "      <td>-0.058986</td>\n",
       "      <td>-0.312913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007005</td>\n",
       "      <td>-0.875517</td>\n",
       "      <td>-0.099053</td>\n",
       "      <td>-0.109234</td>\n",
       "      <td>-0.137630</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-0.282251</td>\n",
       "      <td>-0.080798</td>\n",
       "      <td>-0.284355</td>\n",
       "      <td>-0.273412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190907</th>\n",
       "      <td>-0.401097</td>\n",
       "      <td>-0.379439</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>-0.009744</td>\n",
       "      <td>-0.065642</td>\n",
       "      <td>-0.008485</td>\n",
       "      <td>-0.243077</td>\n",
       "      <td>0.383819</td>\n",
       "      <td>-0.101699</td>\n",
       "      <td>-0.312913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008917</td>\n",
       "      <td>1.005074</td>\n",
       "      <td>-0.099053</td>\n",
       "      <td>-0.109234</td>\n",
       "      <td>-0.137630</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-0.282251</td>\n",
       "      <td>-0.080798</td>\n",
       "      <td>-0.284355</td>\n",
       "      <td>-0.273412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190908</th>\n",
       "      <td>-0.401097</td>\n",
       "      <td>-0.379437</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>-0.009744</td>\n",
       "      <td>-0.064380</td>\n",
       "      <td>-0.008460</td>\n",
       "      <td>-0.234062</td>\n",
       "      <td>0.503093</td>\n",
       "      <td>-0.058986</td>\n",
       "      <td>-0.312913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008917</td>\n",
       "      <td>1.005074</td>\n",
       "      <td>-0.099053</td>\n",
       "      <td>-0.109234</td>\n",
       "      <td>-0.137630</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-0.282251</td>\n",
       "      <td>-0.080798</td>\n",
       "      <td>-0.284355</td>\n",
       "      <td>-0.273412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190909</th>\n",
       "      <td>-0.396902</td>\n",
       "      <td>-0.378896</td>\n",
       "      <td>-0.011688</td>\n",
       "      <td>-0.010420</td>\n",
       "      <td>-0.069679</td>\n",
       "      <td>-0.008552</td>\n",
       "      <td>-0.228654</td>\n",
       "      <td>0.574657</td>\n",
       "      <td>-0.033359</td>\n",
       "      <td>-0.312913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009872</td>\n",
       "      <td>-0.875517</td>\n",
       "      <td>-0.099053</td>\n",
       "      <td>-0.109234</td>\n",
       "      <td>-0.137630</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-0.282251</td>\n",
       "      <td>-0.080798</td>\n",
       "      <td>-0.284355</td>\n",
       "      <td>-0.273412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190910</th>\n",
       "      <td>-0.401097</td>\n",
       "      <td>-0.379440</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>-0.009744</td>\n",
       "      <td>-0.067155</td>\n",
       "      <td>-0.008537</td>\n",
       "      <td>-0.253895</td>\n",
       "      <td>0.240690</td>\n",
       "      <td>-0.152954</td>\n",
       "      <td>-0.312913</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008917</td>\n",
       "      <td>-0.875517</td>\n",
       "      <td>-0.099053</td>\n",
       "      <td>-0.109234</td>\n",
       "      <td>-0.137630</td>\n",
       "      <td>-0.073297</td>\n",
       "      <td>-0.282251</td>\n",
       "      <td>-0.080798</td>\n",
       "      <td>-0.284355</td>\n",
       "      <td>-0.273412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190911 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        destination_port  flow_duration  total_fwd_packets  \\\n",
       "0              -0.208443       3.291795           0.016543   \n",
       "1              -0.380963       3.291791           0.016543   \n",
       "2              -0.404273       3.324902           0.483712   \n",
       "3              -0.083383      -0.376184           0.007436   \n",
       "4              -0.404273      -0.377661          -0.008956   \n",
       "...                  ...            ...                ...   \n",
       "190906         -0.401097      -0.377444          -0.008956   \n",
       "190907         -0.401097      -0.379439          -0.010777   \n",
       "190908         -0.401097      -0.379437          -0.010777   \n",
       "190909         -0.396902      -0.378896          -0.011688   \n",
       "190910         -0.401097      -0.379440          -0.010777   \n",
       "\n",
       "        total_backward_packets  total_length_of_fwd_packets  \\\n",
       "0                    -0.000286                     0.737720   \n",
       "1                    -0.000286                     0.737720   \n",
       "2                    -0.011095                    -0.075734   \n",
       "3                    -0.011095                     0.001978   \n",
       "4                    -0.011095                    -0.075734   \n",
       "...                        ...                          ...   \n",
       "190906               -0.009744                    -0.053026   \n",
       "190907               -0.009744                    -0.065642   \n",
       "190908               -0.009744                    -0.064380   \n",
       "190909               -0.010420                    -0.069679   \n",
       "190910               -0.009744                    -0.067155   \n",
       "\n",
       "        total_length_of_bwd_packets  fwd_packet_length_max  \\\n",
       "0                         -0.008219               0.411408   \n",
       "1                         -0.007042               0.411408   \n",
       "2                         -0.008567              -0.315197   \n",
       "3                         -0.008567              -0.264713   \n",
       "4                         -0.008567              -0.315197   \n",
       "...                             ...                    ...   \n",
       "190906                    -0.008460              -0.234062   \n",
       "190907                    -0.008485              -0.243077   \n",
       "190908                    -0.008460              -0.234062   \n",
       "190909                    -0.008552              -0.228654   \n",
       "190910                    -0.008537              -0.253895   \n",
       "\n",
       "        fwd_packet_length_min  fwd_packet_length_mean  fwd_packet_length_std  \\\n",
       "0                   -0.570375                1.277916               0.962640   \n",
       "1                   -0.570375                1.277916               0.962640   \n",
       "2                   -0.570375               -0.443399              -0.312913   \n",
       "3                    0.097560               -0.204209              -0.312913   \n",
       "4                   -0.570375               -0.443399              -0.312913   \n",
       "...                       ...                     ...                    ...   \n",
       "190906               0.503093               -0.058986              -0.312913   \n",
       "190907               0.383819               -0.101699              -0.312913   \n",
       "190908               0.503093               -0.058986              -0.312913   \n",
       "190909               0.574657               -0.033359              -0.312913   \n",
       "190910               0.240690               -0.152954              -0.312913   \n",
       "\n",
       "        ...  act_data_pkt_fwd  min_seg_size_forward  active_mean  active_std  \\\n",
       "0       ...          0.004463              1.005074    -0.098639   -0.109209   \n",
       "1       ...          0.004463              1.005074    -0.098684   -0.109202   \n",
       "2       ...         -0.009872             -4.009835    10.681442   14.744955   \n",
       "3       ...          0.010197              1.005074    -0.099053   -0.109234   \n",
       "4       ...         -0.009872             -4.009835    -0.099053   -0.109234   \n",
       "...     ...               ...                   ...          ...         ...   \n",
       "190906  ...         -0.007005             -0.875517    -0.099053   -0.109234   \n",
       "190907  ...         -0.008917              1.005074    -0.099053   -0.109234   \n",
       "190908  ...         -0.008917              1.005074    -0.099053   -0.109234   \n",
       "190909  ...         -0.009872             -0.875517    -0.099053   -0.109234   \n",
       "190910  ...         -0.008917             -0.875517    -0.099053   -0.109234   \n",
       "\n",
       "        active_max  active_min  idle_mean  idle_std  idle_max  idle_min  \n",
       "0        -0.137345   -0.072847   0.877130  0.149265  0.866895  0.849955  \n",
       "1        -0.137382   -0.072923   0.877130  0.149260  0.866895  0.849955  \n",
       "2        14.028865   -0.073272   0.596286  3.118198  1.175767  0.128155  \n",
       "3        -0.137630   -0.073297  -0.282251 -0.080798 -0.284355 -0.273412  \n",
       "4        -0.137630   -0.073297  -0.282251 -0.080798 -0.284355 -0.273412  \n",
       "...            ...         ...        ...       ...       ...       ...  \n",
       "190906   -0.137630   -0.073297  -0.282251 -0.080798 -0.284355 -0.273412  \n",
       "190907   -0.137630   -0.073297  -0.282251 -0.080798 -0.284355 -0.273412  \n",
       "190908   -0.137630   -0.073297  -0.282251 -0.080798 -0.284355 -0.273412  \n",
       "190909   -0.137630   -0.073297  -0.282251 -0.080798 -0.284355 -0.273412  \n",
       "190910   -0.137630   -0.073297  -0.282251 -0.080798 -0.284355 -0.273412  \n",
       "\n",
       "[190911 rows x 78 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled = StandardScaler().fit_transform(df_no_labels)\n",
    "# Converting back to dataframe\n",
    "df_scaled = pd.DataFrame(data = df_scaled, columns = df_features)\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting principle component variance\n",
    "\n",
    "A scree plot displays the variance explained by each principal component within the analysis.\n",
    "\n",
    "**The plot below shows that using the first 30 PCA components actually describes most/ all of the variation (information) within the original data. This is a huge dimension reduction from the initial 78 features, down to just 30.**\n",
    "\n",
    "Thus, looking at the Environment Variables (at the top of the notebook), the 'dimensions_num_for_PCA' variable will be set to **30** based upon this evidence.\n",
    "\n",
    "(Code reference: https://medium.com/district-data-labs/principal-component-analysis-with-python-4962cd026465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca_test = PCA().fit(df_scaled)\n",
    "plt.plot(np.cumsum(pca_test.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now fitting and transforming the data with PCA\n",
    "\n",
    "Thus, the optimal number of principle components is set to the environment variable and this is now used to produce the appropriate multi-dimensional principle component array. This will be formatted back to a Pandas dataframe afterwards.\n",
    "\n",
    "References: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html and https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.09274929e+00, -1.03935146e+00,  1.26069052e+00, ...,\n",
       "         4.05336204e-02,  1.06574237e+00, -2.21164772e-01],\n",
       "       [ 4.44413610e+00, -9.90881808e-01,  7.66886528e-01, ...,\n",
       "         2.40953745e-02,  1.56627205e+00,  5.01799885e-01],\n",
       "       [ 4.18433641e+00, -6.83524722e-01,  8.51537189e-03, ...,\n",
       "        -2.57007647e-01, -1.82334181e+00, -1.03678262e-03],\n",
       "       ...,\n",
       "       [-1.41932208e+00,  3.17637366e-01,  8.22303595e-02, ...,\n",
       "        -1.62401809e-02,  7.54705563e-02,  1.69020295e-01],\n",
       "       [-1.64998846e+00,  2.66933665e-01,  4.67961193e-01, ...,\n",
       "         5.60165904e-03, -2.05599506e-01, -2.95715483e-01],\n",
       "       [-1.67577535e+00,  2.66191192e-01,  4.96849568e-01, ...,\n",
       "         4.02769175e-02, -9.89530797e-02, -1.37579419e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=dimensions_num_for_PCA)\n",
    "principal_components = pca.fit(df_scaled).transform(df_scaled)\n",
    "principal_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Principal Component feature names, dynamically, for the optimal number of components (passed in as a param)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Methods at the top of the notebook\n",
    "principal_component_headings = get_PCA_feature_names(dimensions_num_for_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the Principal Components back into a Pandas Dataframe, ready for concatting back with the **label** feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal component 1</th>\n",
       "      <th>Principal component 2</th>\n",
       "      <th>Principal component 3</th>\n",
       "      <th>Principal component 4</th>\n",
       "      <th>Principal component 5</th>\n",
       "      <th>Principal component 6</th>\n",
       "      <th>Principal component 7</th>\n",
       "      <th>Principal component 8</th>\n",
       "      <th>Principal component 9</th>\n",
       "      <th>Principal component 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Principal component 21</th>\n",
       "      <th>Principal component 22</th>\n",
       "      <th>Principal component 23</th>\n",
       "      <th>Principal component 24</th>\n",
       "      <th>Principal component 25</th>\n",
       "      <th>Principal component 26</th>\n",
       "      <th>Principal component 27</th>\n",
       "      <th>Principal component 28</th>\n",
       "      <th>Principal component 29</th>\n",
       "      <th>Principal component 30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.092749</td>\n",
       "      <td>-1.039351</td>\n",
       "      <td>1.260691</td>\n",
       "      <td>-4.038849</td>\n",
       "      <td>1.985634</td>\n",
       "      <td>1.064285</td>\n",
       "      <td>0.626387</td>\n",
       "      <td>-4.319053</td>\n",
       "      <td>-0.378186</td>\n",
       "      <td>-2.292077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442761</td>\n",
       "      <td>-0.674434</td>\n",
       "      <td>0.700384</td>\n",
       "      <td>0.538268</td>\n",
       "      <td>-0.173432</td>\n",
       "      <td>-0.348808</td>\n",
       "      <td>-1.069114</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>1.065742</td>\n",
       "      <td>-0.221165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.444136</td>\n",
       "      <td>-0.990882</td>\n",
       "      <td>0.766887</td>\n",
       "      <td>-2.737491</td>\n",
       "      <td>1.758523</td>\n",
       "      <td>1.036710</td>\n",
       "      <td>1.748314</td>\n",
       "      <td>-4.440705</td>\n",
       "      <td>-0.321942</td>\n",
       "      <td>-2.087103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161841</td>\n",
       "      <td>-2.012189</td>\n",
       "      <td>2.037390</td>\n",
       "      <td>-0.565709</td>\n",
       "      <td>-1.112169</td>\n",
       "      <td>-0.363729</td>\n",
       "      <td>-1.425324</td>\n",
       "      <td>0.024095</td>\n",
       "      <td>1.566272</td>\n",
       "      <td>0.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.184336</td>\n",
       "      <td>-0.683525</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>-4.374319</td>\n",
       "      <td>-0.862230</td>\n",
       "      <td>18.046185</td>\n",
       "      <td>2.732318</td>\n",
       "      <td>4.997712</td>\n",
       "      <td>0.845690</td>\n",
       "      <td>-1.541559</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.289033</td>\n",
       "      <td>0.236197</td>\n",
       "      <td>-0.113712</td>\n",
       "      <td>-0.441430</td>\n",
       "      <td>0.408279</td>\n",
       "      <td>1.302977</td>\n",
       "      <td>-0.906553</td>\n",
       "      <td>-0.257008</td>\n",
       "      <td>-1.823342</td>\n",
       "      <td>-0.001037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.713239</td>\n",
       "      <td>0.312049</td>\n",
       "      <td>0.565426</td>\n",
       "      <td>0.083323</td>\n",
       "      <td>-0.084603</td>\n",
       "      <td>0.206758</td>\n",
       "      <td>-0.418716</td>\n",
       "      <td>-0.327378</td>\n",
       "      <td>-0.026131</td>\n",
       "      <td>-0.799707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272844</td>\n",
       "      <td>-0.637691</td>\n",
       "      <td>-0.992782</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>-0.016019</td>\n",
       "      <td>0.183645</td>\n",
       "      <td>0.195657</td>\n",
       "      <td>-0.208603</td>\n",
       "      <td>0.041691</td>\n",
       "      <td>-0.588364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.653736</td>\n",
       "      <td>0.194937</td>\n",
       "      <td>0.633811</td>\n",
       "      <td>-0.673525</td>\n",
       "      <td>-0.474191</td>\n",
       "      <td>0.111487</td>\n",
       "      <td>-0.577059</td>\n",
       "      <td>-0.013320</td>\n",
       "      <td>0.094306</td>\n",
       "      <td>-0.967332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063274</td>\n",
       "      <td>-0.020065</td>\n",
       "      <td>0.383190</td>\n",
       "      <td>0.180168</td>\n",
       "      <td>0.378840</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.092612</td>\n",
       "      <td>0.160256</td>\n",
       "      <td>-0.325747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190906</th>\n",
       "      <td>-1.354979</td>\n",
       "      <td>0.294521</td>\n",
       "      <td>0.096545</td>\n",
       "      <td>1.349656</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.248632</td>\n",
       "      <td>0.582599</td>\n",
       "      <td>-0.536717</td>\n",
       "      <td>0.049130</td>\n",
       "      <td>-0.606542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096936</td>\n",
       "      <td>-1.160574</td>\n",
       "      <td>0.345408</td>\n",
       "      <td>-0.166113</td>\n",
       "      <td>-0.641177</td>\n",
       "      <td>0.143569</td>\n",
       "      <td>0.115087</td>\n",
       "      <td>-0.283245</td>\n",
       "      <td>0.156974</td>\n",
       "      <td>-0.168432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190907</th>\n",
       "      <td>-1.517663</td>\n",
       "      <td>0.304064</td>\n",
       "      <td>0.216418</td>\n",
       "      <td>1.565045</td>\n",
       "      <td>-0.019931</td>\n",
       "      <td>0.317247</td>\n",
       "      <td>0.311972</td>\n",
       "      <td>-0.303801</td>\n",
       "      <td>-0.008274</td>\n",
       "      <td>0.138634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273440</td>\n",
       "      <td>-0.281598</td>\n",
       "      <td>-0.072978</td>\n",
       "      <td>-0.002141</td>\n",
       "      <td>-0.048658</td>\n",
       "      <td>0.023680</td>\n",
       "      <td>-0.054482</td>\n",
       "      <td>-0.003589</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.090318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190908</th>\n",
       "      <td>-1.419322</td>\n",
       "      <td>0.317637</td>\n",
       "      <td>0.082230</td>\n",
       "      <td>1.903840</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.323318</td>\n",
       "      <td>0.545954</td>\n",
       "      <td>-0.360911</td>\n",
       "      <td>-0.001133</td>\n",
       "      <td>0.171949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.313809</td>\n",
       "      <td>-0.466276</td>\n",
       "      <td>0.120292</td>\n",
       "      <td>-0.185041</td>\n",
       "      <td>-0.201036</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>-0.080849</td>\n",
       "      <td>-0.016240</td>\n",
       "      <td>0.075471</td>\n",
       "      <td>0.169020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190909</th>\n",
       "      <td>-1.649988</td>\n",
       "      <td>0.266934</td>\n",
       "      <td>0.467961</td>\n",
       "      <td>1.250169</td>\n",
       "      <td>0.159151</td>\n",
       "      <td>0.432371</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-0.330663</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.088190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036056</td>\n",
       "      <td>0.638537</td>\n",
       "      <td>-0.087614</td>\n",
       "      <td>0.225365</td>\n",
       "      <td>0.481676</td>\n",
       "      <td>0.015481</td>\n",
       "      <td>0.191181</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>-0.205600</td>\n",
       "      <td>-0.295715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190910</th>\n",
       "      <td>-1.675775</td>\n",
       "      <td>0.266191</td>\n",
       "      <td>0.496850</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>-0.015304</td>\n",
       "      <td>0.340036</td>\n",
       "      <td>-0.132997</td>\n",
       "      <td>-0.187160</td>\n",
       "      <td>0.017165</td>\n",
       "      <td>0.038310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117895</td>\n",
       "      <td>0.449693</td>\n",
       "      <td>-0.046242</td>\n",
       "      <td>0.176543</td>\n",
       "      <td>0.465732</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.120160</td>\n",
       "      <td>0.040277</td>\n",
       "      <td>-0.098953</td>\n",
       "      <td>-0.137579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190911 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Principal component 1  Principal component 2  Principal component 3  \\\n",
       "0                    4.092749              -1.039351               1.260691   \n",
       "1                    4.444136              -0.990882               0.766887   \n",
       "2                    4.184336              -0.683525               0.008515   \n",
       "3                   -1.713239               0.312049               0.565426   \n",
       "4                   -1.653736               0.194937               0.633811   \n",
       "...                       ...                    ...                    ...   \n",
       "190906              -1.354979               0.294521               0.096545   \n",
       "190907              -1.517663               0.304064               0.216418   \n",
       "190908              -1.419322               0.317637               0.082230   \n",
       "190909              -1.649988               0.266934               0.467961   \n",
       "190910              -1.675775               0.266191               0.496850   \n",
       "\n",
       "        Principal component 4  Principal component 5  Principal component 6  \\\n",
       "0                   -4.038849               1.985634               1.064285   \n",
       "1                   -2.737491               1.758523               1.036710   \n",
       "2                   -4.374319              -0.862230              18.046185   \n",
       "3                    0.083323              -0.084603               0.206758   \n",
       "4                   -0.673525              -0.474191               0.111487   \n",
       "...                       ...                    ...                    ...   \n",
       "190906               1.349656               0.014952               0.248632   \n",
       "190907               1.565045              -0.019931               0.317247   \n",
       "190908               1.903840               0.017500               0.323318   \n",
       "190909               1.250169               0.159151               0.432371   \n",
       "190910               0.886289              -0.015304               0.340036   \n",
       "\n",
       "        Principal component 7  Principal component 8  Principal component 9  \\\n",
       "0                    0.626387              -4.319053              -0.378186   \n",
       "1                    1.748314              -4.440705              -0.321942   \n",
       "2                    2.732318               4.997712               0.845690   \n",
       "3                   -0.418716              -0.327378              -0.026131   \n",
       "4                   -0.577059              -0.013320               0.094306   \n",
       "...                       ...                    ...                    ...   \n",
       "190906               0.582599              -0.536717               0.049130   \n",
       "190907               0.311972              -0.303801              -0.008274   \n",
       "190908               0.545954              -0.360911              -0.001133   \n",
       "190909              -0.073269              -0.330663               0.011988   \n",
       "190910              -0.132997              -0.187160               0.017165   \n",
       "\n",
       "        Principal component 10  ...  Principal component 21  \\\n",
       "0                    -2.292077  ...                0.442761   \n",
       "1                    -2.087103  ...               -0.161841   \n",
       "2                    -1.541559  ...               -1.289033   \n",
       "3                    -0.799707  ...                0.272844   \n",
       "4                    -0.967332  ...               -0.063274   \n",
       "...                        ...  ...                     ...   \n",
       "190906               -0.606542  ...                0.096936   \n",
       "190907                0.138634  ...               -0.273440   \n",
       "190908                0.171949  ...               -0.313809   \n",
       "190909               -0.088190  ...                0.036056   \n",
       "190910                0.038310  ...               -0.117895   \n",
       "\n",
       "        Principal component 22  Principal component 23  \\\n",
       "0                    -0.674434                0.700384   \n",
       "1                    -2.012189                2.037390   \n",
       "2                     0.236197               -0.113712   \n",
       "3                    -0.637691               -0.992782   \n",
       "4                    -0.020065                0.383190   \n",
       "...                        ...                     ...   \n",
       "190906               -1.160574                0.345408   \n",
       "190907               -0.281598               -0.072978   \n",
       "190908               -0.466276                0.120292   \n",
       "190909                0.638537               -0.087614   \n",
       "190910                0.449693               -0.046242   \n",
       "\n",
       "        Principal component 24  Principal component 25  \\\n",
       "0                     0.538268               -0.173432   \n",
       "1                    -0.565709               -1.112169   \n",
       "2                    -0.441430                0.408279   \n",
       "3                     0.698782               -0.016019   \n",
       "4                     0.180168                0.378840   \n",
       "...                        ...                     ...   \n",
       "190906               -0.166113               -0.641177   \n",
       "190907               -0.002141               -0.048658   \n",
       "190908               -0.185041               -0.201036   \n",
       "190909                0.225365                0.481676   \n",
       "190910                0.176543                0.465732   \n",
       "\n",
       "        Principal component 26  Principal component 27  \\\n",
       "0                    -0.348808               -1.069114   \n",
       "1                    -0.363729               -1.425324   \n",
       "2                     1.302977               -0.906553   \n",
       "3                     0.183645                0.195657   \n",
       "4                     0.077930                0.169476   \n",
       "...                        ...                     ...   \n",
       "190906                0.143569                0.115087   \n",
       "190907                0.023680               -0.054482   \n",
       "190908                0.023039               -0.080849   \n",
       "190909                0.015481                0.191181   \n",
       "190910                0.006283                0.120160   \n",
       "\n",
       "        Principal component 28  Principal component 29  Principal component 30  \n",
       "0                     0.040534                1.065742               -0.221165  \n",
       "1                     0.024095                1.566272                0.501800  \n",
       "2                    -0.257008               -1.823342               -0.001037  \n",
       "3                    -0.208603                0.041691               -0.588364  \n",
       "4                    -0.092612                0.160256               -0.325747  \n",
       "...                        ...                     ...                     ...  \n",
       "190906               -0.283245                0.156974               -0.168432  \n",
       "190907               -0.003589                0.037226                0.090318  \n",
       "190908               -0.016240                0.075471                0.169020  \n",
       "190909                0.005602               -0.205600               -0.295715  \n",
       "190910                0.040277               -0.098953               -0.137579  \n",
       "\n",
       "[190911 rows x 30 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pc = pd.DataFrame(data = principal_components, columns = principal_component_headings)\n",
    "df_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining/ concatinating the label feature back onto the pca transformed dataset. Label still needs to be transformed into binary data (for model comprehension/ understanding i.e. the model doesn't understand string data but string data can be transformed into numeric data, which is model can understand and use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal component 1</th>\n",
       "      <th>Principal component 2</th>\n",
       "      <th>Principal component 3</th>\n",
       "      <th>Principal component 4</th>\n",
       "      <th>Principal component 5</th>\n",
       "      <th>Principal component 6</th>\n",
       "      <th>Principal component 7</th>\n",
       "      <th>Principal component 8</th>\n",
       "      <th>Principal component 9</th>\n",
       "      <th>Principal component 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Principal component 22</th>\n",
       "      <th>Principal component 23</th>\n",
       "      <th>Principal component 24</th>\n",
       "      <th>Principal component 25</th>\n",
       "      <th>Principal component 26</th>\n",
       "      <th>Principal component 27</th>\n",
       "      <th>Principal component 28</th>\n",
       "      <th>Principal component 29</th>\n",
       "      <th>Principal component 30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.092749</td>\n",
       "      <td>-1.039351</td>\n",
       "      <td>1.260691</td>\n",
       "      <td>-4.038849</td>\n",
       "      <td>1.985634</td>\n",
       "      <td>1.064285</td>\n",
       "      <td>0.626387</td>\n",
       "      <td>-4.319053</td>\n",
       "      <td>-0.378186</td>\n",
       "      <td>-2.292077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674434</td>\n",
       "      <td>0.700384</td>\n",
       "      <td>0.538268</td>\n",
       "      <td>-0.173432</td>\n",
       "      <td>-0.348808</td>\n",
       "      <td>-1.069114</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>1.065742</td>\n",
       "      <td>-0.221165</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.444136</td>\n",
       "      <td>-0.990882</td>\n",
       "      <td>0.766887</td>\n",
       "      <td>-2.737491</td>\n",
       "      <td>1.758523</td>\n",
       "      <td>1.036710</td>\n",
       "      <td>1.748314</td>\n",
       "      <td>-4.440705</td>\n",
       "      <td>-0.321942</td>\n",
       "      <td>-2.087103</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.012189</td>\n",
       "      <td>2.037390</td>\n",
       "      <td>-0.565709</td>\n",
       "      <td>-1.112169</td>\n",
       "      <td>-0.363729</td>\n",
       "      <td>-1.425324</td>\n",
       "      <td>0.024095</td>\n",
       "      <td>1.566272</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.184336</td>\n",
       "      <td>-0.683525</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>-4.374319</td>\n",
       "      <td>-0.862230</td>\n",
       "      <td>18.046185</td>\n",
       "      <td>2.732318</td>\n",
       "      <td>4.997712</td>\n",
       "      <td>0.845690</td>\n",
       "      <td>-1.541559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236197</td>\n",
       "      <td>-0.113712</td>\n",
       "      <td>-0.441430</td>\n",
       "      <td>0.408279</td>\n",
       "      <td>1.302977</td>\n",
       "      <td>-0.906553</td>\n",
       "      <td>-0.257008</td>\n",
       "      <td>-1.823342</td>\n",
       "      <td>-0.001037</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.713239</td>\n",
       "      <td>0.312049</td>\n",
       "      <td>0.565426</td>\n",
       "      <td>0.083323</td>\n",
       "      <td>-0.084603</td>\n",
       "      <td>0.206758</td>\n",
       "      <td>-0.418716</td>\n",
       "      <td>-0.327378</td>\n",
       "      <td>-0.026131</td>\n",
       "      <td>-0.799707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.637691</td>\n",
       "      <td>-0.992782</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>-0.016019</td>\n",
       "      <td>0.183645</td>\n",
       "      <td>0.195657</td>\n",
       "      <td>-0.208603</td>\n",
       "      <td>0.041691</td>\n",
       "      <td>-0.588364</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.653736</td>\n",
       "      <td>0.194937</td>\n",
       "      <td>0.633811</td>\n",
       "      <td>-0.673525</td>\n",
       "      <td>-0.474191</td>\n",
       "      <td>0.111487</td>\n",
       "      <td>-0.577059</td>\n",
       "      <td>-0.013320</td>\n",
       "      <td>0.094306</td>\n",
       "      <td>-0.967332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020065</td>\n",
       "      <td>0.383190</td>\n",
       "      <td>0.180168</td>\n",
       "      <td>0.378840</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.092612</td>\n",
       "      <td>0.160256</td>\n",
       "      <td>-0.325747</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190906</th>\n",
       "      <td>-1.354979</td>\n",
       "      <td>0.294521</td>\n",
       "      <td>0.096545</td>\n",
       "      <td>1.349656</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.248632</td>\n",
       "      <td>0.582599</td>\n",
       "      <td>-0.536717</td>\n",
       "      <td>0.049130</td>\n",
       "      <td>-0.606542</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.160574</td>\n",
       "      <td>0.345408</td>\n",
       "      <td>-0.166113</td>\n",
       "      <td>-0.641177</td>\n",
       "      <td>0.143569</td>\n",
       "      <td>0.115087</td>\n",
       "      <td>-0.283245</td>\n",
       "      <td>0.156974</td>\n",
       "      <td>-0.168432</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190907</th>\n",
       "      <td>-1.517663</td>\n",
       "      <td>0.304064</td>\n",
       "      <td>0.216418</td>\n",
       "      <td>1.565045</td>\n",
       "      <td>-0.019931</td>\n",
       "      <td>0.317247</td>\n",
       "      <td>0.311972</td>\n",
       "      <td>-0.303801</td>\n",
       "      <td>-0.008274</td>\n",
       "      <td>0.138634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281598</td>\n",
       "      <td>-0.072978</td>\n",
       "      <td>-0.002141</td>\n",
       "      <td>-0.048658</td>\n",
       "      <td>0.023680</td>\n",
       "      <td>-0.054482</td>\n",
       "      <td>-0.003589</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.090318</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190908</th>\n",
       "      <td>-1.419322</td>\n",
       "      <td>0.317637</td>\n",
       "      <td>0.082230</td>\n",
       "      <td>1.903840</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.323318</td>\n",
       "      <td>0.545954</td>\n",
       "      <td>-0.360911</td>\n",
       "      <td>-0.001133</td>\n",
       "      <td>0.171949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.466276</td>\n",
       "      <td>0.120292</td>\n",
       "      <td>-0.185041</td>\n",
       "      <td>-0.201036</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>-0.080849</td>\n",
       "      <td>-0.016240</td>\n",
       "      <td>0.075471</td>\n",
       "      <td>0.169020</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190909</th>\n",
       "      <td>-1.649988</td>\n",
       "      <td>0.266934</td>\n",
       "      <td>0.467961</td>\n",
       "      <td>1.250169</td>\n",
       "      <td>0.159151</td>\n",
       "      <td>0.432371</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-0.330663</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.088190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638537</td>\n",
       "      <td>-0.087614</td>\n",
       "      <td>0.225365</td>\n",
       "      <td>0.481676</td>\n",
       "      <td>0.015481</td>\n",
       "      <td>0.191181</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>-0.205600</td>\n",
       "      <td>-0.295715</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190910</th>\n",
       "      <td>-1.675775</td>\n",
       "      <td>0.266191</td>\n",
       "      <td>0.496850</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>-0.015304</td>\n",
       "      <td>0.340036</td>\n",
       "      <td>-0.132997</td>\n",
       "      <td>-0.187160</td>\n",
       "      <td>0.017165</td>\n",
       "      <td>0.038310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449693</td>\n",
       "      <td>-0.046242</td>\n",
       "      <td>0.176543</td>\n",
       "      <td>0.465732</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.120160</td>\n",
       "      <td>0.040277</td>\n",
       "      <td>-0.098953</td>\n",
       "      <td>-0.137579</td>\n",
       "      <td>BENIGN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190911 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Principal component 1  Principal component 2  Principal component 3  \\\n",
       "0                    4.092749              -1.039351               1.260691   \n",
       "1                    4.444136              -0.990882               0.766887   \n",
       "2                    4.184336              -0.683525               0.008515   \n",
       "3                   -1.713239               0.312049               0.565426   \n",
       "4                   -1.653736               0.194937               0.633811   \n",
       "...                       ...                    ...                    ...   \n",
       "190906              -1.354979               0.294521               0.096545   \n",
       "190907              -1.517663               0.304064               0.216418   \n",
       "190908              -1.419322               0.317637               0.082230   \n",
       "190909              -1.649988               0.266934               0.467961   \n",
       "190910              -1.675775               0.266191               0.496850   \n",
       "\n",
       "        Principal component 4  Principal component 5  Principal component 6  \\\n",
       "0                   -4.038849               1.985634               1.064285   \n",
       "1                   -2.737491               1.758523               1.036710   \n",
       "2                   -4.374319              -0.862230              18.046185   \n",
       "3                    0.083323              -0.084603               0.206758   \n",
       "4                   -0.673525              -0.474191               0.111487   \n",
       "...                       ...                    ...                    ...   \n",
       "190906               1.349656               0.014952               0.248632   \n",
       "190907               1.565045              -0.019931               0.317247   \n",
       "190908               1.903840               0.017500               0.323318   \n",
       "190909               1.250169               0.159151               0.432371   \n",
       "190910               0.886289              -0.015304               0.340036   \n",
       "\n",
       "        Principal component 7  Principal component 8  Principal component 9  \\\n",
       "0                    0.626387              -4.319053              -0.378186   \n",
       "1                    1.748314              -4.440705              -0.321942   \n",
       "2                    2.732318               4.997712               0.845690   \n",
       "3                   -0.418716              -0.327378              -0.026131   \n",
       "4                   -0.577059              -0.013320               0.094306   \n",
       "...                       ...                    ...                    ...   \n",
       "190906               0.582599              -0.536717               0.049130   \n",
       "190907               0.311972              -0.303801              -0.008274   \n",
       "190908               0.545954              -0.360911              -0.001133   \n",
       "190909              -0.073269              -0.330663               0.011988   \n",
       "190910              -0.132997              -0.187160               0.017165   \n",
       "\n",
       "        Principal component 10  ...  Principal component 22  \\\n",
       "0                    -2.292077  ...               -0.674434   \n",
       "1                    -2.087103  ...               -2.012189   \n",
       "2                    -1.541559  ...                0.236197   \n",
       "3                    -0.799707  ...               -0.637691   \n",
       "4                    -0.967332  ...               -0.020065   \n",
       "...                        ...  ...                     ...   \n",
       "190906               -0.606542  ...               -1.160574   \n",
       "190907                0.138634  ...               -0.281598   \n",
       "190908                0.171949  ...               -0.466276   \n",
       "190909               -0.088190  ...                0.638537   \n",
       "190910                0.038310  ...                0.449693   \n",
       "\n",
       "        Principal component 23  Principal component 24  \\\n",
       "0                     0.700384                0.538268   \n",
       "1                     2.037390               -0.565709   \n",
       "2                    -0.113712               -0.441430   \n",
       "3                    -0.992782                0.698782   \n",
       "4                     0.383190                0.180168   \n",
       "...                        ...                     ...   \n",
       "190906                0.345408               -0.166113   \n",
       "190907               -0.072978               -0.002141   \n",
       "190908                0.120292               -0.185041   \n",
       "190909               -0.087614                0.225365   \n",
       "190910               -0.046242                0.176543   \n",
       "\n",
       "        Principal component 25  Principal component 26  \\\n",
       "0                    -0.173432               -0.348808   \n",
       "1                    -1.112169               -0.363729   \n",
       "2                     0.408279                1.302977   \n",
       "3                    -0.016019                0.183645   \n",
       "4                     0.378840                0.077930   \n",
       "...                        ...                     ...   \n",
       "190906               -0.641177                0.143569   \n",
       "190907               -0.048658                0.023680   \n",
       "190908               -0.201036                0.023039   \n",
       "190909                0.481676                0.015481   \n",
       "190910                0.465732                0.006283   \n",
       "\n",
       "        Principal component 27  Principal component 28  \\\n",
       "0                    -1.069114                0.040534   \n",
       "1                    -1.425324                0.024095   \n",
       "2                    -0.906553               -0.257008   \n",
       "3                     0.195657               -0.208603   \n",
       "4                     0.169476               -0.092612   \n",
       "...                        ...                     ...   \n",
       "190906                0.115087               -0.283245   \n",
       "190907               -0.054482               -0.003589   \n",
       "190908               -0.080849               -0.016240   \n",
       "190909                0.191181                0.005602   \n",
       "190910                0.120160                0.040277   \n",
       "\n",
       "        Principal component 29  Principal component 30   label  \n",
       "0                     1.065742               -0.221165  BENIGN  \n",
       "1                     1.566272                0.501800  BENIGN  \n",
       "2                    -1.823342               -0.001037  BENIGN  \n",
       "3                     0.041691               -0.588364  BENIGN  \n",
       "4                     0.160256               -0.325747  BENIGN  \n",
       "...                        ...                     ...     ...  \n",
       "190906                0.156974               -0.168432  BENIGN  \n",
       "190907                0.037226                0.090318  BENIGN  \n",
       "190908                0.075471                0.169020  BENIGN  \n",
       "190909               -0.205600               -0.295715  BENIGN  \n",
       "190910               -0.098953               -0.137579  BENIGN  \n",
       "\n",
       "[190911 rows x 31 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final = pd.concat([df_pc, df_labels], axis = 1)\n",
    "# Scroll to the RHS end of dataframe to see attached label feature\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the label feature's categorical data into numeric data (via LabelBinarizer)\n",
    "\n",
    "Again, a model can't understand e.g. 'yes' and 'no' strings but, these can be mapped to a 1 for yes and a 0 for no.\n",
    "\n",
    "The **sklearn.preprocessing.LabelBinarizer** can be used to convert the column data into binary numbers, which will then be correctly interpreted.\n",
    "\n",
    "1. Fit the List- this tells the LabelBinarizer what values exist, and how to map them. \n",
    "\n",
    "2. Call transform, passing a List, and this will return the encoded List.\n",
    "\n",
    "**(Note: if label column has more than 2 unique labels, pandas.get_dummies is required instead)**\n",
    "\n",
    "(Code reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal component 1</th>\n",
       "      <th>Principal component 2</th>\n",
       "      <th>Principal component 3</th>\n",
       "      <th>Principal component 4</th>\n",
       "      <th>Principal component 5</th>\n",
       "      <th>Principal component 6</th>\n",
       "      <th>Principal component 7</th>\n",
       "      <th>Principal component 8</th>\n",
       "      <th>Principal component 9</th>\n",
       "      <th>Principal component 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Principal component 22</th>\n",
       "      <th>Principal component 23</th>\n",
       "      <th>Principal component 24</th>\n",
       "      <th>Principal component 25</th>\n",
       "      <th>Principal component 26</th>\n",
       "      <th>Principal component 27</th>\n",
       "      <th>Principal component 28</th>\n",
       "      <th>Principal component 29</th>\n",
       "      <th>Principal component 30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.092749</td>\n",
       "      <td>-1.039351</td>\n",
       "      <td>1.260691</td>\n",
       "      <td>-4.038849</td>\n",
       "      <td>1.985634</td>\n",
       "      <td>1.064285</td>\n",
       "      <td>0.626387</td>\n",
       "      <td>-4.319053</td>\n",
       "      <td>-0.378186</td>\n",
       "      <td>-2.292077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674434</td>\n",
       "      <td>0.700384</td>\n",
       "      <td>0.538268</td>\n",
       "      <td>-0.173432</td>\n",
       "      <td>-0.348808</td>\n",
       "      <td>-1.069114</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>1.065742</td>\n",
       "      <td>-0.221165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.444136</td>\n",
       "      <td>-0.990882</td>\n",
       "      <td>0.766887</td>\n",
       "      <td>-2.737491</td>\n",
       "      <td>1.758523</td>\n",
       "      <td>1.036710</td>\n",
       "      <td>1.748314</td>\n",
       "      <td>-4.440705</td>\n",
       "      <td>-0.321942</td>\n",
       "      <td>-2.087103</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.012189</td>\n",
       "      <td>2.037390</td>\n",
       "      <td>-0.565709</td>\n",
       "      <td>-1.112169</td>\n",
       "      <td>-0.363729</td>\n",
       "      <td>-1.425324</td>\n",
       "      <td>0.024095</td>\n",
       "      <td>1.566272</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.184336</td>\n",
       "      <td>-0.683525</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>-4.374319</td>\n",
       "      <td>-0.862230</td>\n",
       "      <td>18.046185</td>\n",
       "      <td>2.732318</td>\n",
       "      <td>4.997712</td>\n",
       "      <td>0.845690</td>\n",
       "      <td>-1.541559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236197</td>\n",
       "      <td>-0.113712</td>\n",
       "      <td>-0.441430</td>\n",
       "      <td>0.408279</td>\n",
       "      <td>1.302977</td>\n",
       "      <td>-0.906553</td>\n",
       "      <td>-0.257008</td>\n",
       "      <td>-1.823342</td>\n",
       "      <td>-0.001037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.713239</td>\n",
       "      <td>0.312049</td>\n",
       "      <td>0.565426</td>\n",
       "      <td>0.083323</td>\n",
       "      <td>-0.084603</td>\n",
       "      <td>0.206758</td>\n",
       "      <td>-0.418716</td>\n",
       "      <td>-0.327378</td>\n",
       "      <td>-0.026131</td>\n",
       "      <td>-0.799707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.637691</td>\n",
       "      <td>-0.992782</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>-0.016019</td>\n",
       "      <td>0.183645</td>\n",
       "      <td>0.195657</td>\n",
       "      <td>-0.208603</td>\n",
       "      <td>0.041691</td>\n",
       "      <td>-0.588364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.653736</td>\n",
       "      <td>0.194937</td>\n",
       "      <td>0.633811</td>\n",
       "      <td>-0.673525</td>\n",
       "      <td>-0.474191</td>\n",
       "      <td>0.111487</td>\n",
       "      <td>-0.577059</td>\n",
       "      <td>-0.013320</td>\n",
       "      <td>0.094306</td>\n",
       "      <td>-0.967332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020065</td>\n",
       "      <td>0.383190</td>\n",
       "      <td>0.180168</td>\n",
       "      <td>0.378840</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.092612</td>\n",
       "      <td>0.160256</td>\n",
       "      <td>-0.325747</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190906</th>\n",
       "      <td>-1.354979</td>\n",
       "      <td>0.294521</td>\n",
       "      <td>0.096545</td>\n",
       "      <td>1.349656</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.248632</td>\n",
       "      <td>0.582599</td>\n",
       "      <td>-0.536717</td>\n",
       "      <td>0.049130</td>\n",
       "      <td>-0.606542</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.160574</td>\n",
       "      <td>0.345408</td>\n",
       "      <td>-0.166113</td>\n",
       "      <td>-0.641177</td>\n",
       "      <td>0.143569</td>\n",
       "      <td>0.115087</td>\n",
       "      <td>-0.283245</td>\n",
       "      <td>0.156974</td>\n",
       "      <td>-0.168432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190907</th>\n",
       "      <td>-1.517663</td>\n",
       "      <td>0.304064</td>\n",
       "      <td>0.216418</td>\n",
       "      <td>1.565045</td>\n",
       "      <td>-0.019931</td>\n",
       "      <td>0.317247</td>\n",
       "      <td>0.311972</td>\n",
       "      <td>-0.303801</td>\n",
       "      <td>-0.008274</td>\n",
       "      <td>0.138634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281598</td>\n",
       "      <td>-0.072978</td>\n",
       "      <td>-0.002141</td>\n",
       "      <td>-0.048658</td>\n",
       "      <td>0.023680</td>\n",
       "      <td>-0.054482</td>\n",
       "      <td>-0.003589</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.090318</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190908</th>\n",
       "      <td>-1.419322</td>\n",
       "      <td>0.317637</td>\n",
       "      <td>0.082230</td>\n",
       "      <td>1.903840</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.323318</td>\n",
       "      <td>0.545954</td>\n",
       "      <td>-0.360911</td>\n",
       "      <td>-0.001133</td>\n",
       "      <td>0.171949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.466276</td>\n",
       "      <td>0.120292</td>\n",
       "      <td>-0.185041</td>\n",
       "      <td>-0.201036</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>-0.080849</td>\n",
       "      <td>-0.016240</td>\n",
       "      <td>0.075471</td>\n",
       "      <td>0.169020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190909</th>\n",
       "      <td>-1.649988</td>\n",
       "      <td>0.266934</td>\n",
       "      <td>0.467961</td>\n",
       "      <td>1.250169</td>\n",
       "      <td>0.159151</td>\n",
       "      <td>0.432371</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-0.330663</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.088190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638537</td>\n",
       "      <td>-0.087614</td>\n",
       "      <td>0.225365</td>\n",
       "      <td>0.481676</td>\n",
       "      <td>0.015481</td>\n",
       "      <td>0.191181</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>-0.205600</td>\n",
       "      <td>-0.295715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190910</th>\n",
       "      <td>-1.675775</td>\n",
       "      <td>0.266191</td>\n",
       "      <td>0.496850</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>-0.015304</td>\n",
       "      <td>0.340036</td>\n",
       "      <td>-0.132997</td>\n",
       "      <td>-0.187160</td>\n",
       "      <td>0.017165</td>\n",
       "      <td>0.038310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449693</td>\n",
       "      <td>-0.046242</td>\n",
       "      <td>0.176543</td>\n",
       "      <td>0.465732</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.120160</td>\n",
       "      <td>0.040277</td>\n",
       "      <td>-0.098953</td>\n",
       "      <td>-0.137579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190911 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Principal component 1  Principal component 2  Principal component 3  \\\n",
       "0                    4.092749              -1.039351               1.260691   \n",
       "1                    4.444136              -0.990882               0.766887   \n",
       "2                    4.184336              -0.683525               0.008515   \n",
       "3                   -1.713239               0.312049               0.565426   \n",
       "4                   -1.653736               0.194937               0.633811   \n",
       "...                       ...                    ...                    ...   \n",
       "190906              -1.354979               0.294521               0.096545   \n",
       "190907              -1.517663               0.304064               0.216418   \n",
       "190908              -1.419322               0.317637               0.082230   \n",
       "190909              -1.649988               0.266934               0.467961   \n",
       "190910              -1.675775               0.266191               0.496850   \n",
       "\n",
       "        Principal component 4  Principal component 5  Principal component 6  \\\n",
       "0                   -4.038849               1.985634               1.064285   \n",
       "1                   -2.737491               1.758523               1.036710   \n",
       "2                   -4.374319              -0.862230              18.046185   \n",
       "3                    0.083323              -0.084603               0.206758   \n",
       "4                   -0.673525              -0.474191               0.111487   \n",
       "...                       ...                    ...                    ...   \n",
       "190906               1.349656               0.014952               0.248632   \n",
       "190907               1.565045              -0.019931               0.317247   \n",
       "190908               1.903840               0.017500               0.323318   \n",
       "190909               1.250169               0.159151               0.432371   \n",
       "190910               0.886289              -0.015304               0.340036   \n",
       "\n",
       "        Principal component 7  Principal component 8  Principal component 9  \\\n",
       "0                    0.626387              -4.319053              -0.378186   \n",
       "1                    1.748314              -4.440705              -0.321942   \n",
       "2                    2.732318               4.997712               0.845690   \n",
       "3                   -0.418716              -0.327378              -0.026131   \n",
       "4                   -0.577059              -0.013320               0.094306   \n",
       "...                       ...                    ...                    ...   \n",
       "190906               0.582599              -0.536717               0.049130   \n",
       "190907               0.311972              -0.303801              -0.008274   \n",
       "190908               0.545954              -0.360911              -0.001133   \n",
       "190909              -0.073269              -0.330663               0.011988   \n",
       "190910              -0.132997              -0.187160               0.017165   \n",
       "\n",
       "        Principal component 10  ...  Principal component 22  \\\n",
       "0                    -2.292077  ...               -0.674434   \n",
       "1                    -2.087103  ...               -2.012189   \n",
       "2                    -1.541559  ...                0.236197   \n",
       "3                    -0.799707  ...               -0.637691   \n",
       "4                    -0.967332  ...               -0.020065   \n",
       "...                        ...  ...                     ...   \n",
       "190906               -0.606542  ...               -1.160574   \n",
       "190907                0.138634  ...               -0.281598   \n",
       "190908                0.171949  ...               -0.466276   \n",
       "190909               -0.088190  ...                0.638537   \n",
       "190910                0.038310  ...                0.449693   \n",
       "\n",
       "        Principal component 23  Principal component 24  \\\n",
       "0                     0.700384                0.538268   \n",
       "1                     2.037390               -0.565709   \n",
       "2                    -0.113712               -0.441430   \n",
       "3                    -0.992782                0.698782   \n",
       "4                     0.383190                0.180168   \n",
       "...                        ...                     ...   \n",
       "190906                0.345408               -0.166113   \n",
       "190907               -0.072978               -0.002141   \n",
       "190908                0.120292               -0.185041   \n",
       "190909               -0.087614                0.225365   \n",
       "190910               -0.046242                0.176543   \n",
       "\n",
       "        Principal component 25  Principal component 26  \\\n",
       "0                    -0.173432               -0.348808   \n",
       "1                    -1.112169               -0.363729   \n",
       "2                     0.408279                1.302977   \n",
       "3                    -0.016019                0.183645   \n",
       "4                     0.378840                0.077930   \n",
       "...                        ...                     ...   \n",
       "190906               -0.641177                0.143569   \n",
       "190907               -0.048658                0.023680   \n",
       "190908               -0.201036                0.023039   \n",
       "190909                0.481676                0.015481   \n",
       "190910                0.465732                0.006283   \n",
       "\n",
       "        Principal component 27  Principal component 28  \\\n",
       "0                    -1.069114                0.040534   \n",
       "1                    -1.425324                0.024095   \n",
       "2                    -0.906553               -0.257008   \n",
       "3                     0.195657               -0.208603   \n",
       "4                     0.169476               -0.092612   \n",
       "...                        ...                     ...   \n",
       "190906                0.115087               -0.283245   \n",
       "190907               -0.054482               -0.003589   \n",
       "190908               -0.080849               -0.016240   \n",
       "190909                0.191181                0.005602   \n",
       "190910                0.120160                0.040277   \n",
       "\n",
       "        Principal component 29  Principal component 30  label  \n",
       "0                     1.065742               -0.221165      0  \n",
       "1                     1.566272                0.501800      0  \n",
       "2                    -1.823342               -0.001037      0  \n",
       "3                     0.041691               -0.588364      0  \n",
       "4                     0.160256               -0.325747      0  \n",
       "...                        ...                     ...    ...  \n",
       "190906                0.156974               -0.168432      0  \n",
       "190907                0.037226                0.090318      0  \n",
       "190908                0.075471                0.169020      0  \n",
       "190909               -0.205600               -0.295715      0  \n",
       "190910               -0.098953               -0.137579      0  \n",
       "\n",
       "[190911 rows x 31 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = LabelBinarizer()\n",
    "df_final['label'] = lb.fit_transform(df_final['label'])\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the transformation. **Again, to note, if label isn't binary then pd.get_dummies is required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LabelBinarizer:  ['BENIGN' 'Bot']\n",
      "After LabelBinarizer:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Before LabelBinarizer: \", df_labels.unique())\n",
    "print(\"After LabelBinarizer: \", df_final['label'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data is now fully cleaned and transformed, ready for pre-modeling test_train data splitting\n",
    "\n",
    "## K-Fold Cross Validation and Stratified splitting\n",
    "K-Fold is a technique which splits data into K folds (splits). Train of a model K times, and for each training iteration, K-Fold selects a different fold to use for testing; the remaining K - 1 folds become the training data. Typically, the optimal K value can be derived using the size of your dataset (num of rows). Ideally, each fold should be statistically representative of the population. Too small and it won't be useful. Too large, and you lose the positives from doing K-Fold.\n",
    "\n",
    "You can use Stratified splitting with K-Fold, which ensures balance between some criteria (balances out the classes) e.g. equal portion of label classes in each fold.\n",
    "\n",
    "Class Imbalance is a significant issue in the ML/ Data Mining domain. It leads to incorrect results e.g. if one fold had all of 1 label (accidentally), then it would produce terrible predictive results as it wouldn't know what the other label class data point would look like. You can only work with the data you have, so this has to be dealt with.\n",
    "\n",
    "Benefits of K-Fold:\n",
    "- Use more of the data towards making a succesful model.\n",
    "- Obtain K models to evaluate, can improve the confidence that you have selected an appropriate model algorithm and cleaned/ prepared the data correctly, e.g. normal split with 1 model, one doesn't know if it's good or not- it could be heavily biased. Multiple models ensures less bias and increased variance.\n",
    "- Looking at the accuracy results from each of the k-Folds, you can identify data issues e.g. a certain fold performs really badly. Could this suggest that more cleaning is required? Maybe the data preparation was performed incorrectly?\n",
    "- If all folds return similar accuracies, one can be more confident that a deployed model will perform similarly to how one expects.\n",
    "\n",
    "Issues with K-Fold:\n",
    "- Creating K separate models requires more computation.\n",
    "- If you haven't got much data, you might not get many folds. Less folds means K-Fold loses its benefits.\n",
    "- If K is very large, each fold is small, and harder to ensure statistical distribution of.\n",
    "- Choosing the best of K models introduces bias. Real world data could perform better under a more general, lower performing model.\n",
    "\n",
    "Code reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "190906    0\n",
       "190907    0\n",
       "190908    0\n",
       "190909    0\n",
       "190910    0\n",
       "Name: label, Length: 190911, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separating the label so that the answers aren't provided to the model, in training.\n",
    "X = df_final.drop(['label'], axis = 1)\n",
    "y = df_final['label']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising the StratifiedKFold model (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedKFold(n_splits=10, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=num_of_splits_for_skf, shuffle=False)\n",
    "skf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, splitting the data into train and test data, using the optimal splitting techniques of K-Fold and Stratified Splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train length:  171820\n",
      "y_train length:  171820\n",
      "X_test length:  19091\n",
      "y_test length:  19091\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    reshaped_y_train = np.asarray(y_train).reshape(-1, 1)\n",
    "    reshaped_y_test = np.asarray(y_test).reshape(-1, 1)\n",
    "    \n",
    "print( 'X_train length: ', len(X_train) ) # To check if splits worked\n",
    "print( 'y_train length: ', len(y_train) )\n",
    "print( 'X_test length: ', len(X_test) )\n",
    "print( 'y_test length: ', len(y_test) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling stage\n",
    "Data is now fully transformed and ready for ML model training and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest neighbor ML classifier\n",
    "The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are assumed to be near to each other.\n",
    "\n",
    "The most important factor in training a KNN model is the **number of neighbors hyperparameter**. You want to choose the K  value that reduces the number of errors, while maintaining the algorithm’s ability to accurately make predictions when it’s given data it hasn’t seen before. Here are some important considerations:\n",
    "\n",
    "- As you decrease the value of K to **1**, predictions become less stable e.g. imagine K=1 and you have a query point surrounded by several red 'dots' and one green 'dot', but the green dot is the single nearest neighbor. Reasonably, you would think the query point is most likely red, but because K=1, KNN incorrectly predicts that the query point is green.\n",
    "\n",
    "- Inversely, as you increase the value of K, predictions become more stable due to majority voting/ averaging, and thus, more likely to make more accurate predictions (up to a certain critical point- an **'overfitting' threshold**). Eventually, you would begin to witness an increasing number of errors. It is at this point you'd know that you have pushed the value of K too far.\n",
    "\n",
    "- In cases where you are taking a majority vote (e.g. picking the mode in a classification problem) amongst labels/ classes, you usually make K an odd number to have a tiebreaker.\n",
    "\n",
    "The sklearn's default k value is 5 (also true for MATLAB's implementation).\n",
    "\n",
    "References: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html & https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model and predicting test data results (confusion matrix)\n",
    "The selected Machine Learning classifier/ model/ models can now be trained on training data (from the StratifiedKFold splitting). Once the model is trained, it can be used to predict the test data's labels (based upon what it has seen before).\n",
    "\n",
    "The performance of the model can be seen below in the Classification Report, Confusion Matrix, and the model's predicitive accuracy result.\n",
    "\n",
    "(see **methods** section, at the top of the notebook, for the train_model_predict() method code. Note that it can take a few minutes to run due to the vast amount of data used, and the training time required for the model to learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     18895\n",
      "           1       0.86      1.00      0.92       196\n",
      "\n",
      "    accuracy                           1.00     19091\n",
      "   macro avg       0.93      1.00      0.96     19091\n",
      "weighted avg       1.00      1.00      1.00     19091\n",
      " \n",
      "\n",
      "Model accuracy=  99.8271436802682 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unpacking the method return values. Last 4 are needed for statistical distance measure methods.\n",
    "accuracy, X_train, X_test, y_train, pred_y = train_model_predict(knn_model, \"K-Nearest Neighbor\", X, y, skf)\n",
    "print(\"Model accuracy= \", accuracy*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SafeML statistical distance measures: test/ example run\n",
    "Now the ML models have been run, and the accuracy is found (along with other useful performance metrics), a test run of using the SafeML statistical distance measures can be run. This is a test run, and the real run will be done later. This test run is a guided 'tour' around how to set-up the code, in Python and SciKit-Learn, so that results can be re-produced easily for future case-studies or projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  2\n",
      "Labels:  [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the number of classes and labels from the label feature\n",
    "class_num = len(df_final['label'].unique())\n",
    "labels = df_final['label'].unique()\n",
    "print(\"Number of classes: \", class_num)\n",
    "print(\"Labels: \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal component 1</th>\n",
       "      <th>Principal component 2</th>\n",
       "      <th>Principal component 3</th>\n",
       "      <th>Principal component 4</th>\n",
       "      <th>Principal component 5</th>\n",
       "      <th>Principal component 6</th>\n",
       "      <th>Principal component 7</th>\n",
       "      <th>Principal component 8</th>\n",
       "      <th>Principal component 9</th>\n",
       "      <th>Principal component 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Principal component 21</th>\n",
       "      <th>Principal component 22</th>\n",
       "      <th>Principal component 23</th>\n",
       "      <th>Principal component 24</th>\n",
       "      <th>Principal component 25</th>\n",
       "      <th>Principal component 26</th>\n",
       "      <th>Principal component 27</th>\n",
       "      <th>Principal component 28</th>\n",
       "      <th>Principal component 29</th>\n",
       "      <th>Principal component 30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.092749</td>\n",
       "      <td>-1.039351</td>\n",
       "      <td>1.260691</td>\n",
       "      <td>-4.038849</td>\n",
       "      <td>1.985634</td>\n",
       "      <td>1.064285</td>\n",
       "      <td>0.626387</td>\n",
       "      <td>-4.319053</td>\n",
       "      <td>-0.378186</td>\n",
       "      <td>-2.292077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442761</td>\n",
       "      <td>-0.674434</td>\n",
       "      <td>0.700384</td>\n",
       "      <td>0.538268</td>\n",
       "      <td>-0.173432</td>\n",
       "      <td>-0.348808</td>\n",
       "      <td>-1.069114</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>1.065742</td>\n",
       "      <td>-0.221165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.444136</td>\n",
       "      <td>-0.990882</td>\n",
       "      <td>0.766887</td>\n",
       "      <td>-2.737491</td>\n",
       "      <td>1.758523</td>\n",
       "      <td>1.036710</td>\n",
       "      <td>1.748314</td>\n",
       "      <td>-4.440705</td>\n",
       "      <td>-0.321942</td>\n",
       "      <td>-2.087103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161841</td>\n",
       "      <td>-2.012189</td>\n",
       "      <td>2.037390</td>\n",
       "      <td>-0.565709</td>\n",
       "      <td>-1.112169</td>\n",
       "      <td>-0.363729</td>\n",
       "      <td>-1.425324</td>\n",
       "      <td>0.024095</td>\n",
       "      <td>1.566272</td>\n",
       "      <td>0.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.184336</td>\n",
       "      <td>-0.683525</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>-4.374319</td>\n",
       "      <td>-0.862230</td>\n",
       "      <td>18.046185</td>\n",
       "      <td>2.732318</td>\n",
       "      <td>4.997712</td>\n",
       "      <td>0.845690</td>\n",
       "      <td>-1.541559</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.289033</td>\n",
       "      <td>0.236197</td>\n",
       "      <td>-0.113712</td>\n",
       "      <td>-0.441430</td>\n",
       "      <td>0.408279</td>\n",
       "      <td>1.302977</td>\n",
       "      <td>-0.906553</td>\n",
       "      <td>-0.257008</td>\n",
       "      <td>-1.823342</td>\n",
       "      <td>-0.001037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.713239</td>\n",
       "      <td>0.312049</td>\n",
       "      <td>0.565426</td>\n",
       "      <td>0.083323</td>\n",
       "      <td>-0.084603</td>\n",
       "      <td>0.206758</td>\n",
       "      <td>-0.418716</td>\n",
       "      <td>-0.327378</td>\n",
       "      <td>-0.026131</td>\n",
       "      <td>-0.799707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272844</td>\n",
       "      <td>-0.637691</td>\n",
       "      <td>-0.992782</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>-0.016019</td>\n",
       "      <td>0.183645</td>\n",
       "      <td>0.195657</td>\n",
       "      <td>-0.208603</td>\n",
       "      <td>0.041691</td>\n",
       "      <td>-0.588364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.653736</td>\n",
       "      <td>0.194937</td>\n",
       "      <td>0.633811</td>\n",
       "      <td>-0.673525</td>\n",
       "      <td>-0.474191</td>\n",
       "      <td>0.111487</td>\n",
       "      <td>-0.577059</td>\n",
       "      <td>-0.013320</td>\n",
       "      <td>0.094306</td>\n",
       "      <td>-0.967332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063274</td>\n",
       "      <td>-0.020065</td>\n",
       "      <td>0.383190</td>\n",
       "      <td>0.180168</td>\n",
       "      <td>0.378840</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.092612</td>\n",
       "      <td>0.160256</td>\n",
       "      <td>-0.325747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180458</th>\n",
       "      <td>-1.309732</td>\n",
       "      <td>0.163678</td>\n",
       "      <td>0.269997</td>\n",
       "      <td>-0.301117</td>\n",
       "      <td>-0.877977</td>\n",
       "      <td>-0.022937</td>\n",
       "      <td>-1.058550</td>\n",
       "      <td>0.221214</td>\n",
       "      <td>-0.038836</td>\n",
       "      <td>-0.652436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.217063</td>\n",
       "      <td>0.859949</td>\n",
       "      <td>-0.616744</td>\n",
       "      <td>-0.641489</td>\n",
       "      <td>0.423233</td>\n",
       "      <td>0.424507</td>\n",
       "      <td>0.172662</td>\n",
       "      <td>0.181614</td>\n",
       "      <td>0.738116</td>\n",
       "      <td>0.014353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180459</th>\n",
       "      <td>-1.309538</td>\n",
       "      <td>0.163464</td>\n",
       "      <td>0.270769</td>\n",
       "      <td>-0.300558</td>\n",
       "      <td>-0.877276</td>\n",
       "      <td>-0.023715</td>\n",
       "      <td>-1.058222</td>\n",
       "      <td>0.223174</td>\n",
       "      <td>-0.038801</td>\n",
       "      <td>-0.653191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216391</td>\n",
       "      <td>0.859942</td>\n",
       "      <td>-0.616294</td>\n",
       "      <td>-0.641450</td>\n",
       "      <td>0.423682</td>\n",
       "      <td>0.425169</td>\n",
       "      <td>0.172423</td>\n",
       "      <td>0.181048</td>\n",
       "      <td>0.738087</td>\n",
       "      <td>0.014523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180460</th>\n",
       "      <td>-1.310430</td>\n",
       "      <td>0.163905</td>\n",
       "      <td>0.270045</td>\n",
       "      <td>-0.300560</td>\n",
       "      <td>-0.877339</td>\n",
       "      <td>-0.023643</td>\n",
       "      <td>-1.058168</td>\n",
       "      <td>0.223168</td>\n",
       "      <td>-0.038803</td>\n",
       "      <td>-0.653154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216426</td>\n",
       "      <td>0.859966</td>\n",
       "      <td>-0.616254</td>\n",
       "      <td>-0.641398</td>\n",
       "      <td>0.423837</td>\n",
       "      <td>0.425360</td>\n",
       "      <td>0.172421</td>\n",
       "      <td>0.180635</td>\n",
       "      <td>0.738085</td>\n",
       "      <td>0.014608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180498</th>\n",
       "      <td>-1.306906</td>\n",
       "      <td>0.162115</td>\n",
       "      <td>0.273153</td>\n",
       "      <td>-0.300319</td>\n",
       "      <td>-0.876812</td>\n",
       "      <td>-0.024225</td>\n",
       "      <td>-1.058242</td>\n",
       "      <td>0.223964</td>\n",
       "      <td>-0.038788</td>\n",
       "      <td>-0.653538</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215993</td>\n",
       "      <td>0.859884</td>\n",
       "      <td>-0.616220</td>\n",
       "      <td>-0.641597</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.172352</td>\n",
       "      <td>0.181976</td>\n",
       "      <td>0.738110</td>\n",
       "      <td>0.014327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180500</th>\n",
       "      <td>-1.312233</td>\n",
       "      <td>0.164846</td>\n",
       "      <td>0.268328</td>\n",
       "      <td>-0.300792</td>\n",
       "      <td>-0.877751</td>\n",
       "      <td>-0.023184</td>\n",
       "      <td>-1.058200</td>\n",
       "      <td>0.222353</td>\n",
       "      <td>-0.038821</td>\n",
       "      <td>-0.652777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216776</td>\n",
       "      <td>0.860018</td>\n",
       "      <td>-0.616369</td>\n",
       "      <td>-0.641316</td>\n",
       "      <td>0.423951</td>\n",
       "      <td>0.425429</td>\n",
       "      <td>0.172523</td>\n",
       "      <td>0.180049</td>\n",
       "      <td>0.738107</td>\n",
       "      <td>0.014699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171820 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Principal component 1  Principal component 2  Principal component 3  \\\n",
       "0                    4.092749              -1.039351               1.260691   \n",
       "1                    4.444136              -0.990882               0.766887   \n",
       "2                    4.184336              -0.683525               0.008515   \n",
       "3                   -1.713239               0.312049               0.565426   \n",
       "4                   -1.653736               0.194937               0.633811   \n",
       "...                       ...                    ...                    ...   \n",
       "180458              -1.309732               0.163678               0.269997   \n",
       "180459              -1.309538               0.163464               0.270769   \n",
       "180460              -1.310430               0.163905               0.270045   \n",
       "180498              -1.306906               0.162115               0.273153   \n",
       "180500              -1.312233               0.164846               0.268328   \n",
       "\n",
       "        Principal component 4  Principal component 5  Principal component 6  \\\n",
       "0                   -4.038849               1.985634               1.064285   \n",
       "1                   -2.737491               1.758523               1.036710   \n",
       "2                   -4.374319              -0.862230              18.046185   \n",
       "3                    0.083323              -0.084603               0.206758   \n",
       "4                   -0.673525              -0.474191               0.111487   \n",
       "...                       ...                    ...                    ...   \n",
       "180458              -0.301117              -0.877977              -0.022937   \n",
       "180459              -0.300558              -0.877276              -0.023715   \n",
       "180460              -0.300560              -0.877339              -0.023643   \n",
       "180498              -0.300319              -0.876812              -0.024225   \n",
       "180500              -0.300792              -0.877751              -0.023184   \n",
       "\n",
       "        Principal component 7  Principal component 8  Principal component 9  \\\n",
       "0                    0.626387              -4.319053              -0.378186   \n",
       "1                    1.748314              -4.440705              -0.321942   \n",
       "2                    2.732318               4.997712               0.845690   \n",
       "3                   -0.418716              -0.327378              -0.026131   \n",
       "4                   -0.577059              -0.013320               0.094306   \n",
       "...                       ...                    ...                    ...   \n",
       "180458              -1.058550               0.221214              -0.038836   \n",
       "180459              -1.058222               0.223174              -0.038801   \n",
       "180460              -1.058168               0.223168              -0.038803   \n",
       "180498              -1.058242               0.223964              -0.038788   \n",
       "180500              -1.058200               0.222353              -0.038821   \n",
       "\n",
       "        Principal component 10  ...  Principal component 21  \\\n",
       "0                    -2.292077  ...                0.442761   \n",
       "1                    -2.087103  ...               -0.161841   \n",
       "2                    -1.541559  ...               -1.289033   \n",
       "3                    -0.799707  ...                0.272844   \n",
       "4                    -0.967332  ...               -0.063274   \n",
       "...                        ...  ...                     ...   \n",
       "180458               -0.652436  ...               -0.217063   \n",
       "180459               -0.653191  ...               -0.216391   \n",
       "180460               -0.653154  ...               -0.216426   \n",
       "180498               -0.653538  ...               -0.215993   \n",
       "180500               -0.652777  ...               -0.216776   \n",
       "\n",
       "        Principal component 22  Principal component 23  \\\n",
       "0                    -0.674434                0.700384   \n",
       "1                    -2.012189                2.037390   \n",
       "2                     0.236197               -0.113712   \n",
       "3                    -0.637691               -0.992782   \n",
       "4                    -0.020065                0.383190   \n",
       "...                        ...                     ...   \n",
       "180458                0.859949               -0.616744   \n",
       "180459                0.859942               -0.616294   \n",
       "180460                0.859966               -0.616254   \n",
       "180498                0.859884               -0.616220   \n",
       "180500                0.860018               -0.616369   \n",
       "\n",
       "        Principal component 24  Principal component 25  \\\n",
       "0                     0.538268               -0.173432   \n",
       "1                    -0.565709               -1.112169   \n",
       "2                    -0.441430                0.408279   \n",
       "3                     0.698782               -0.016019   \n",
       "4                     0.180168                0.378840   \n",
       "...                        ...                     ...   \n",
       "180458               -0.641489                0.423233   \n",
       "180459               -0.641450                0.423682   \n",
       "180460               -0.641398                0.423837   \n",
       "180498               -0.641597                0.423400   \n",
       "180500               -0.641316                0.423951   \n",
       "\n",
       "        Principal component 26  Principal component 27  \\\n",
       "0                    -0.348808               -1.069114   \n",
       "1                    -0.363729               -1.425324   \n",
       "2                     1.302977               -0.906553   \n",
       "3                     0.183645                0.195657   \n",
       "4                     0.077930                0.169476   \n",
       "...                        ...                     ...   \n",
       "180458                0.424507                0.172662   \n",
       "180459                0.425169                0.172423   \n",
       "180460                0.425360                0.172421   \n",
       "180498                0.424800                0.172352   \n",
       "180500                0.425429                0.172523   \n",
       "\n",
       "        Principal component 28  Principal component 29  Principal component 30  \n",
       "0                     0.040534                1.065742               -0.221165  \n",
       "1                     0.024095                1.566272                0.501800  \n",
       "2                    -0.257008               -1.823342               -0.001037  \n",
       "3                    -0.208603                0.041691               -0.588364  \n",
       "4                    -0.092612                0.160256               -0.325747  \n",
       "...                        ...                     ...                     ...  \n",
       "180458                0.181614                0.738116                0.014353  \n",
       "180459                0.181048                0.738087                0.014523  \n",
       "180460                0.180635                0.738085                0.014608  \n",
       "180498                0.181976                0.738110                0.014327  \n",
       "180500                0.180049                0.738107                0.014699  \n",
       "\n",
       "[171820 rows x 30 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-finished code using just the first label in the labels array. Selects all the relevant rows/ data points. \n",
    "\n",
    "- df.loc[] method can be used to locate certain rows based upon the query passed into the square brackets[]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal component 1</th>\n",
       "      <th>Principal component 2</th>\n",
       "      <th>Principal component 3</th>\n",
       "      <th>Principal component 4</th>\n",
       "      <th>Principal component 5</th>\n",
       "      <th>Principal component 6</th>\n",
       "      <th>Principal component 7</th>\n",
       "      <th>Principal component 8</th>\n",
       "      <th>Principal component 9</th>\n",
       "      <th>Principal component 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Principal component 21</th>\n",
       "      <th>Principal component 22</th>\n",
       "      <th>Principal component 23</th>\n",
       "      <th>Principal component 24</th>\n",
       "      <th>Principal component 25</th>\n",
       "      <th>Principal component 26</th>\n",
       "      <th>Principal component 27</th>\n",
       "      <th>Principal component 28</th>\n",
       "      <th>Principal component 29</th>\n",
       "      <th>Principal component 30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.092749</td>\n",
       "      <td>-1.039351</td>\n",
       "      <td>1.260691</td>\n",
       "      <td>-4.038849</td>\n",
       "      <td>1.985634</td>\n",
       "      <td>1.064285</td>\n",
       "      <td>0.626387</td>\n",
       "      <td>-4.319053</td>\n",
       "      <td>-0.378186</td>\n",
       "      <td>-2.292077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442761</td>\n",
       "      <td>-0.674434</td>\n",
       "      <td>0.700384</td>\n",
       "      <td>0.538268</td>\n",
       "      <td>-0.173432</td>\n",
       "      <td>-0.348808</td>\n",
       "      <td>-1.069114</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>1.065742</td>\n",
       "      <td>-0.221165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.444136</td>\n",
       "      <td>-0.990882</td>\n",
       "      <td>0.766887</td>\n",
       "      <td>-2.737491</td>\n",
       "      <td>1.758523</td>\n",
       "      <td>1.036710</td>\n",
       "      <td>1.748314</td>\n",
       "      <td>-4.440705</td>\n",
       "      <td>-0.321942</td>\n",
       "      <td>-2.087103</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161841</td>\n",
       "      <td>-2.012189</td>\n",
       "      <td>2.037390</td>\n",
       "      <td>-0.565709</td>\n",
       "      <td>-1.112169</td>\n",
       "      <td>-0.363729</td>\n",
       "      <td>-1.425324</td>\n",
       "      <td>0.024095</td>\n",
       "      <td>1.566272</td>\n",
       "      <td>0.501800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.184336</td>\n",
       "      <td>-0.683525</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>-4.374319</td>\n",
       "      <td>-0.862230</td>\n",
       "      <td>18.046185</td>\n",
       "      <td>2.732318</td>\n",
       "      <td>4.997712</td>\n",
       "      <td>0.845690</td>\n",
       "      <td>-1.541559</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.289033</td>\n",
       "      <td>0.236197</td>\n",
       "      <td>-0.113712</td>\n",
       "      <td>-0.441430</td>\n",
       "      <td>0.408279</td>\n",
       "      <td>1.302977</td>\n",
       "      <td>-0.906553</td>\n",
       "      <td>-0.257008</td>\n",
       "      <td>-1.823342</td>\n",
       "      <td>-0.001037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.713239</td>\n",
       "      <td>0.312049</td>\n",
       "      <td>0.565426</td>\n",
       "      <td>0.083323</td>\n",
       "      <td>-0.084603</td>\n",
       "      <td>0.206758</td>\n",
       "      <td>-0.418716</td>\n",
       "      <td>-0.327378</td>\n",
       "      <td>-0.026131</td>\n",
       "      <td>-0.799707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272844</td>\n",
       "      <td>-0.637691</td>\n",
       "      <td>-0.992782</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>-0.016019</td>\n",
       "      <td>0.183645</td>\n",
       "      <td>0.195657</td>\n",
       "      <td>-0.208603</td>\n",
       "      <td>0.041691</td>\n",
       "      <td>-0.588364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.653736</td>\n",
       "      <td>0.194937</td>\n",
       "      <td>0.633811</td>\n",
       "      <td>-0.673525</td>\n",
       "      <td>-0.474191</td>\n",
       "      <td>0.111487</td>\n",
       "      <td>-0.577059</td>\n",
       "      <td>-0.013320</td>\n",
       "      <td>0.094306</td>\n",
       "      <td>-0.967332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063274</td>\n",
       "      <td>-0.020065</td>\n",
       "      <td>0.383190</td>\n",
       "      <td>0.180168</td>\n",
       "      <td>0.378840</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.092612</td>\n",
       "      <td>0.160256</td>\n",
       "      <td>-0.325747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171738</th>\n",
       "      <td>-2.080635</td>\n",
       "      <td>0.281920</td>\n",
       "      <td>0.917342</td>\n",
       "      <td>-1.160169</td>\n",
       "      <td>-0.283203</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>-1.517045</td>\n",
       "      <td>1.134613</td>\n",
       "      <td>-0.000422</td>\n",
       "      <td>0.947635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355567</td>\n",
       "      <td>0.922128</td>\n",
       "      <td>0.944757</td>\n",
       "      <td>0.926720</td>\n",
       "      <td>-1.245846</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>-0.311228</td>\n",
       "      <td>-0.197733</td>\n",
       "      <td>0.156889</td>\n",
       "      <td>-0.096722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171739</th>\n",
       "      <td>-1.988899</td>\n",
       "      <td>0.292872</td>\n",
       "      <td>0.770566</td>\n",
       "      <td>-2.050039</td>\n",
       "      <td>0.178067</td>\n",
       "      <td>-0.284153</td>\n",
       "      <td>-0.720700</td>\n",
       "      <td>0.551885</td>\n",
       "      <td>0.011976</td>\n",
       "      <td>0.059235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427968</td>\n",
       "      <td>-1.417568</td>\n",
       "      <td>-0.826078</td>\n",
       "      <td>-0.735086</td>\n",
       "      <td>1.149698</td>\n",
       "      <td>-0.323733</td>\n",
       "      <td>0.229033</td>\n",
       "      <td>0.119760</td>\n",
       "      <td>-0.298181</td>\n",
       "      <td>0.141480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171740</th>\n",
       "      <td>-2.286003</td>\n",
       "      <td>0.326393</td>\n",
       "      <td>1.031072</td>\n",
       "      <td>-2.154463</td>\n",
       "      <td>0.082432</td>\n",
       "      <td>-0.304836</td>\n",
       "      <td>-1.476853</td>\n",
       "      <td>1.601041</td>\n",
       "      <td>-0.002130</td>\n",
       "      <td>2.050197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566127</td>\n",
       "      <td>0.181841</td>\n",
       "      <td>0.065557</td>\n",
       "      <td>-0.939717</td>\n",
       "      <td>-0.071471</td>\n",
       "      <td>0.359359</td>\n",
       "      <td>0.403155</td>\n",
       "      <td>0.073381</td>\n",
       "      <td>0.043813</td>\n",
       "      <td>-0.215638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171741</th>\n",
       "      <td>-2.288205</td>\n",
       "      <td>0.326853</td>\n",
       "      <td>1.032382</td>\n",
       "      <td>-2.163379</td>\n",
       "      <td>0.087006</td>\n",
       "      <td>-0.308509</td>\n",
       "      <td>-1.469079</td>\n",
       "      <td>1.604203</td>\n",
       "      <td>-0.001871</td>\n",
       "      <td>2.074340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573426</td>\n",
       "      <td>0.172568</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>-0.939130</td>\n",
       "      <td>-0.072575</td>\n",
       "      <td>0.359498</td>\n",
       "      <td>0.403008</td>\n",
       "      <td>0.072601</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.216689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171742</th>\n",
       "      <td>-1.979263</td>\n",
       "      <td>0.292334</td>\n",
       "      <td>0.761868</td>\n",
       "      <td>-2.009196</td>\n",
       "      <td>0.151581</td>\n",
       "      <td>-0.276704</td>\n",
       "      <td>-0.741047</td>\n",
       "      <td>0.556305</td>\n",
       "      <td>0.011177</td>\n",
       "      <td>0.029298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449435</td>\n",
       "      <td>-1.454866</td>\n",
       "      <td>-0.767357</td>\n",
       "      <td>-0.781413</td>\n",
       "      <td>1.142284</td>\n",
       "      <td>-0.339676</td>\n",
       "      <td>0.244014</td>\n",
       "      <td>0.121743</td>\n",
       "      <td>-0.296423</td>\n",
       "      <td>0.163109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170060 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Principal component 1  Principal component 2  Principal component 3  \\\n",
       "0                    4.092749              -1.039351               1.260691   \n",
       "1                    4.444136              -0.990882               0.766887   \n",
       "2                    4.184336              -0.683525               0.008515   \n",
       "3                   -1.713239               0.312049               0.565426   \n",
       "4                   -1.653736               0.194937               0.633811   \n",
       "...                       ...                    ...                    ...   \n",
       "171738              -2.080635               0.281920               0.917342   \n",
       "171739              -1.988899               0.292872               0.770566   \n",
       "171740              -2.286003               0.326393               1.031072   \n",
       "171741              -2.288205               0.326853               1.032382   \n",
       "171742              -1.979263               0.292334               0.761868   \n",
       "\n",
       "        Principal component 4  Principal component 5  Principal component 6  \\\n",
       "0                   -4.038849               1.985634               1.064285   \n",
       "1                   -2.737491               1.758523               1.036710   \n",
       "2                   -4.374319              -0.862230              18.046185   \n",
       "3                    0.083323              -0.084603               0.206758   \n",
       "4                   -0.673525              -0.474191               0.111487   \n",
       "...                       ...                    ...                    ...   \n",
       "171738              -1.160169              -0.283203               0.004132   \n",
       "171739              -2.050039               0.178067              -0.284153   \n",
       "171740              -2.154463               0.082432              -0.304836   \n",
       "171741              -2.163379               0.087006              -0.308509   \n",
       "171742              -2.009196               0.151581              -0.276704   \n",
       "\n",
       "        Principal component 7  Principal component 8  Principal component 9  \\\n",
       "0                    0.626387              -4.319053              -0.378186   \n",
       "1                    1.748314              -4.440705              -0.321942   \n",
       "2                    2.732318               4.997712               0.845690   \n",
       "3                   -0.418716              -0.327378              -0.026131   \n",
       "4                   -0.577059              -0.013320               0.094306   \n",
       "...                       ...                    ...                    ...   \n",
       "171738              -1.517045               1.134613              -0.000422   \n",
       "171739              -0.720700               0.551885               0.011976   \n",
       "171740              -1.476853               1.601041              -0.002130   \n",
       "171741              -1.469079               1.604203              -0.001871   \n",
       "171742              -0.741047               0.556305               0.011177   \n",
       "\n",
       "        Principal component 10  ...  Principal component 21  \\\n",
       "0                    -2.292077  ...                0.442761   \n",
       "1                    -2.087103  ...               -0.161841   \n",
       "2                    -1.541559  ...               -1.289033   \n",
       "3                    -0.799707  ...                0.272844   \n",
       "4                    -0.967332  ...               -0.063274   \n",
       "...                        ...  ...                     ...   \n",
       "171738                0.947635  ...                0.355567   \n",
       "171739                0.059235  ...                0.427968   \n",
       "171740                2.050197  ...                0.566127   \n",
       "171741                2.074340  ...                0.573426   \n",
       "171742                0.029298  ...                0.449435   \n",
       "\n",
       "        Principal component 22  Principal component 23  \\\n",
       "0                    -0.674434                0.700384   \n",
       "1                    -2.012189                2.037390   \n",
       "2                     0.236197               -0.113712   \n",
       "3                    -0.637691               -0.992782   \n",
       "4                    -0.020065                0.383190   \n",
       "...                        ...                     ...   \n",
       "171738                0.922128                0.944757   \n",
       "171739               -1.417568               -0.826078   \n",
       "171740                0.181841                0.065557   \n",
       "171741                0.172568                0.060751   \n",
       "171742               -1.454866               -0.767357   \n",
       "\n",
       "        Principal component 24  Principal component 25  \\\n",
       "0                     0.538268               -0.173432   \n",
       "1                    -0.565709               -1.112169   \n",
       "2                    -0.441430                0.408279   \n",
       "3                     0.698782               -0.016019   \n",
       "4                     0.180168                0.378840   \n",
       "...                        ...                     ...   \n",
       "171738                0.926720               -1.245846   \n",
       "171739               -0.735086                1.149698   \n",
       "171740               -0.939717               -0.071471   \n",
       "171741               -0.939130               -0.072575   \n",
       "171742               -0.781413                1.142284   \n",
       "\n",
       "        Principal component 26  Principal component 27  \\\n",
       "0                    -0.348808               -1.069114   \n",
       "1                    -0.363729               -1.425324   \n",
       "2                     1.302977               -0.906553   \n",
       "3                     0.183645                0.195657   \n",
       "4                     0.077930                0.169476   \n",
       "...                        ...                     ...   \n",
       "171738                0.223691               -0.311228   \n",
       "171739               -0.323733                0.229033   \n",
       "171740                0.359359                0.403155   \n",
       "171741                0.359498                0.403008   \n",
       "171742               -0.339676                0.244014   \n",
       "\n",
       "        Principal component 28  Principal component 29  Principal component 30  \n",
       "0                     0.040534                1.065742               -0.221165  \n",
       "1                     0.024095                1.566272                0.501800  \n",
       "2                    -0.257008               -1.823342               -0.001037  \n",
       "3                    -0.208603                0.041691               -0.588364  \n",
       "4                    -0.092612                0.160256               -0.325747  \n",
       "...                        ...                     ...                     ...  \n",
       "171738               -0.197733                0.156889               -0.096722  \n",
       "171739                0.119760               -0.298181                0.141480  \n",
       "171740                0.073381                0.043813               -0.215638  \n",
       "171741                0.072601                0.044451               -0.216689  \n",
       "171742                0.121743               -0.296423                0.163109  \n",
       "\n",
       "[170060 rows x 30 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x1 = X_test[np.where(np.asarray(y_train).reshape(-1, 1).ravel() == labels[0])]\n",
    "# X_train_L = X_train.iloc[np.where(y_train[:,1] == 1)]\n",
    "X_train_L = X_train.loc[y_train == labels[0]]\n",
    "X_train_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-finished code using just the first label in the labels array. Selects all the relevant rows/ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal component 1</th>\n",
       "      <th>Principal component 2</th>\n",
       "      <th>Principal component 3</th>\n",
       "      <th>Principal component 4</th>\n",
       "      <th>Principal component 5</th>\n",
       "      <th>Principal component 6</th>\n",
       "      <th>Principal component 7</th>\n",
       "      <th>Principal component 8</th>\n",
       "      <th>Principal component 9</th>\n",
       "      <th>Principal component 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Principal component 21</th>\n",
       "      <th>Principal component 22</th>\n",
       "      <th>Principal component 23</th>\n",
       "      <th>Principal component 24</th>\n",
       "      <th>Principal component 25</th>\n",
       "      <th>Principal component 26</th>\n",
       "      <th>Principal component 27</th>\n",
       "      <th>Principal component 28</th>\n",
       "      <th>Principal component 29</th>\n",
       "      <th>Principal component 30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171745</th>\n",
       "      <td>-2.303129</td>\n",
       "      <td>0.361628</td>\n",
       "      <td>0.947852</td>\n",
       "      <td>-3.620681</td>\n",
       "      <td>1.067760</td>\n",
       "      <td>-0.965932</td>\n",
       "      <td>1.098259</td>\n",
       "      <td>0.684323</td>\n",
       "      <td>0.061317</td>\n",
       "      <td>3.117149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628069</td>\n",
       "      <td>-1.895014</td>\n",
       "      <td>-0.954208</td>\n",
       "      <td>-0.657958</td>\n",
       "      <td>0.854606</td>\n",
       "      <td>-0.246395</td>\n",
       "      <td>0.128871</td>\n",
       "      <td>0.073815</td>\n",
       "      <td>-0.186069</td>\n",
       "      <td>0.085861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171746</th>\n",
       "      <td>-2.327750</td>\n",
       "      <td>0.334694</td>\n",
       "      <td>1.057779</td>\n",
       "      <td>-2.300704</td>\n",
       "      <td>0.164222</td>\n",
       "      <td>-0.363147</td>\n",
       "      <td>-1.342109</td>\n",
       "      <td>1.652518</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>2.466938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675268</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>-0.042131</td>\n",
       "      <td>-0.922967</td>\n",
       "      <td>-0.097237</td>\n",
       "      <td>0.372179</td>\n",
       "      <td>0.395776</td>\n",
       "      <td>0.057406</td>\n",
       "      <td>0.055095</td>\n",
       "      <td>-0.231572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171747</th>\n",
       "      <td>-1.648880</td>\n",
       "      <td>0.266311</td>\n",
       "      <td>0.469178</td>\n",
       "      <td>1.250193</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.431733</td>\n",
       "      <td>-0.073524</td>\n",
       "      <td>-0.329310</td>\n",
       "      <td>0.012102</td>\n",
       "      <td>-0.091711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034353</td>\n",
       "      <td>0.638336</td>\n",
       "      <td>-0.088737</td>\n",
       "      <td>0.225347</td>\n",
       "      <td>0.482053</td>\n",
       "      <td>0.015635</td>\n",
       "      <td>0.190969</td>\n",
       "      <td>0.005160</td>\n",
       "      <td>-0.205524</td>\n",
       "      <td>-0.295392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171748</th>\n",
       "      <td>12.481084</td>\n",
       "      <td>-3.616323</td>\n",
       "      <td>0.451260</td>\n",
       "      <td>-2.301646</td>\n",
       "      <td>-0.770633</td>\n",
       "      <td>1.066906</td>\n",
       "      <td>-2.625990</td>\n",
       "      <td>-5.464694</td>\n",
       "      <td>-0.555513</td>\n",
       "      <td>1.549606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.789315</td>\n",
       "      <td>-0.557111</td>\n",
       "      <td>-0.165775</td>\n",
       "      <td>-0.372385</td>\n",
       "      <td>-0.448510</td>\n",
       "      <td>-0.547764</td>\n",
       "      <td>0.114492</td>\n",
       "      <td>-1.602567</td>\n",
       "      <td>-0.241119</td>\n",
       "      <td>-0.255728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171749</th>\n",
       "      <td>-1.459016</td>\n",
       "      <td>0.309033</td>\n",
       "      <td>0.141391</td>\n",
       "      <td>2.051974</td>\n",
       "      <td>0.169310</td>\n",
       "      <td>0.410693</td>\n",
       "      <td>0.461944</td>\n",
       "      <td>-0.464321</td>\n",
       "      <td>-0.010347</td>\n",
       "      <td>0.050964</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134433</td>\n",
       "      <td>-0.168244</td>\n",
       "      <td>-0.045204</td>\n",
       "      <td>-0.026509</td>\n",
       "      <td>-0.100042</td>\n",
       "      <td>0.032717</td>\n",
       "      <td>0.003517</td>\n",
       "      <td>-0.046756</td>\n",
       "      <td>-0.057856</td>\n",
       "      <td>-0.034660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190906</th>\n",
       "      <td>-1.354979</td>\n",
       "      <td>0.294521</td>\n",
       "      <td>0.096545</td>\n",
       "      <td>1.349656</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.248632</td>\n",
       "      <td>0.582599</td>\n",
       "      <td>-0.536717</td>\n",
       "      <td>0.049130</td>\n",
       "      <td>-0.606542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096936</td>\n",
       "      <td>-1.160574</td>\n",
       "      <td>0.345408</td>\n",
       "      <td>-0.166113</td>\n",
       "      <td>-0.641177</td>\n",
       "      <td>0.143569</td>\n",
       "      <td>0.115087</td>\n",
       "      <td>-0.283245</td>\n",
       "      <td>0.156974</td>\n",
       "      <td>-0.168432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190907</th>\n",
       "      <td>-1.517663</td>\n",
       "      <td>0.304064</td>\n",
       "      <td>0.216418</td>\n",
       "      <td>1.565045</td>\n",
       "      <td>-0.019931</td>\n",
       "      <td>0.317247</td>\n",
       "      <td>0.311972</td>\n",
       "      <td>-0.303801</td>\n",
       "      <td>-0.008274</td>\n",
       "      <td>0.138634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273440</td>\n",
       "      <td>-0.281598</td>\n",
       "      <td>-0.072978</td>\n",
       "      <td>-0.002141</td>\n",
       "      <td>-0.048658</td>\n",
       "      <td>0.023680</td>\n",
       "      <td>-0.054482</td>\n",
       "      <td>-0.003589</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.090318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190908</th>\n",
       "      <td>-1.419322</td>\n",
       "      <td>0.317637</td>\n",
       "      <td>0.082230</td>\n",
       "      <td>1.903840</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.323318</td>\n",
       "      <td>0.545954</td>\n",
       "      <td>-0.360911</td>\n",
       "      <td>-0.001133</td>\n",
       "      <td>0.171949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.313809</td>\n",
       "      <td>-0.466276</td>\n",
       "      <td>0.120292</td>\n",
       "      <td>-0.185041</td>\n",
       "      <td>-0.201036</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>-0.080849</td>\n",
       "      <td>-0.016240</td>\n",
       "      <td>0.075471</td>\n",
       "      <td>0.169020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190909</th>\n",
       "      <td>-1.649988</td>\n",
       "      <td>0.266934</td>\n",
       "      <td>0.467961</td>\n",
       "      <td>1.250169</td>\n",
       "      <td>0.159151</td>\n",
       "      <td>0.432371</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-0.330663</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.088190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036056</td>\n",
       "      <td>0.638537</td>\n",
       "      <td>-0.087614</td>\n",
       "      <td>0.225365</td>\n",
       "      <td>0.481676</td>\n",
       "      <td>0.015481</td>\n",
       "      <td>0.191181</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>-0.205600</td>\n",
       "      <td>-0.295715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190910</th>\n",
       "      <td>-1.675775</td>\n",
       "      <td>0.266191</td>\n",
       "      <td>0.496850</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>-0.015304</td>\n",
       "      <td>0.340036</td>\n",
       "      <td>-0.132997</td>\n",
       "      <td>-0.187160</td>\n",
       "      <td>0.017165</td>\n",
       "      <td>0.038310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117895</td>\n",
       "      <td>0.449693</td>\n",
       "      <td>-0.046242</td>\n",
       "      <td>0.176543</td>\n",
       "      <td>0.465732</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.120160</td>\n",
       "      <td>0.040277</td>\n",
       "      <td>-0.098953</td>\n",
       "      <td>-0.137579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18862 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Principal component 1  Principal component 2  Principal component 3  \\\n",
       "171745              -2.303129               0.361628               0.947852   \n",
       "171746              -2.327750               0.334694               1.057779   \n",
       "171747              -1.648880               0.266311               0.469178   \n",
       "171748              12.481084              -3.616323               0.451260   \n",
       "171749              -1.459016               0.309033               0.141391   \n",
       "...                       ...                    ...                    ...   \n",
       "190906              -1.354979               0.294521               0.096545   \n",
       "190907              -1.517663               0.304064               0.216418   \n",
       "190908              -1.419322               0.317637               0.082230   \n",
       "190909              -1.649988               0.266934               0.467961   \n",
       "190910              -1.675775               0.266191               0.496850   \n",
       "\n",
       "        Principal component 4  Principal component 5  Principal component 6  \\\n",
       "171745              -3.620681               1.067760              -0.965932   \n",
       "171746              -2.300704               0.164222              -0.363147   \n",
       "171747               1.250193               0.159521               0.431733   \n",
       "171748              -2.301646              -0.770633               1.066906   \n",
       "171749               2.051974               0.169310               0.410693   \n",
       "...                       ...                    ...                    ...   \n",
       "190906               1.349656               0.014952               0.248632   \n",
       "190907               1.565045              -0.019931               0.317247   \n",
       "190908               1.903840               0.017500               0.323318   \n",
       "190909               1.250169               0.159151               0.432371   \n",
       "190910               0.886289              -0.015304               0.340036   \n",
       "\n",
       "        Principal component 7  Principal component 8  Principal component 9  \\\n",
       "171745               1.098259               0.684323               0.061317   \n",
       "171746              -1.342109               1.652518               0.003006   \n",
       "171747              -0.073524              -0.329310               0.012102   \n",
       "171748              -2.625990              -5.464694              -0.555513   \n",
       "171749               0.461944              -0.464321              -0.010347   \n",
       "...                       ...                    ...                    ...   \n",
       "190906               0.582599              -0.536717               0.049130   \n",
       "190907               0.311972              -0.303801              -0.008274   \n",
       "190908               0.545954              -0.360911              -0.001133   \n",
       "190909              -0.073269              -0.330663               0.011988   \n",
       "190910              -0.132997              -0.187160               0.017165   \n",
       "\n",
       "        Principal component 10  ...  Principal component 21  \\\n",
       "171745                3.117149  ...                0.628069   \n",
       "171746                2.466938  ...                0.675268   \n",
       "171747               -0.091711  ...                0.034353   \n",
       "171748                1.549606  ...               -0.789315   \n",
       "171749                0.050964  ...               -0.134433   \n",
       "...                        ...  ...                     ...   \n",
       "190906               -0.606542  ...                0.096936   \n",
       "190907                0.138634  ...               -0.273440   \n",
       "190908                0.171949  ...               -0.313809   \n",
       "190909               -0.088190  ...                0.036056   \n",
       "190910                0.038310  ...               -0.117895   \n",
       "\n",
       "        Principal component 22  Principal component 23  \\\n",
       "171745               -1.895014               -0.954208   \n",
       "171746                0.017202               -0.042131   \n",
       "171747                0.638336               -0.088737   \n",
       "171748               -0.557111               -0.165775   \n",
       "171749               -0.168244               -0.045204   \n",
       "...                        ...                     ...   \n",
       "190906               -1.160574                0.345408   \n",
       "190907               -0.281598               -0.072978   \n",
       "190908               -0.466276                0.120292   \n",
       "190909                0.638537               -0.087614   \n",
       "190910                0.449693               -0.046242   \n",
       "\n",
       "        Principal component 24  Principal component 25  \\\n",
       "171745               -0.657958                0.854606   \n",
       "171746               -0.922967               -0.097237   \n",
       "171747                0.225347                0.482053   \n",
       "171748               -0.372385               -0.448510   \n",
       "171749               -0.026509               -0.100042   \n",
       "...                        ...                     ...   \n",
       "190906               -0.166113               -0.641177   \n",
       "190907               -0.002141               -0.048658   \n",
       "190908               -0.185041               -0.201036   \n",
       "190909                0.225365                0.481676   \n",
       "190910                0.176543                0.465732   \n",
       "\n",
       "        Principal component 26  Principal component 27  \\\n",
       "171745               -0.246395                0.128871   \n",
       "171746                0.372179                0.395776   \n",
       "171747                0.015635                0.190969   \n",
       "171748               -0.547764                0.114492   \n",
       "171749                0.032717                0.003517   \n",
       "...                        ...                     ...   \n",
       "190906                0.143569                0.115087   \n",
       "190907                0.023680               -0.054482   \n",
       "190908                0.023039               -0.080849   \n",
       "190909                0.015481                0.191181   \n",
       "190910                0.006283                0.120160   \n",
       "\n",
       "        Principal component 28  Principal component 29  Principal component 30  \n",
       "171745                0.073815               -0.186069                0.085861  \n",
       "171746                0.057406                0.055095               -0.231572  \n",
       "171747                0.005160               -0.205524               -0.295392  \n",
       "171748               -1.602567               -0.241119               -0.255728  \n",
       "171749               -0.046756               -0.057856               -0.034660  \n",
       "...                        ...                     ...                     ...  \n",
       "190906               -0.283245                0.156974               -0.168432  \n",
       "190907               -0.003589                0.037226                0.090318  \n",
       "190908               -0.016240                0.075471                0.169020  \n",
       "190909                0.005602               -0.205600               -0.295715  \n",
       "190910                0.040277               -0.098953               -0.137579  \n",
       "\n",
       "[18862 rows x 30 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_L = X_test.loc[pred_y == labels[0]]\n",
    "X_test_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Statistical distance measure algorithms\n",
    "This is just a preliminary run to show an example of how to use the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_features = len(X_train_L.columns)\n",
    "num_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the statistical measure (univariate version) methods from the local Python modules defined in the project folder.\n",
    "\n",
    "Because the Python methods are slightly different to the current MATLAB versions, one has to pass in a feature 1 at a time (univariate). Then, after all the features have been passed in and sent back, this means the full dataset has been used to calculate the various ECDF-based statistical distance measures. This is what is displayed in the plotting of results section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cramer Von Mises distances:  [4931.06913973 1880.22609555 2637.74853648 3887.08189088 2817.73400374\n",
      " 1228.99862326 5326.72344433 1441.84903199 5581.9304574  1870.07340615\n",
      " 2519.32470931 1750.8082772  2960.96771103 2652.20239772 2528.64404831\n",
      " 2096.51552976 3333.99816795 2706.48495659 2372.50233069 5436.64931689\n",
      " 3777.57495035 4925.00252582 5050.56161273 5449.59664843 2873.31023483\n",
      " 3181.46095623 2522.44419938 3196.61503693 6118.89177346 1681.43774696]\n",
      "\n",
      " Anderson Darling distances:  [26.75836521 13.96957615 15.04432319 21.44928371 16.33276503  8.07662147\n",
      " 27.73575527  8.61158416 29.58329696 11.35074387 12.95107542 10.54389511\n",
      " 16.26973466 15.61034957 15.15529759 11.06755426 18.73245964 15.46430099\n",
      " 14.29728255 26.96297443 20.03427739 26.87518677 26.36601908 29.07657652\n",
      " 15.61414686 16.30856267 15.81665662 17.54938689 32.84830419  9.74571338]\n",
      "\n",
      " Kolmogorov Smirnov distances:  [0.06031173 0.02489308 0.03294141 0.04644905 0.03471953 0.02084796\n",
      " 0.06044592 0.0279256  0.05558753 0.02576305 0.03573841 0.01999573\n",
      " 0.03628943 0.03214395 0.03222831 0.02750462 0.03788592 0.03841825\n",
      " 0.02533481 0.07742486 0.05014193 0.04808604 0.05346439 0.07160053\n",
      " 0.0330098  0.04937766 0.03301063 0.03836271 0.06011866 0.02550153]\n",
      "\n",
      " Kuiper distances:  [0.06032349 0.01149973 0.01935854 0.04646081 0.00626701 0.01230803\n",
      " 0.06045768 0.0173451  0.05559929 0.00435825 0.00963785 0.00166906\n",
      " 0.0160907  0.02111281 0.03227545 0.00673302 0.03697399 0.01307906\n",
      " 0.02338477 0.01059116 0.00910143 0.00760807 0.05347615 0.00976979\n",
      " 0.00929793 0.00540413 0.03302239 0.03837447 0.06013042 0.02551329]\n",
      "\n",
      " Wasserstein distances:  [0.24958577 0.15824563 0.25178723 0.11977606 0.10573819 0.06168662\n",
      " 0.11639157 0.06753823 0.03397168 0.08187523 0.0369315  0.05216177\n",
      " 0.06188003 0.06288675 0.06319308 0.02899481 0.07470969 0.05762343\n",
      " 0.06237184 0.02684291 0.05343275 0.0682461  0.05514995 0.05826239\n",
      " 0.03005149 0.01775112 0.03355342 0.02152919 0.03756587 0.01767137]\n",
      "\n",
      " DTS distances:  [0.00538804 0.01093877 0.00650061 0.00201896 0.00296578 0.00379281\n",
      " 0.00213027 0.00249206 0.00395448 0.00355204 0.00045691 0.00229925\n",
      " 0.00265188 0.00132097 0.00184694 0.00097039 0.00496045 0.00277177\n",
      " 0.00363574 0.00140761 0.00174648 0.00147676 0.0008543  0.00086065\n",
      " 0.00047709 0.00057554 0.00123237 0.00050832 0.00109688 0.00059107]\n"
     ]
    }
   ],
   "source": [
    "CVM_distances = np.zeros(num_of_features)\n",
    "Anderson_Darling_distances = np.zeros(num_of_features)\n",
    "Kolmogorov_Smirnov_distances = np.zeros(num_of_features)\n",
    "Kuiper_distances = np.zeros(num_of_features)\n",
    "Wasserstein_distances = np.zeros(num_of_features)\n",
    "DTS_distances = np.zeros(num_of_features)\n",
    "\n",
    "for i in range(0, num_of_features):\n",
    "    # iloc[:, i] allows selection of the ith feature in the Pandas dataframe \n",
    "    CVM_distances[i] = Cramer_Von_Mises_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "    Anderson_Darling_distances[i] = Anderson_Darling_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "    Kolmogorov_Smirnov_distances[i] = Kolmogorov_Smirnov_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "    Kuiper_distances[i] = Kuiper_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "    Wasserstein_distances[i] = Wasserstein_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "    DTS_distances[i] = DTS_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "    \n",
    "print(\" Cramer Von Mises distances: \", CVM_distances)\n",
    "print(\"\\n Anderson Darling distances: \", Anderson_Darling_distances)\n",
    "print(\"\\n Kolmogorov Smirnov distances: \", Kolmogorov_Smirnov_distances)\n",
    "print(\"\\n Kuiper distances: \", Kuiper_distances)\n",
    "print(\"\\n Wasserstein distances: \", Wasserstein_distances)\n",
    "print(\"\\n DTS distances: \", DTS_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the test/ example results\n",
    "This shows the varying distances of each statistical distance measure, between **each feature only**, in a single dataset split (1 permutation). Therefore, it is only an example and not representative of the real run (like a CRISP-DM cycle run through, where one can evaluate and re-run after).\n",
    "\n",
    "The real run will plot the distances between all the features (entire dataset) of each dataset split (Permutation) against the average accuracy from all permutations. Thus, showing how statistical distances can be used to estimate the accuracy of models (the SafeML idea)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 3)\n",
    "\n",
    "axs[0, 0].boxplot(CVM_distances)\n",
    "axs[0, 0].set_title('CVM distances')\n",
    "axs[0, 0].set_ylabel(\"Distance\")\n",
    "\n",
    "axs[0, 1].boxplot(Anderson_Darling_distances)\n",
    "axs[0, 1].set_title('Anderson Darling distances')\n",
    "axs[0, 1].set_ylabel(\"Distance\")\n",
    "\n",
    "axs[0, 2].boxplot(Kolmogorov_Smirnov_distances)\n",
    "axs[0, 2].set_title('Kolmogorov Smirnov distances')\n",
    "axs[0, 2].set_ylabel(\"Distance\")\n",
    "\n",
    "axs[1, 0].boxplot(Kuiper_distances)\n",
    "axs[1, 0].set_title(\"Kuiper distances\")\n",
    "axs[1, 0].set_ylabel(\"Distance\")\n",
    "\n",
    "axs[1, 1].boxplot(Wasserstein_distances)\n",
    "axs[1, 1].set_title(\"Wasserstein distances\")\n",
    "axs[1, 1].set_ylabel(\"Distance\")\n",
    "\n",
    "axs[1, 2].boxplot(DTS_distances)\n",
    "axs[1, 2].set_title(\"DTS distances\")\n",
    "axs[1, 2].set_ylabel(\"Distance\")\n",
    "\n",
    "fig.subplots_adjust(left=0.08, right=1.8, bottom=0.05, top=1.5, hspace=0.5, wspace=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting the real run, using the PCA-transformed dataset\n",
    "The dataset shown below is the PCA-transformed data, with the label/ class feature attached (which has been LabelBinarized, into numbers from class names- so the model can understand).\n",
    "\n",
    "To simulate the SafeML idea, a number of permutations have to be run. Each permutation will have its data split differently, and this results in varying model accuracies (different data => different accuracies). The hope (for the simulation) is that as these accuracies vary between permutations, the statistical difference values vary along with it- in a highly (positive) correlated way. This would show that ECDF-based statistical distance measures can actually be used to predict model performance. The final graphing/ plotting will show these relationships, so the correlation can be easily identified. A case-study for this dataset has been run in MATLAB, but not Python. The aim of this file is to have a Python code case-study for the same dataset, and verify the results to see if they match/ are similar to the MATLAB implementation. If it matches, then this case-study can be confidently merged into the master branch, of the SafeML repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Principal component 1</th>\n",
       "      <th>Principal component 2</th>\n",
       "      <th>Principal component 3</th>\n",
       "      <th>Principal component 4</th>\n",
       "      <th>Principal component 5</th>\n",
       "      <th>Principal component 6</th>\n",
       "      <th>Principal component 7</th>\n",
       "      <th>Principal component 8</th>\n",
       "      <th>Principal component 9</th>\n",
       "      <th>Principal component 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Principal component 22</th>\n",
       "      <th>Principal component 23</th>\n",
       "      <th>Principal component 24</th>\n",
       "      <th>Principal component 25</th>\n",
       "      <th>Principal component 26</th>\n",
       "      <th>Principal component 27</th>\n",
       "      <th>Principal component 28</th>\n",
       "      <th>Principal component 29</th>\n",
       "      <th>Principal component 30</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.092749</td>\n",
       "      <td>-1.039351</td>\n",
       "      <td>1.260691</td>\n",
       "      <td>-4.038849</td>\n",
       "      <td>1.985634</td>\n",
       "      <td>1.064285</td>\n",
       "      <td>0.626387</td>\n",
       "      <td>-4.319053</td>\n",
       "      <td>-0.378186</td>\n",
       "      <td>-2.292077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.674434</td>\n",
       "      <td>0.700384</td>\n",
       "      <td>0.538268</td>\n",
       "      <td>-0.173432</td>\n",
       "      <td>-0.348808</td>\n",
       "      <td>-1.069114</td>\n",
       "      <td>0.040534</td>\n",
       "      <td>1.065742</td>\n",
       "      <td>-0.221165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.444136</td>\n",
       "      <td>-0.990882</td>\n",
       "      <td>0.766887</td>\n",
       "      <td>-2.737491</td>\n",
       "      <td>1.758523</td>\n",
       "      <td>1.036710</td>\n",
       "      <td>1.748314</td>\n",
       "      <td>-4.440705</td>\n",
       "      <td>-0.321942</td>\n",
       "      <td>-2.087103</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.012189</td>\n",
       "      <td>2.037390</td>\n",
       "      <td>-0.565709</td>\n",
       "      <td>-1.112169</td>\n",
       "      <td>-0.363729</td>\n",
       "      <td>-1.425324</td>\n",
       "      <td>0.024095</td>\n",
       "      <td>1.566272</td>\n",
       "      <td>0.501800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.184336</td>\n",
       "      <td>-0.683525</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>-4.374319</td>\n",
       "      <td>-0.862230</td>\n",
       "      <td>18.046185</td>\n",
       "      <td>2.732318</td>\n",
       "      <td>4.997712</td>\n",
       "      <td>0.845690</td>\n",
       "      <td>-1.541559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236197</td>\n",
       "      <td>-0.113712</td>\n",
       "      <td>-0.441430</td>\n",
       "      <td>0.408279</td>\n",
       "      <td>1.302977</td>\n",
       "      <td>-0.906553</td>\n",
       "      <td>-0.257008</td>\n",
       "      <td>-1.823342</td>\n",
       "      <td>-0.001037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.713239</td>\n",
       "      <td>0.312049</td>\n",
       "      <td>0.565426</td>\n",
       "      <td>0.083323</td>\n",
       "      <td>-0.084603</td>\n",
       "      <td>0.206758</td>\n",
       "      <td>-0.418716</td>\n",
       "      <td>-0.327378</td>\n",
       "      <td>-0.026131</td>\n",
       "      <td>-0.799707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.637691</td>\n",
       "      <td>-0.992782</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>-0.016019</td>\n",
       "      <td>0.183645</td>\n",
       "      <td>0.195657</td>\n",
       "      <td>-0.208603</td>\n",
       "      <td>0.041691</td>\n",
       "      <td>-0.588364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.653736</td>\n",
       "      <td>0.194937</td>\n",
       "      <td>0.633811</td>\n",
       "      <td>-0.673525</td>\n",
       "      <td>-0.474191</td>\n",
       "      <td>0.111487</td>\n",
       "      <td>-0.577059</td>\n",
       "      <td>-0.013320</td>\n",
       "      <td>0.094306</td>\n",
       "      <td>-0.967332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020065</td>\n",
       "      <td>0.383190</td>\n",
       "      <td>0.180168</td>\n",
       "      <td>0.378840</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>-0.092612</td>\n",
       "      <td>0.160256</td>\n",
       "      <td>-0.325747</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190906</th>\n",
       "      <td>-1.354979</td>\n",
       "      <td>0.294521</td>\n",
       "      <td>0.096545</td>\n",
       "      <td>1.349656</td>\n",
       "      <td>0.014952</td>\n",
       "      <td>0.248632</td>\n",
       "      <td>0.582599</td>\n",
       "      <td>-0.536717</td>\n",
       "      <td>0.049130</td>\n",
       "      <td>-0.606542</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.160574</td>\n",
       "      <td>0.345408</td>\n",
       "      <td>-0.166113</td>\n",
       "      <td>-0.641177</td>\n",
       "      <td>0.143569</td>\n",
       "      <td>0.115087</td>\n",
       "      <td>-0.283245</td>\n",
       "      <td>0.156974</td>\n",
       "      <td>-0.168432</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190907</th>\n",
       "      <td>-1.517663</td>\n",
       "      <td>0.304064</td>\n",
       "      <td>0.216418</td>\n",
       "      <td>1.565045</td>\n",
       "      <td>-0.019931</td>\n",
       "      <td>0.317247</td>\n",
       "      <td>0.311972</td>\n",
       "      <td>-0.303801</td>\n",
       "      <td>-0.008274</td>\n",
       "      <td>0.138634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281598</td>\n",
       "      <td>-0.072978</td>\n",
       "      <td>-0.002141</td>\n",
       "      <td>-0.048658</td>\n",
       "      <td>0.023680</td>\n",
       "      <td>-0.054482</td>\n",
       "      <td>-0.003589</td>\n",
       "      <td>0.037226</td>\n",
       "      <td>0.090318</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190908</th>\n",
       "      <td>-1.419322</td>\n",
       "      <td>0.317637</td>\n",
       "      <td>0.082230</td>\n",
       "      <td>1.903840</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.323318</td>\n",
       "      <td>0.545954</td>\n",
       "      <td>-0.360911</td>\n",
       "      <td>-0.001133</td>\n",
       "      <td>0.171949</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.466276</td>\n",
       "      <td>0.120292</td>\n",
       "      <td>-0.185041</td>\n",
       "      <td>-0.201036</td>\n",
       "      <td>0.023039</td>\n",
       "      <td>-0.080849</td>\n",
       "      <td>-0.016240</td>\n",
       "      <td>0.075471</td>\n",
       "      <td>0.169020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190909</th>\n",
       "      <td>-1.649988</td>\n",
       "      <td>0.266934</td>\n",
       "      <td>0.467961</td>\n",
       "      <td>1.250169</td>\n",
       "      <td>0.159151</td>\n",
       "      <td>0.432371</td>\n",
       "      <td>-0.073269</td>\n",
       "      <td>-0.330663</td>\n",
       "      <td>0.011988</td>\n",
       "      <td>-0.088190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638537</td>\n",
       "      <td>-0.087614</td>\n",
       "      <td>0.225365</td>\n",
       "      <td>0.481676</td>\n",
       "      <td>0.015481</td>\n",
       "      <td>0.191181</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>-0.205600</td>\n",
       "      <td>-0.295715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190910</th>\n",
       "      <td>-1.675775</td>\n",
       "      <td>0.266191</td>\n",
       "      <td>0.496850</td>\n",
       "      <td>0.886289</td>\n",
       "      <td>-0.015304</td>\n",
       "      <td>0.340036</td>\n",
       "      <td>-0.132997</td>\n",
       "      <td>-0.187160</td>\n",
       "      <td>0.017165</td>\n",
       "      <td>0.038310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449693</td>\n",
       "      <td>-0.046242</td>\n",
       "      <td>0.176543</td>\n",
       "      <td>0.465732</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.120160</td>\n",
       "      <td>0.040277</td>\n",
       "      <td>-0.098953</td>\n",
       "      <td>-0.137579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190911 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Principal component 1  Principal component 2  Principal component 3  \\\n",
       "0                    4.092749              -1.039351               1.260691   \n",
       "1                    4.444136              -0.990882               0.766887   \n",
       "2                    4.184336              -0.683525               0.008515   \n",
       "3                   -1.713239               0.312049               0.565426   \n",
       "4                   -1.653736               0.194937               0.633811   \n",
       "...                       ...                    ...                    ...   \n",
       "190906              -1.354979               0.294521               0.096545   \n",
       "190907              -1.517663               0.304064               0.216418   \n",
       "190908              -1.419322               0.317637               0.082230   \n",
       "190909              -1.649988               0.266934               0.467961   \n",
       "190910              -1.675775               0.266191               0.496850   \n",
       "\n",
       "        Principal component 4  Principal component 5  Principal component 6  \\\n",
       "0                   -4.038849               1.985634               1.064285   \n",
       "1                   -2.737491               1.758523               1.036710   \n",
       "2                   -4.374319              -0.862230              18.046185   \n",
       "3                    0.083323              -0.084603               0.206758   \n",
       "4                   -0.673525              -0.474191               0.111487   \n",
       "...                       ...                    ...                    ...   \n",
       "190906               1.349656               0.014952               0.248632   \n",
       "190907               1.565045              -0.019931               0.317247   \n",
       "190908               1.903840               0.017500               0.323318   \n",
       "190909               1.250169               0.159151               0.432371   \n",
       "190910               0.886289              -0.015304               0.340036   \n",
       "\n",
       "        Principal component 7  Principal component 8  Principal component 9  \\\n",
       "0                    0.626387              -4.319053              -0.378186   \n",
       "1                    1.748314              -4.440705              -0.321942   \n",
       "2                    2.732318               4.997712               0.845690   \n",
       "3                   -0.418716              -0.327378              -0.026131   \n",
       "4                   -0.577059              -0.013320               0.094306   \n",
       "...                       ...                    ...                    ...   \n",
       "190906               0.582599              -0.536717               0.049130   \n",
       "190907               0.311972              -0.303801              -0.008274   \n",
       "190908               0.545954              -0.360911              -0.001133   \n",
       "190909              -0.073269              -0.330663               0.011988   \n",
       "190910              -0.132997              -0.187160               0.017165   \n",
       "\n",
       "        Principal component 10  ...  Principal component 22  \\\n",
       "0                    -2.292077  ...               -0.674434   \n",
       "1                    -2.087103  ...               -2.012189   \n",
       "2                    -1.541559  ...                0.236197   \n",
       "3                    -0.799707  ...               -0.637691   \n",
       "4                    -0.967332  ...               -0.020065   \n",
       "...                        ...  ...                     ...   \n",
       "190906               -0.606542  ...               -1.160574   \n",
       "190907                0.138634  ...               -0.281598   \n",
       "190908                0.171949  ...               -0.466276   \n",
       "190909               -0.088190  ...                0.638537   \n",
       "190910                0.038310  ...                0.449693   \n",
       "\n",
       "        Principal component 23  Principal component 24  \\\n",
       "0                     0.700384                0.538268   \n",
       "1                     2.037390               -0.565709   \n",
       "2                    -0.113712               -0.441430   \n",
       "3                    -0.992782                0.698782   \n",
       "4                     0.383190                0.180168   \n",
       "...                        ...                     ...   \n",
       "190906                0.345408               -0.166113   \n",
       "190907               -0.072978               -0.002141   \n",
       "190908                0.120292               -0.185041   \n",
       "190909               -0.087614                0.225365   \n",
       "190910               -0.046242                0.176543   \n",
       "\n",
       "        Principal component 25  Principal component 26  \\\n",
       "0                    -0.173432               -0.348808   \n",
       "1                    -1.112169               -0.363729   \n",
       "2                     0.408279                1.302977   \n",
       "3                    -0.016019                0.183645   \n",
       "4                     0.378840                0.077930   \n",
       "...                        ...                     ...   \n",
       "190906               -0.641177                0.143569   \n",
       "190907               -0.048658                0.023680   \n",
       "190908               -0.201036                0.023039   \n",
       "190909                0.481676                0.015481   \n",
       "190910                0.465732                0.006283   \n",
       "\n",
       "        Principal component 27  Principal component 28  \\\n",
       "0                    -1.069114                0.040534   \n",
       "1                    -1.425324                0.024095   \n",
       "2                    -0.906553               -0.257008   \n",
       "3                     0.195657               -0.208603   \n",
       "4                     0.169476               -0.092612   \n",
       "...                        ...                     ...   \n",
       "190906                0.115087               -0.283245   \n",
       "190907               -0.054482               -0.003589   \n",
       "190908               -0.080849               -0.016240   \n",
       "190909                0.191181                0.005602   \n",
       "190910                0.120160                0.040277   \n",
       "\n",
       "        Principal component 29  Principal component 30  label  \n",
       "0                     1.065742               -0.221165      0  \n",
       "1                     1.566272                0.501800      0  \n",
       "2                    -1.823342               -0.001037      0  \n",
       "3                     0.041691               -0.588364      0  \n",
       "4                     0.160256               -0.325747      0  \n",
       "...                        ...                     ...    ...  \n",
       "190906                0.156974               -0.168432      0  \n",
       "190907                0.037226                0.090318      0  \n",
       "190908                0.075471                0.169020      0  \n",
       "190909               -0.205600               -0.295715      0  \n",
       "190910               -0.098953               -0.137579      0  \n",
       "\n",
       "[190911 rows x 31 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the main loop and all other required loops (for permutations)\n",
    "The permutation number is instantiated at the top of the notebook, in the environment variables section. This allows the number of permutations to be easily changed.\n",
    "\n",
    "IMPORTANT NOTE: A **for loop** in one Jupyter Notebook cell cannot be extended past a single cell. This is bad for documentation purposes because markdown cells cannot be inserted between cells as this would cause the **for loop** to become out of scope for other cells. Therefore, a highly modulated approach must be taken, where everything is put into a function/ method. This allows markdown cells to exist above a section of code/ method so it can be explained/ documented. Code comments will be used when necessary too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Set all variables needed outside for loop scope, e.g. the multi-dimensional Results array.\n",
    "# Results(row, column, page) in matlab. results(row, column, depth) in Python/ numpy.\n",
    "#results = [[],[],[]]\n",
    "\n",
    "#number_of_classes = len(df_final['label'].unique())\n",
    "\n",
    "#results = numpy.zeros((number_of_classes, ,k))\n",
    "#for current_permutation in range(number_of_permutations):\n",
    "    # 1.1. Cross validation, train test splitting here (extract to functions)\n",
    "    # 1.2. Train classifiers here using the train and test data. 'classifiers' variable name to be used.\n",
    "    # 1.3. Extract unique labels here, for next loop\n",
    "    #for current_classifier in range(classifiers):\n",
    "        # 2.1. Run model.predict for current_classifier.\n",
    "        # 2.2. Using the pred_y, get the confusion matrix or classification report for this.\n",
    "        # 2.3. Use the num of unique labels variable or set new one (for num of classes), for next loop\n",
    "        #for current_label in range(labels.size or classNum.size or .length):\n",
    "            # 3.1. Get train and test rows with matching current_label/ class (via .loc)\n",
    "            # 3.2. Using the multi-dim Results array, save the model accuracy of that specific...\n",
    "            # ... permutation, by using the pred_y and Y_Test in previous loop. Save to the ...\n",
    "            # ... Results(current_label, 1, current_permutation) index (of multi-dim array)\n",
    "            # 3.3. Use train and test rows (matching current_label) and pass into statistical distance methods ...\n",
    "            # ... They're Univariate methods (in Python) so be careful to use them correctly like examples above.\n",
    "            # 3.4. Save each stastical distance measure result to Results(current_label, x, current_permutation) ...\n",
    "            # ... where x is an int for each statistical distance result to save.\n",
    "            # 3.5. END the current_label for loop\n",
    "        # 2.4. After for loop above is finished, plot the accuracies via MatPlotLib. Koorosh sorts the results ...\n",
    "        # ... first for some reason. May not need to but this is to be decided.\n",
    "        # 2.5. END the current_classifier for loop\n",
    "    # 1.4. END the current_permutation for loop (loop is finished)\n",
    "    \n",
    "# Finally, boxplot the results ontop of the current accuracy graph/ plot. Then, show the final plots.\n",
    "# Koorosh uses the number of classes for this, to iterate over them, so may need this.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipping above implementation to try something simpler and better hopefully\n",
    "Instead of a 3D array, just use 1 classifier at a time and then have a 2D array (or Pandas DataFrame for easy plotting later) where each row is a permutation and each column has: the model's accuracy in 1st column, then each subsequent statistical distance measure to be used in the later columns.\n",
    "\n",
    "Then, one can graph a line graph/ Seaborn displot of the accuracy on the x-axis vs the specific statistical distance measure on the y-axis. Therefore resulting in 6 plots/ graphs, 1 for each ECDF statistical distance measure.\n",
    "\n",
    "Each class will represent a line on the graph (differentiated via hue colouring) or by having separate plots for each class/ label.\n",
    "\n",
    "NOTE: Could also store F1-Score, Sensitivity score, Precision score, etc...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Anderson_Darling_dist</th>\n",
       "      <th>CVM_dist</th>\n",
       "      <th>DTS_dist</th>\n",
       "      <th>Kolmogorov_Smirnov_dist</th>\n",
       "      <th>Kuiper_dist</th>\n",
       "      <th>Wasserstein_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Accuracy, Anderson_Darling_dist, CVM_dist, DTS_dist, Kolmogorov_Smirnov_dist, Kuiper_dist, Wasserstein_dist]\n",
       "Index: []"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Firstly, creating the final 2D-array (Pandas Dataframe) which will be used to store the Results\n",
    "# PRESERVE THE ORDERING\n",
    "results_column_names = ['Accuracy', 'Anderson_Darling_dist', 'CVM_dist',\n",
    "                                     'DTS_dist', 'Kolmogorov_Smirnov_dist','Kuiper_dist', 'Wasserstein_dist']\n",
    "# Creating the empty Dataframe for Results\n",
    "df_results = pd.DataFrame(columns = results_column_names)\n",
    "# Can copy this dataframe for future results tables e.g. for each class/ label\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for each permutation\n",
    "Now, running the code for each permutation requires StratifiedKFold splitting, model training and predicting classes via the chosen classifier, and finally the iteration over each class/ label to calculate the statistical distance. Then, once complete, the results table can be used to plot the results from all the permutations.\n",
    "\n",
    "**Quick note on terminology:** The terminology 'class' and 'label' is used interchangeably throughout the industry, but both mean the same things. They represent the different possible answers the dataset holds and/ or the model can output e.g. a dataset (for supervised learning i.e. given the answers to train with) with a person's personal data may have 2 classes/ labels (answers). Either Male or Female. Male is 1 class/ label, and Female is 1 class/ label. The number of classes in the dataset would be 2. Also, a model would only be able to predict and output 2 different options/ results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Set all variables needed outside for loop scope, e.g. the multi-dimensional Results array.\n",
    "labels = df_final['label'].unique()\n",
    "number_of_classes = len(df_final['label'].unique())\n",
    "# List of Lists to hold each separate label result, using list_index->label_number mapping (for accessing)\n",
    "# Using Python 'List Comprehensions'. This way is Python sub-list ID safe, see link for more info:\n",
    "# https://thispointer.com/how-to-create-and-initialize-a-list-of-lists-in-python/\n",
    "list_of_lists_results = [[] for i in range(number_of_classes)]\n",
    "# See https://stackoverflow.com/a/17496530. Fast way to 'append' to dataframe for results table.\n",
    "\n",
    "for current_permutation in range(number_of_permutations):\n",
    "    # 1.1. Cross validation, train test splitting here (extract to functions)\n",
    "    X_train, X_test, y_train, y_test = get_stratifiedKFold_train_test_split(X, y)\n",
    "    \n",
    "    # 1.2. Train MLP classifier here using the train and test data, and call model.predict to get pred_y\n",
    "    pred_y, accuracy = train_and_predict_Neural_Network_MLP_model(X_train, X_test, y_train, y_test)\n",
    "    # TODO: Make an object to add the accuracy, and subsequent statistical dist. measure results\n",
    "    \n",
    "    # 1.3. loop over each label/ class to add a new boxplot, or add new hue to plot, or a new plot entirely\n",
    "    for current_label in range(number_of_classes):\n",
    "        \n",
    "        # 1.4. Run the statistical dist measures and save result to Pd.Series obj, to add to results Dataframe\n",
    "        X_train_loc_for_label, X_test_loc_for_label = get_X_train_and_test_data_for_given_label(labels,\n",
    "                                                              current_label, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # 1.5. Get all statistical dist measures for current_label\n",
    "        dict_result_row = get_statistical_dist_measures_for_class_result(accuracy,\n",
    "                                                X_train_loc_for_label, X_test_loc_for_label)\n",
    "        \n",
    "        # 1.6. Append new dict row to current_label index of list of lists\n",
    "        # See: https://thispointer.com/how-to-create-and-initialize-a-list-of-lists-in-python/\n",
    "        list_of_lists_results[current_label].append(dict_result_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all permutations, convert each index (dict list) to a pandas dataframe: //stackoverflow.com/a/17496530"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After all permutations, convert each index (dict list) to a pandas dataframe: //stackoverflow.com/a/17496530\n",
    "# Access specific dataframe by index e.g. class 1 dataframe->index 1 mapping\n",
    "result_dataframes = []\n",
    "\n",
    "for dict_result_list in list_of_lists_results:\n",
    "    result_dataframes.append(pd.DataFrame(dict_result_list, columns = results_column_names))\n",
    "\n",
    "# Print first dataframe result table, for class 0\n",
    "result_dataframes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send methods to top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send to top and add documentation\n",
    "def get_statistical_dist_measures_for_class_result(accuracy, X_train_L, X_test_L):\n",
    "    # Can use this to loop over all the features, since the ECDF Python methods are currently Univariate only\n",
    "    num_of_features = len(X_train_L.columns)\n",
    "    \n",
    "    # Instantiate empty arrays with large enough size, to hold statistical distance data\n",
    "    CVM_distances = np.zeros(num_of_features)\n",
    "    Anderson_Darling_distances = np.zeros(num_of_features)\n",
    "    Kolmogorov_Smirnov_distances = np.zeros(num_of_features)\n",
    "    Kuiper_distances = np.zeros(num_of_features)\n",
    "    Wasserstein_distances = np.zeros(num_of_features)\n",
    "    DTS_distances = np.zeros(num_of_features)\n",
    "\n",
    "    for i in range(0, num_of_features):\n",
    "        # iloc[:, i] allows selection of the ith feature in the Pandas dataframe\n",
    "        # Calling the methods from the imported Python modules (see import section at top of notebook)\n",
    "        CVM_distances[i] = Cramer_Von_Mises_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        Anderson_Darling_distances[i] = Anderson_Darling_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        Kolmogorov_Smirnov_distances[i] = Kolmogorov_Smirnov_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        Kuiper_distances[i] = Kuiper_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        Wasserstein_distances[i] = Wasserstein_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        DTS_distances[i] = DTS_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        \n",
    "    # Computing mean/ average, to get ECDF distance of full dataset. Float64 to keep accuracy high.\n",
    "    # See: https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n",
    "    CVM_distance = np.mean(CVM_distances, dtype=np.float64)\n",
    "    Anderson_Darling_distance = np.mean(Anderson_Darling_distances, dtype=np.float64)\n",
    "    Kolmogorov_Smirnov_distance = np.mean(Kolmogorov_Smirnov_distances, dtype=np.float64)\n",
    "    Kuiper_distance = np.mean(Kuiper_distances, dtype=np.float64)\n",
    "    Wasserstein_distance = np.mean(Wasserstein_distances, dtype=np.float64)\n",
    "    DTS_distance = np.mean(DTS_distances, dtype=np.float64)\n",
    "    \n",
    "    # Returning dictionary, for efficient and fast DataFrame creation. Returns mean for each distance.\n",
    "    # See https://stackoverflow.com/a/17496530. Fast way to 'append' to dataframe for results table.\n",
    "    # PRESERVE THE ORDERING\n",
    "    return {'Accuracy': accuracy,\n",
    "            'Anderson_Darling_dist': Anderson_Darling_distance,\n",
    "            'CVM_dist': CVM_distance,\n",
    "            'DTS_dist':DTS_distance,\n",
    "            'Kolmogorov_Smirnov_dist':Kolmogorov_Smirnov_distance,\n",
    "            'Kuiper_dist': Kuiper_distance,\n",
    "            'Wasserstein_dist': Wasserstein_distance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send to top and add documentation for it\n",
    "def get_X_train_and_test_data_for_given_label(labels, label_index, X_train, X_test, y_train, y_test):\n",
    "    X_train_loc_for_label = X_train.loc[y_train == labels[label_index]]\n",
    "    X_test_loc_for_label = X_test.loc[pred_y == labels[label_index]]\n",
    "    \n",
    "    return X_train_loc_for_label, X_test_loc_for_label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
