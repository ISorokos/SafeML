{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SafeML Implementation for German Traffic Sign Recognition\nIn this example, we try to show how SafeML approach can be used for German Traffic Sign Recognition.We have used an existing code from the following URL that do the traffic sign classification using Convolutional Neural Networks (CNNs) with about 97% accuracy. Then the SafeML part is added to show how the safety of the approach can be monitored:\n\nSource: https://www.kaggle.com/lalithmovva/99-accuracy-on-german-traffic-sign-recognition\n\nHere is the table of content:\n* Defining the required libraries and loading the german traffic sgin recognition dataset.\n* Separating Train, Test and Validation Data\n* Defining the CNN model and its architecture.\n* Training the model and calculating its accuracy\n* Applying the model on test data\n* Comparing the true labels with predicted labels and using the statistical parametric mapping (as the SafeML method)"},{"metadata":{},"cell_type":"markdown","source":"## Defining the required libraries and loading the german traffic sgin recognition dataset.\nBefore running the code, the dataset should be downloaded an stored on the local computer. Please make sure that the path has been chosen correctly. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Source: https://www.kaggle.com/lalithmovva/99-accuracy-on-german-traffic-sign-recognition\n# Libraries \nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\nfrom PIL import Image\nimport os\n\n# Reading the input images and putting them into a numpy array\ndata=[]\nlabels=[]\n\nheight = 30\nwidth = 30\nchannels = 3\nclasses = 43\nn_inputs = height * width*channels\n\nfor i in range(classes) :\n    path = \"../input/gtsrb-german-traffic-sign/Train/{0}/\".format(i)\n    #path = \"C:/cmder/Python_Tests/GTSRB/Train/{0}/\".format(i)\n    print(path)\n    Class=os.listdir(path)\n    for a in Class:\n        try:\n            image=cv2.imread(path+a)\n            image_from_array = Image.fromarray(image, 'RGB')\n            size_image = image_from_array.resize((height, width))\n            data.append(np.array(size_image))\n            labels.append(i)\n        except AttributeError:\n            print(\" \")\n            \nCells=np.array(data)\nlabels=np.array(labels)\n\n#Randomize the order of the input images\ns=np.arange(Cells.shape[0])\nnp.random.seed(43)\nnp.random.shuffle(s)\nCells=Cells[s]\nlabels=labels[s]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Separating Train, Test and Validation Data\nIn this section, 20% of data is separated for test and validation and the rest is used for training. This section can be improved using methods like K-fold cross-validation. We tried to make it as simple as possible."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Spliting the images into train and validation sets\n(X_train,X_val)=Cells[(int)(0.2*len(labels)):],Cells[:(int)(0.2*len(labels))]\nX_train = X_train.astype('float32')/255 \nX_val = X_val.astype('float32')/255\n(y_train,y_val)=labels[(int)(0.2*len(labels)):],labels[:(int)(0.2*len(labels))]\n\n#Using one hote encoding for the train and validation labels\nfrom keras.utils import to_categorical\n\ny_train = to_categorical(y_train, 43)\ny_val = to_categorical(y_val, 43)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining the CNN model and its architecture\nHere the CNN model's structure is defined. Regarding the loss function the categorical crossentropy is used, and the ADAM is selection for the network optimization."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Definition of the DNN model\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(rate=0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(rate=0.5))\nmodel.add(Dense(43, activation='softmax'))\n\n#Compilation of the model\nmodel.compile(\n    loss='categorical_crossentropy', \n    optimizer='adam', \n    metrics=['accuracy']\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the Model and calculating its accuracy\nIn this section, the CNN model is trained and the accuracy plot is shown. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#using ten epochs for the training and saving the accuracy for each epoch\nepochs = 20\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=epochs,\nvalidation_data=(X_val, y_val))\n\n#Display of the accuracy and the loss values\nimport matplotlib.pyplot as plt\n\nplt.figure(0)\nplt.plot(history.history['accuracy'], label='training accuracy')\nplt.plot(history.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying the model on test data\nHaving trained the CNN model, the test data is used. The predicted decisions has been stored as \"y_pred\". As can be seen the accuracy of the model is about 0.97%."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Predicting with the test data\ny_test=pd.read_csv(\"../input/gtsrb-german-traffic-sign/Test.csv\")\n# labels=y_test['Path'].as_matrix()\nlabels=y_test['Path'].to_numpy()\ny_test=y_test['ClassId'].values\n\ndata=[]\n\nfor f in labels:\n    image=cv2.imread('../input/gtsrb-german-traffic-sign/Test/'+f.replace('Test/', ''))\n    image_from_array = Image.fromarray(image, 'RGB')\n    size_image = image_from_array.resize((height, width))\n    data.append(np.array(size_image))\n\nX_test=np.array(data)\nX_test = X_test.astype('float32')/255 \ny_pred = model.predict_classes(X_test)\n\n#Accuracy with the test data\nfrom sklearn.metrics import accuracy_score\n\nprint(y_pred)\n\naccuracy_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparing the true labels with predicted labels and using the statistical parametric mapping (as the SafeML method)\n<p style='text-align: justify;'> \nThe idea of SafeML is to measure statistical distances and estimate the accuracy of the model when there is no available labels. Having an accuracy estimation at run time can be so vital for safety-critical applications. In this section, the statistical difference is addressed and the accuracy estimation will be considered on our later versions. Here, we simply find the test data where y_test is not equal to the predicted one (y_pred) and name it as X_test_wrong, y_test_wrong. Then for example, for label == 2, we compare the trained data (trusted data) and the X_test_wrong (selecting only label 2 ones for instance). Then using statistical parametric mapping, we try to find the statistical explanation to see what was different that our CNN model made a wronge decision. This explanation can be investigated in different perspectives and we just provide a sample.</p>\n\nTo find more about SafeML idea, please check our GitHub:\n\nhttps://github.com/ISorokos/SafeML"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Safety Monitoring through Statistical Parametric Mapping\n!pip install spm1d\nimport os\nfrom matplotlib import pyplot\nimport spm1d\n\n# Separating Wrong Responses of the CNN Classifier\nX_test_wrong, y_test_wrong = X_test[np.where(y_test != y_pred)], y_test[np.where(y_test != y_pred)]\n\n# print(X_test_wrong.shape)\n\n# Finding Wrong Decisions for Label 2 (just an example)\nX_test_wrong3, y_test_wrong3 = X_test_wrong[np.where(y_test_wrong == 3)], y_test_wrong[np.where(y_test_wrong == 3)]\n\n# print(X_test_wrong1.shape)\n\nX_train3 = X_train[np.where(y_train[:,3] == 1)]\n\n\nfig2, ax6 = pyplot.subplots(1,3, figsize = (20,6))\n\nfor ii, c_ax in enumerate(ax6.flatten()):\n    # Comparing X_train for Label == 2 with X_Test_wronge for Label == 2\n    xxx, yyy= X_train3[:,:,:,ii], X_test_wrong3[:,:,:,ii]\n    xxx_2   = np.array([yy.flatten() for yy in xxx])\n    yyy_2   = np.array([yy.flatten() for yy in yyy]) \n    snpm    = spm1d.stats.nonparam.ttest2(xxx_2[:30], yyy_2[:30])\n    snpmi   = snpm.inference(0.05, two_tailed=True, iterations=1000) # Alpha is considered as 0.05\n    \n   # print(xxx_2.shape)\n    J,Q     = xxx_2.shape\n    z       = snpmi.z\n    zstar   = snpmi.zstar\n    z0      = np.zeros(Q)\n    z0      = z\n    Z0      = np.reshape(z0, (30,30))\n    Z0i     = Z0.copy()\n    Z0i[np.abs(Z0i)<zstar] = 0\n    ZZ      = np.hstack( [Z0, Z0i] )\n    \n    print(1 - z[z != 0].mean())\n    \n    c_ax.imshow(Z0, 'jet') # Can be replaced with Z0i\n    c_ax.set_title('RGB: {} of Set {} vs. Set {}'.format(ii, 1,2))\n    c_ax.axis('off')\n\nfig3, ax7 = pyplot.subplots(1,3, figsize = (20,6))    \n\nfor ii, c_ax in enumerate(ax7.flatten()):\n    c_ax.imshow(X_train3[1,:,:,ii], interpolation = 'none')\n    c_ax.set_title('RGB: {} -- Test {}'.format(ii+1, 3))\n    c_ax.axis(\"off\")\n    \nfig4, ax8 = pyplot.subplots(1,3, figsize = (20,6))     \n\nfor ii, c_ax in enumerate(ax8.flatten()):\n    c_ax.imshow(X_test_wrong3[1,:,:,ii], interpolation = 'none')\n    c_ax.set_title('RGB: {} -- Test {}'.format(ii+1, y_test_wrong3[1]))\n    c_ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Safety Monitoring through Statistical Parametric Mapping\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom matplotlib import pyplot\nimport spm1d\n\nfig, ax1 = plt.subplots(1,3, figsize = (30,12))\n\nfor ii, c_ax in enumerate(ax1.flatten()):\n    c_ax.imshow(X_test[1,:,:,ii], interpolation = 'none')\n    c_ax.set_title('RGB: {} -- Test {}'.format(ii+1, y_test[1]))\n    \nX_train1, y_train1 = X_train[np.where(y_train[2,:] == 1)], y_train[np.where(y_train[2,:] == 1)]\nX_test1, y_test1 = X_test[np.where(y_test == 1)], y_test[np.where(y_test == 1)]\nX_test1_pred, y_test1_pred = X_test[np.where(y_pred == 40)], y_test[np.where(y_pred == 40)]\n\n# print(X_test1.shape)\n\nfig2, ax2 = pyplot.subplots(1,3, figsize = (30,10))\n\nfor ii, c_ax in enumerate(ax2.flatten()):\n    xxx, yyy= X_test1[:,:,:,ii], X_test1_pred[:,:,:,ii]\n    xxx_2   = np.array([yy.flatten() for yy in xxx])\n    yyy_2   = np.array([yy.flatten() for yy in yyy]) \n    snpm    = spm1d.stats.nonparam.ttest2(xxx_2[:30], yyy_2[:30])\n    snpmi   = snpm.inference(0.01, two_tailed=True, iterations=1000)\n    \n    print(xxx_2.shape[1])\n    J,Q     = xxx_2.shape\n    z       = snpmi.z\n    zstar   = snpmi.zstar\n    z0      = np.zeros(Q)\n    z0      = z\n    Z0      = np.reshape(z0, (30,30))\n    Z0i     = Z0.copy()\n    Z0i[np.abs(Z0i)<zstar] = 0\n    ZZ      = np.hstack( [Z0, Z0i] )\n    \n    c_ax.imshow(Z0i, 'jet')\n    c_ax.set_title('RGB: {} of Set {} vs. Set {}'.format(ii, 1,2))\n    c_ax.axis('off')\n\nfig3, ax3 = pyplot.subplots(1,3, figsize = (30,10))    \n\nfor ii, c_ax in enumerate(ax3.flatten()):\n    c_ax.imshow(X_test1[1,:,:,ii], interpolation = 'none')\n    c_ax.set_title('RGB: {} -- Test {}'.format(ii+1, y_test1[1]))\n    c_ax.axis('off')\n    \nfig4, ax4 = pyplot.subplots(1,3, figsize = (30,10))     \n\nfor ii, c_ax in enumerate(ax4.flatten()):\n    c_ax.imshow(X_test1_pred[1,:,:,ii], interpolation = 'none')\n    c_ax.set_title('RGB: {} -- Test {}'.format(ii+1, y_test1_pred[1]))\n    c_ax.axis('off')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Wasserstein Distance Measure"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Wasserstein_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    Res = 0\n    E_CDF = 0\n    F_CDF = 0\n    power = 1\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        height = abs(F_CDF-E_CDF)\n        width = XY_Sorted[ii+1] - XY_Sorted[ii]\n        Res = Res + (height ** power) * width;  \n \n    return Res\n\ndef  Wasserstein_Dist_PVal(XX, YY):\n    # Information about Bootstrap: https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60\n    import random\n    nboots = 1000\n    WD = Wasserstein_Dist(XX,YY)\n    na = len(XX)\n    nb = len(YY)\n    n = na + nb\n    comb = np.concatenate([XX,YY])\n    reps = 0\n    bigger = 0\n    for ii in range(1, nboots):\n        e = random.sample(range(n), na)\n        f = random.sample(range(n), nb)\n        boost_WD = Wasserstein_Dist(comb[e],comb[f]);\n        if (boost_WD > WD):\n            bigger = 1 + bigger\n            \n    pVal = bigger/nboots;\n\n    return pVal, WD\n\nC_num = 3\n\n# Separating Wrong Responses of the CNN Classifier\nX_test_wrong, y_test_wrong = X_test[np.where(y_test != y_pred)], y_test[np.where(y_test != y_pred)]\n\n# print(X_test_wrong.shape)\n\n# Finding Wrong Decisions for Label 2 (just an example)\nX_test_wrong3, y_test_wrong3 = X_test_wrong[np.where(y_test_wrong == C_num)], y_test_wrong[np.where(y_test_wrong == C_num)]\n\n# print(X_test_wrong1.shape)\n\nX_train3 = X_train[np.where(y_train[:,C_num] == 1)]\n\nfig4, ax44 = pyplot.subplots(1,3, figsize = (20,6))\n#fig5, ax55 = pyplot.subplots(1,3, figsize = (20,6))\n\nfor ii, c_ax44 in enumerate(ax44.flatten()):\n    # Comparing X_train for Label == 2 with X_Test_wronge for Label == 2\n    xxx, yyy= X_train3[:,:,:,ii], X_test_wrong3[:,:,:,ii]\n    xxx_2   = np.array([yy.flatten() for yy in xxx])\n    yyy_2   = np.array([yy.flatten() for yy in yyy]) \n    \n    WD = np.zeros(900)\n    pVal = np.zeros(900)\n      \n    for kk in range(1, 900):\n        WD[kk], pVal[kk] = Wasserstein_Dist_PVal(xxx_2[:30,kk], yyy_2[:30,kk]) \n        \n    WD2 = WD    \n\n    print(1 - WD2[WD2 != 0].mean())\n    #print(1 - WD2.mean())\n    \n    WD3 = WD\n    WD3[pVal > 0.05] = 0\n    \n    J,Q     = xxx_2.shape\n    z       = WD3\n    zstar   = pVal #WD.mean()\n    z0      = np.zeros(Q)\n    z0      = z\n    z1      = z0.copy()\n    #z1[np.abs(z1)<zstar] = 0\n    Z0      = np.reshape(z1, (30,30))\n    #Z0i     = Z0.copy()\n    #Z0i[np.abs(Z0i)<zstar] = 0\n    #ZZ      = np.hstack( [Z0, Z0i] )\n    \n    z2      = WD\n    Z02      = np.reshape(z2, (30,30))\n    \n    c_ax44.imshow(Z0, 'jet') # Can be replaced with Z0i\n    c_ax44.set_title('RGB: {} of Set {} vs. Set {}'.format(ii, 1,2))\n    c_ax44.axis('off')\n    \nfig5, ax55 = pyplot.subplots(1,3, figsize = (20,6))     \n\nfor ii, c_ax55 in enumerate(ax55.flatten()):\n    c_ax55.imshow(X_train3[1,:,:,ii], interpolation = 'none')\n    c_ax55.set_title('RGB: {} -- Test {}'.format(ii+1, 3))\n    c_ax55.axis(\"off\")\n    \nfig4, ax8 = pyplot.subplots(1,3, figsize = (20,6))     \n\nfor ii, c_ax in enumerate(ax8.flatten()):\n    c_ax.imshow(X_test_wrong3[1,:,:,ii], interpolation = 'none')\n    c_ax.set_title('RGB: {} -- Test {}'.format(ii+1, y_test_wrong3[1]))\n    c_ax.axis('off')    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculating Wilson Interval Confidence\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Kläs, M., & Sembach, L. (2019). Uncertainty wrappers for data-driven models. In International Conference on Computer Safety, Reliability, and Security. Springer.\nfrom math import sqrt\ndef wilson(p, n, z = 3.29): # The z-score for a 95% confidence interval is 1.96.\n    denominator = 1 + z**2/n\n    centre_adjusted_probability = p + z*z / (2*n)\n    adjusted_standard_deviation = sqrt((p*(1 - p) + z*z / (4*n)) / n)\n    \n    lower_bound = (centre_adjusted_probability - z*adjusted_standard_deviation) / denominator\n    upper_bound = (centre_adjusted_probability + z*adjusted_standard_deviation) / denominator\n    \n    return (lower_bound, upper_bound)\n\n\nX_test_correct, y_test_correct = X_test[np.where(y_test == y_pred)], y_test[np.where(y_test == y_pred)]\n\nX_test_correct3, y_test_correct3 = X_test_correct[np.where(y_test_correct == 3)], y_test_correct[np.where(y_test_correct == 3)]\n\nprint(X_test_correct3.shape[0])\nprint(X_test_wrong3.shape[0])\n\nprint(1 - X_test_wrong3.shape[0]/X_test_correct3.shape[0])\n\nNegative = X_test_wrong3.shape[0] \ntotal = X_test_wrong3.shape[0] + X_test_correct3.shape[0] \np  = 1 - Negative / total\n\nprint(wilson(p, total))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}